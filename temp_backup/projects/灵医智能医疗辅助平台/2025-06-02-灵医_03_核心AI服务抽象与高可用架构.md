---
title: "灵医_03_核心AI服务抽象与高可用架构"
date: 2025-06-02 02:02:02 +0800
categories: [JAVA后端项目, 灵医智能医疗辅助平台]
tags: [RAG, Agent, Spring AI]
pin: false
toc: true
math: true
---

## 相关概念:

### Spring AI ChatClient & EmbeddingClient:

这是 Spring AI 框架提供的两个核心抽象接口，旨在将应用程序的业务逻辑与底层具体的 AI 模型实现完全分离开。

- ChatClient: 该接口定义了与大语言模型进行聊天交互的标准操作（例如，发送消息、接收回复）。无论底层使用的是 OpenAI 的 GPT-4、Azure OpenAI 的服务，还是本地部署的 Ollama 模型，应用程序都只与 ChatClient 这个统一的接口进行交互。
- EmbeddingClient: 该接口定义了将文本转换为向量嵌入（Embedding）的标准操作。这是 RAG 和向量检索的基础。应用程序通过调用 EmbeddingClient 来生成文本的向量表示，而无需关心具体是由哪个模型或服务来完成这个转换。

核心优势（AI 能力无关性）: 通过依赖这些抽象接口而非具体实现，实现了架构级的 AI 能力无关性。 这意味着，如果未来需要从 OpenAI 切换到 Azure OpenAI，或者出于成本/私有化考虑换成开源的 Ollama 模型，开发者几乎不需要修改任何业务代码，仅需更新项目的配置文件（例如，修改 API 密钥、模型名称和 URL），即可完成模型的无缝切换。

### RabbitMQ:

RabbitMQ 是一个实现了高级消息队列协议（AMQP）的开源消息代理软件，也就是我们常说的“消息队列”。 在系统中，它的核心作用是将耗时的操作异步化。

- 工作原理: 当一个需要较长处理时间（高时延）的请求（如完整的 RAG 检索流程或调用外部工具的 Agent 任务）到达时，系统不会立即处理它并让用户等待，而是快速地将这个任务作为一个“消息”发送到 RabbitMQ 的消息队列中，然后立即返回一个受理成功的响应给用户。 后台会有专门的工作者进程（Worker）从队列中获取这些任务并慢慢处理。
- 流量削峰 (Load Balancing): 在高并发场景下，大量请求瞬间涌入可能会压垮系统。RabbitMQ 像一个缓冲区，能够平稳地接收所有请求，然后根据后端服务的处理能力，以一个平稳的速率将任务分发下去，从而避免了系统因瞬时压力过大而崩溃。

### P95 响应时间:

P95（95th percentile）响应时间是一个关键的性能衡量指标。它指的是在一段时间内，95% 的用户请求的响应时间都低于这个数值。 例如，P95 响应时间为 2 秒意味着 95% 的用户等待时间都在 2 秒以内，只有 5% 的用户等待时间超过 2 秒。相比于平均响应时间，P95 更能反映系统在大部分情况下的性能表现，因为它排除了少数极端异常值的影响，是衡量系统稳定性的重要参考。将 P95 响应时间从 5 秒以上优化至 2 秒内，意味着绝大多数用户的体验得到了显著提升。

### Docker:

Docker 是一个开源的应用容器引擎，它允许开发者将一个应用及其所有依赖（如库、配置文件、环境变量）打包到一个轻量级、可移植的容器中。

- 核心优势: 这个容器可以在任何安装了 Docker 的机器上以完全相同的方式运行，无论是开发人员的笔记本电脑、测试服务器还是生产环境。 它解决了“在我电脑上明明是好的”这一经典难题，确保了开发、测试和部署环境的一致性，极大地简化了部署流程并提高了可靠性。

### Spring Boot Actuator:

Spring Boot Actuator 是 Spring Boot 提供的一个子项目，用于为应用程序添加生产级的监控和管理功能。 当它被集成到应用中后，会自动提供一系列用于监控的 Web 端点（Endpoints）。

- 健康检查 (/health): Actuator 可以提供一个/health 端点，用于检查应用程序当前的运行状态是否正常（例如，数据库连接是否正常、磁盘空间是否充足）。这对于自动化运维系统（如 Kubernetes）判断是否需要重启或替换应用实例至关重要。
- 指标监控 (/metrics): 它还能暴露大量有用的性能指标，如 JVM 内存使用情况、HTTP 请求的吞吐量和响应时间等。这些指标可以被监控系统（如 Prometheus）采集，用于构建仪表盘、分析系统性能瓶颈和设置告警，从而构建一个稳定且易于观察和维护的 AI 服务体系。

## 知识点:

### Spring AI 核心抽象 (ChatClient, EmbeddingClient)

1. 是什么： 这是 Spring AI 的“门面接口”（Facade Pattern）。它将底层不同 LLM 提供商（OpenAI, Azure, Ollama 等）的 API 差异完全屏蔽，为上层应用提供了统一、简洁的调用接口。
2. 为什么用：

   - 避免厂商锁定： 今天用 OpenAI，明天想换成更便宜或更安全的私有化模型，只需修改配置文件和依赖，业务代码一行都不用动。这是企业级架构设计的核心原则。
   - 简化开发： 开发者只需学习一套 API，就能使用所有支持的模型。

3. 面试深挖：

   - Q: "如果现在有一个新的国产大模型，Spring AI 官方还不支持，你想在项目里接入它，该怎么做？"
     - A: 这正是接口抽象的魅力所在。我会自定义一个 MyModelChatClient 类，去实现 Spring AI 提供的 ChatClient 接口。在这个类里面，封装调用国产大模型 HTTP API 的逻辑、处理它的请求和响应格式。然后，通过 Spring 的配置，将这个自定义的 Bean 注册到容器中。上层业务代码就可以像调用 OpenAI 一样，无差别地调用这个新模型了。

### 异步化与消息队列 (RabbitMQ)

1. 是什么：将耗时的 AI 操作（特别是 RAG 和复杂的 Agent 调用）从同步的 HTTP 请求中剥离，通过消息队列异步处理。

2. 为什么用：

   - 提升用户体验： API 可以瞬间响应，而不是让用户长时间等待。前端可以立即返回一个“正在处理中”的状态。
   - 实现流量削峰： 在高并发场景下，将请求堆积在消息队列中，由后端的消费者按照自己的处理能力平稳地消费，防止瞬间流量冲垮整个系统。
   - 增强系统韧性： 如果处理失败（比如 LLM 服务临时不可用），消息可以被重试，保证任务最终被执行。

3. 面试深挖：

   - Q: "用户提交请求后，你怎么通知他异步任务完成了？"
     - A: 有多种方案：1. 轮询（Polling）：API 立即返回一个任务 ID，前端定期携带此 ID 来查询任务状态。简单但有延迟和资源浪费。2. WebSocket：建立长连接，任务完成后由服务器主动推送结果给客户端。实时性最好，但对服务端有一定压力。3. 服务器发送事件（Server-Sent Events, SSE）：也是服务端推送，比 WebSocket 更轻量，非常适合这种单向的“流式输出”场景。在我们的项目中，对于 RAG 这种流式返回的答案，SSE 是最佳选择。

### 容器化与可观测性 (Docker & Actuator)

1. 是什么：使用 Docker 将应用打包成镜像，保证环境一致性。利用 Spring Boot Actuator 暴露应用的健康状况、指标（Metrics）、配置等端点。

2. 为什么用：这是现代云原生应用开发的标配。

   - Docker： 简化部署、弹性伸缩。
   - Actuator： 为监控（Prometheus）、告警（Alertmanager）、分布式追踪（Zipkin）提供了数据基础。对于 AI 应用，我们可以监控 LLM 调用的平均延迟、Token 消耗量、错误率等关键业务指标。

3. 面试深挖：

   - Q: "针对这个 AI 应用，除了常规的 CPU、内存监控，你会特别关注哪些由 Actuator 暴露的自定义指标？"
     - A: 我会通过 Micrometer 定义并暴露以下几个核心业务指标：
       1. lingyi.rag.retrieval.hits：检索器的命中数，可以按有无命中来区分。
       2. lingyi.llm.requests.duration：对不同 LLM（OpenAI, Cohere）调用的 P99 延迟，一旦升高就告警。
       3. lingyi.llm.tokens.usage：统计 Prompt 和 Completion 的 Token 总数，用于成本控制和分析。
       4. lingyi.agent.tool.calls：按工具名称统计调用次数，分析哪些工具最受欢迎或最常被调用。
       5. lingyi.rating.score：如果引入了用户反馈（点赞/点踩），统计好评率，作为模型和系统优化的黄金指标。
