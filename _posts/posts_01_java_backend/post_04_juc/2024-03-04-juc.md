---
title: Java并发编程
date: 2024-03-04 06:00:00 +0800
categories: [Java Backend, JUC]
tags: [JUC]
toc: true
math: true
pin: false
render_with_liquid: false
image:
  path: https://cdn.jsdelivr.net/gh/Zong-Liang/blog_images/blog/2024/java_backend/20251118105316721.png
  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
---

## Java 中创建线程的几种方式？

通常来说，Java 中创建线程主要有四种方式：

1.  继承 `Thread` 类
2.  实现 `Runnable` 接口
3.  实现 `Callable` 接口并结合 `Future`
4.  通过线程池 `ExecutorService` 创建

### 1. 继承 Thread 类

这是最直观的一种方式，通过创建一个继承自 `Thread` 类的子类，并重写其 `run` 方法来定义线程需要执行的任务。

**实现步骤：**

1.  定义一个类，继承自 `java.lang.Thread` 类。
2.  重写 `run()` 方法，将线程需要执行的逻辑代码放入其中。
3.  创建该子类的实例，即创建了线程对象。
4.  调用该实例的 `start()` 方法来启动线程。

```java
/**
 * 方式一：通过继承 Thread 类创建线程
 */
public class MyThread extends Thread {

    @Override
    public void run() {
        System.out.println("1. 继承 Thread 类：当前线程 " + Thread.currentThread().getName() + " 正在执行。");
    }

    public static void main(String[] args) {
        // 创建线程实例
        MyThread myThread = new MyThread();
        // 启动线程
        myThread.start();
    }
}
```

**代码说明：**

- `MyThread` 类继承了 `Thread`。
- `run()` 方法中定义了线程需要执行的任务。
- `main` 方法中，创建 `MyThread` 的实例并调用 `start()` 方法，此时 JVM 会开辟一个新的线程去执行 `run()` 方法中的逻辑。

**优点：**

- 实现简单，可以直接在 `run` 方法中使用 `this` 来获取当前线程的引用。

**缺点：**

- 由于 Java 的单继承特性，如果类已经继承了其他类，就无法再继承 `Thread` 类，这在复杂的应用场景中限制了其灵活性。
- 任务代码（`run` 方法）和线程本身耦合度高，不利于代码的复用。

### 2. 实现 Runnable 接口

这是一种更常用、更推荐的方式。通过实现 `Runnable` 接口，并将任务逻辑放在 `run` 方法中。

**实现步骤：**

1.  定义一个类，实现 `java.lang.Runnable` 接口。
2.  实现 `run()` 方法，编写线程要执行的任务代码。
3.  创建一个 `Thread` 对象，并将实现了 `Runnable` 接口的类的实例作为参数传递给 `Thread` 的构造函数。
4.  调用 `Thread` 对象的 `start()` 方法启动线程。

```java
/**
 * 方式二：通过实现 Runnable 接口创建线程
 */
public class MyRunnable implements Runnable {

    @Override
    public void run() {
        System.out.println("2. 实现 Runnable 接口：当前线程 " + Thread.currentThread().getName() + " 正在执行。");
    }

    public static void main(String[] args) {
        // 创建任务实例
        MyRunnable myRunnable = new MyRunnable();
        // 创建线程实例，并将任务实例作为参数传入
        Thread thread = new Thread(myRunnable);
        // 启动线程
        thread.start();
    }
}
```

**代码说明：**

- `MyRunnable` 类实现了 `Runnable` 接口。
- 线程要执行的任务逻辑写在 `run()` 方法中。
- 在 `main` 方法中，将 `MyRunnable` 的实例作为 `target` 传入 `Thread` 的构造函数。`Thread` 类本身就实现了 `Runnable` 接口，其构造函数可以接收一个 `Runnable` 对象，在 `run` 方法中会去调用 `target` 的 `run` 方法。

**优点：**

- **解耦**：将任务（`Runnable`）与线程（`Thread`）分离开来，实现了更好的面向对象设计。
- **资源共享**：多个线程可以共享同一个 `Runnable` 实例，非常适合处理同一份资源的情况，可以减少内存开销。
- **灵活性**：由于只是实现接口，类还可以继承其他类，不受单继承的限制。
- 与线程池等高级并发工具兼容性好。

**缺点：**

- 编程上比继承 `Thread` 类稍微复杂一些。
- 如果需要访问当前线程，必须使用 `Thread.currentThread()` 方法。

### 3. 实现 Callable 接口并结合 Future

从 Java 1.5 开始，提供了 `Callable` 接口，它弥补了 `Runnable` 接口的两个不足：`run` 方法没有返回值，并且不能抛出受检异常。

**实现步骤：**

1.  创建一个实现了 `java.util.concurrent.Callable` 接口的类。
2.  实现 `call()` 方法，这个方法可以有返回值，并且可以抛出异常。
3.  创建一个 `FutureTask` 对象，用 `Callable` 的实例作为构造参数。
4.  将 `FutureTask` 对象作为参数创建一个 `Thread` 对象并启动。
5.  通过 `FutureTask` 对象的 `get()` 方法来获取线程执行的返回值。`get()` 方法会阻塞，直到任务执行完毕。

```java
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.FutureTask;

/**
 * 方式三：通过实现 Callable 接口和 FutureTask 创建线程
 */
public class MyCallable implements Callable<String> {

    @Override
    public String call() throws Exception {
        System.out.println("3. 实现 Callable 接口：当前线程 " + Thread.currentThread().getName() + " 正在执行...");
        Thread.sleep(2000); // 模拟耗时操作
        return "任务执行完毕，返回结果！";
    }

    public static void main(String[] args) {
        // 创建 Callable 任务实例
        MyCallable myCallable = new MyCallable();
        // 使用 FutureTask 包装 Callable 任务
        FutureTask<String> futureTask = new FutureTask<>(myCallable);

        // 创建线程实例，并将 FutureTask 作为任务
        // FutureTask 本身也实现了 Runnable 接口
        Thread thread = new Thread(futureTask);
        thread.start();

        System.out.println("主线程继续执行其他任务...");

        try {
            // 阻塞主线程，等待子线程执行完毕并获取返回值
            String result = futureTask.get();
            System.out.println("获取到子线程的执行结果: " + result);
        } catch (InterruptedException | ExecutionException e) {
            e.printStackTrace();
        }
    }
}
```

**代码说明：**

- `MyCallable` 实现了 `Callable<String>` 接口，`call()` 方法返回一个 `String` 类型的结果。
- `FutureTask` 是一个关键的类，它实现了 `Runnable` 和 `Future` 接口，既可以作为任务被线程执行，又可以用来获取异步计算的结果。
- `futureTask.get()` 方法会阻塞当前线程，直到 `call()` 方法执行完成并返回结果。

**与 `Runnable` 的区别：**

- `Callable` 的方法是 `call()`，而 `Runnable` 的方法是 `run()`。
- `call()` 方法可以有返回值，而 `run()` 方法没有。
- `call()` 方法可以抛出异常，而 `run()` 方法不能。

**优点：**

- 可以获取线程执行的异步结果。
- 可以向上传递异常。

### 4. 通过线程池 ExecutorService 创建

在实际的生产环境中，我们通常不直接创建 `Thread` 对象，而是使用线程池来管理线程，以提高性能和资源利用率。 `ExecutorService` 是 Java 并发包中的一个核心接口，提供了管理和执行异步任务的框架。

**实现步骤：**

1.  使用 `Executors` 工厂类创建一个 `ExecutorService` 实例（即线程池），例如 `Executors.newFixedThreadPool(10)`。
2.  创建 `Runnable` 或 `Callable` 的实例作为任务。
3.  通过 `ExecutorService` 的 `submit()` 或 `execute()` 方法提交任务。
    - `execute(Runnable)`：提交一个不需要返回值的任务。
    - `submit(Runnable)` 或 `submit(Callable)`：提交一个任务，并返回一个 `Future` 对象，可以通过它来获取结果或处理异常。
4.  任务执行完毕后，调用 `ExecutorService` 的 `shutdown()` 方法关闭线程池。

```java
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * 方式四：通过线程池创建线程
 */
public class ThreadPoolExample {

    public static void main(String[] args) {
        // 1. 创建一个固定大小的线程池
        ExecutorService executorService = Executors.newFixedThreadPool(5);

        try {
            // 2. 提交多个任务到线程池
            for (int i = 0; i < 10; i++) {
                // 创建一个 Runnable 任务
                Runnable task = () -> {
                    System.out.println("4. 线程池：线程 " + Thread.currentThread().getName() + " 正在执行任务。");
                    try {
                        Thread.sleep(1000); // 模拟任务执行
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                };

                // 将任务提交给线程池执行
                executorService.execute(task);
            }
        } finally {
            // 3. 关闭线程池
            // shutdown() 会等待所有已提交的任务执行完毕后再关闭
            executorService.shutdown();
        }

        System.out.println("所有任务已提交，主线程结束。");
    }
}
```

**代码说明：**

- 我们使用 `Executors.newFixedThreadPool(5)` 创建了一个包含 5 个核心线程的线程池。
- 通过循环提交了 10 个任务，线程池会自动调度这 5 个线程去执行这些任务。
- `executorService.execute()` 用于提交一个 `Runnable` 任务。如果需要返回值，可以使用 `submit()` 方法提交 `Callable` 或 `Runnable` 任务。
- 最后必须调用 `shutdown()` 方法来关闭线程池，否则程序不会终止。`shutdown()` 不会立即终止线程，而是让线程池进入关闭状态，不再接受新任务，并等待已提交的任务执行完成。

**优点：**

- **资源管理**：复用线程，减少了频繁创建和销毁线程带来的性能开销。
- **并发控制**：可以有效控制并发线程的数量，防止资源过度消耗导致系统崩溃。
- **功能强大**：提供了更丰富的线程管理功能，如定时执行、任务队列等。

## Java 中线程有哪几种状态？

Java 中的线程在其生命周期中会经历多种状态，这些状态由`java.lang.Thread.State`这个枚举类明确定义。

根据 Java 的官方定义，线程共有六种状态：

1.  **NEW (新建)**
2.  **RUNNABLE (可运行)**
3.  **BLOCKED (阻塞)**
4.  **WAITING (无限期等待)**
5.  **TIMED_WAITING (限期等待)**
6.  **TERMINATED (终止)**

### 1. NEW (新建)

**定义**：当使用 `new` 关键字创建了一个 `Thread` 对象，但还没有调用其 `start()` 方法时，线程就处于这个状态。

- **特点**：此时的线程对象只是一个普通的 Java 对象，JVM 尚未为其分配真正的操作系统线程资源。
- **状态转换**：
  - 调用线程的 `start()` 方法后，状态会从 `NEW` 转换为 `RUNNABLE`。

### 2. RUNNABLE (可运行)

**定义**：这是一个复合状态，它包含了我们通常在操作系统层面理解的“就绪”(Ready) 和“运行中”(Running) 两种状态。

- **特点**：
  - **就绪 (Ready)**：线程调用 `start()` 方法后，进入线程调度队列，等待 CPU 调度器分配时间片。此时它已经具备了运行的条件，但还没真正开始执行。
  - **运行中 (Running)**：线程获得了 CPU 时间片，正在执行 `run()` 方法中的代码。
- **状态转换**：
  - `NEW` -> `RUNNABLE`：调用 `start()` 方法。
  - `RUNNABLE` -> `BLOCKED`: 线程试图获取一个由其他线程持有的 `synchronized` 同步锁。
  - `RUNNABLE` -> `WAITING`: 线程调用 `Object.wait()`, `Thread.join()`, 或 `LockSupport.park()`。
  - `RUNNABLE` -> `TIMED_WAITING`: 线程调用 `Thread.sleep(long)`, `Object.wait(long)`, `Thread.join(long)`, `LockSupport.parkNanos(long)` 或 `LockSupport.parkUntil(long)`。
  - `RUNNABLE` -> `TERMINATED`: 线程的 `run()` 方法执行完毕，或者因未捕获的异常而退出。
  - CPU 时间片用完，线程会从“运行中”切换回“就绪”状态，等待下一次调度。

### 3. BLOCKED (阻塞)

**定义**：线程因为等待获取一个监视器锁（monitor lock）而处于暂停状态。这特指线程在进入 `synchronized` 修饰的代码块或方法时，发现锁被其他线程占用，从而进入的状态。

- **特点**：处于 `BLOCKED` 状态的线程不会占用 CPU 资源。
- **状态转换**：
  - `RUNNABLE` -> `BLOCKED`：线程试图进入一个 `synchronized` 区域，但该区域的锁已被其他线程持有。
  - `BLOCKED` -> `RUNNABLE`：持有锁的线程释放了该锁，并且线程调度器选择了这个线程来获取锁。

### 4. WAITING (无限期等待)

**定义**：线程因为等待其他线程执行特定的动作而进入无限期等待状态。

- **特点**：处于此状态的线程同样不会消耗 CPU。它需要被其他线程显式地唤醒，否则会一直等待下去。
- **导致进入 WAITING 状态的方法调用**：
  - `Object.wait()`: 在一个对象上调用 `wait()` 方法，当前线程会释放该对象的锁并进入等待状态，直到另一个线程在该对象上调用 `notify()` 或 `notifyAll()`。
  - `Thread.join()`: 一个线程 A 调用另一个线程 B 的 `join()` 方法，线程 A 会进入等待状态，直到线程 B 执行完毕。
  - `LockSupport.park()`: 这是 `java.util.concurrent` 包提供的一个底层工具，调用后会使当前线程进入等待状态，直到有其他线程对其调用 `unpark()`。
- **状态转换**：
  - `RUNNABLE` -> `WAITING`: 调用了上述方法之一。
  - `WAITING` -> `RUNNABLE` 或 `BLOCKED`：
    - 等待的线程被其他线程通过 `Object.notify()` 或 `Object.notifyAll()` 唤醒。
    - 调用 `join()` 的线程所等待的目标线程执行完毕。
    - 等待的线程被中断 (`interrupt()`)。
    - 其他线程调用了 `LockSupport.unpark(Thread)`。
    - 被唤醒后，如果需要重新获取锁，它可能会先进入 `BLOCKED` 状态，成功获取锁后才进入 `RUNNABLE`。

### 5. TIMED_WAITING (限期等待)

**定义**：与 `WAITING` 类似，但它不会无限期等待。线程会在指定的时间后自动被唤醒。

- **特点**：在等待期间不消耗 CPU，是处理超时等场景的常用状态。
- **导致进入 TIMED_WAITING 状态的方法调用**：
  - `Thread.sleep(long millis)`: 使当前线程休眠指定的毫秒数。
  - `Object.wait(long timeout)`: 在指定时间内等待 `notify()` 或 `notifyAll()`，超时后自动唤醒。
  - `Thread.join(long millis)`: 限时等待目标线程执行完毕。
  - `LockSupport.parkNanos(long nanos)` / `LockSupport.parkUntil(long deadline)`: 限时版本的 `park`。
- **状态转换**：
  - `RUNNABLE` -> `TIMED_WAITING`: 调用了上述方法之一。
  - `TIMED_WAITING` -> `RUNNABLE` 或 `BLOCKED`：
    - 等待时间结束。
    - 在等待时间内被 `notify()` / `notifyAll()` / `unpark()` 唤醒或被中断。
    - 和 `WAITING` 状态一样，唤醒后如果需要竞争锁，可能会先进入 `BLOCKED` 状态。

### 6. TERMINATED (终止)

**定义**：线程的 `run()` 方法已经执行完成，或者因为一个未被捕获的异常而提前退出，线程的生命周期结束。

- **特点**：线程已经完全停止执行，所有资源已被释放。这是一个最终状态，无法再转换为任何其他状态。
- **状态转换**：
  - `RUNNABLE` -> `TERMINATED`: 线程正常执行完 `run()` 方法或因异常退出。

### 状态总结与辨析

- **Blocked vs. Waiting/Timed_Waiting**: 这是一个常见的面试重点。
  - `BLOCKED` 是被动的，是线程在尝试获取 `synchronized` 锁失败时，由 JVM 控制进入的状态。
  - `WAITING` 和 `TIMED_WAITING` 是主动的，是线程自己调用了 `wait()`, `join()`, `park()` 等方法进入的状态。
  - 进入 `BLOCKED` 状态的线程只等待 `synchronized` 锁的释放。而进入 `WAITING/TIMED_WAITING` 的线程等待的是一个特定的“通知”（`notify`）或时间的到来。

## Java 中线程有哪调度方法？

首先，必须明确一个核心概念：Java 的线程调度模型是抢占式的，并且依赖于底层操作系统的线程调度器。我们无法用 Java 代码编写一个自己的调度器来替代操作系统，但 Java 提供了一些方法，可以向调度器发出“建议”或“请求”，从而间接地影响线程的执行顺序和时机。

这些方法主要可以分为以下几类：

### 1. 线程优先级 (Thread Priority)

Java 允许我们为每个线程设置一个优先级，理论上，优先级越高的线程越有可能被调度器选中来执行。

- **相关方法**: `void setPriority(int newPriority)` 和 `int getPriority()`。
- **优先级范围**: 线程的优先级用整数表示，范围是 1 到 10。`Thread` 类定义了三个常量：
  - `Thread.MIN_PRIORITY` (值为 1)
  - `Thread.NORM_PRIORITY` (值为 5，默认优先级)
  - `Thread.MAX_PRIORITY` (值为 10)
- **工作机制**: 当有多个线程处于 `RUNNABLE` 状态时，线程调度器倾向于选择优先级更高的线程。
- **重要提醒**:
  - **只是一个“提示”**：设置优先级**并不能保证**高优先级的线程一定先于低优先级的线程执行。这仅仅是对操作系统调度器的一个建议。
  - **平台依赖性**: 不同的操作系统对线程优先级的支持和映射是不同的。例如，Windows 有 32 个优先级，而 Linux 的实现则更为复杂，并不直接映射 Java 的 10 个级别。因此，过度依赖线程优先级来控制程序逻辑，会使程序变得不可移植且行为不稳定。
  - **优先级反转**: 在某些情况下，可能会出现低优先级的线程持有了一个高优先级线程所需要的锁，导致高优先级线程反而长时间等待，这就是所谓的“优先级反转”。

**使用场景**: 很少在业务代码中使用。它的行为不稳定，不应作为控制并发流程的手段。

### 2. 线程休眠 (Thread.sleep)

`sleep()` 方法可以让当前正在执行的线程暂停执行一段指定的时间。

- **相关方法**: `static void sleep(long millis)` 和 `static void sleep(long millis, int nanos)`。
- **工作机制**: 调用 `sleep()` 会使当前线程从 `RUNNABLE` 状态转换到 `TIMED_WAITING` 状态。在这段时间内，线程不会参与 CPU 的调度。当设定的时间结束后，线程会重新回到 `RUNNABLE` 状态，等待调度器再次分配 CPU 时间片。
- **重要提醒**:
  - **不释放锁**: 这是一个**非常重要**的知识点。如果一个线程在一个 `synchronized` 同步块中调用了 `sleep()` 方法，它**不会释放**它所持有的监视器锁（monitor lock）。这意味着其他需要这个锁的线程仍然会处于 `BLOCKED` 状态，无法执行。
  - **时间不精确**: `sleep` 的时间是一个“至少”的时间。实际暂停的时间可能会略长于指定的时间，这取决于操作系统的调度精度。
  - **会抛出 `InterruptedException`**: 如果线程在 `sleep` 期间被其他线程调用了 `interrupt()` 方法，`sleep` 会立即中止并抛出 `InterruptedException` 异常。

**使用场景**: 需要暂停当前线程的执行，例如在循环中进行轮询检查时，为了避免 CPU 空转，可以在每次轮询后 `sleep` 一小段时间。

### 3. 线程让步 (Thread.yield)

`yield()` 方法是一个“建议性”的方法，它暗示调度器当前线程愿意放弃其当前的 CPU 使用。

- **相关方法**: `static void yield()`。
- **工作机制**: 调用 `yield()` 会使当前线程从“运行中”状态回到“就绪”状态（仍在 `RUNNABLE` 状态内），从而让调度器有机会选择其他同优先级的线程来执行。
- **重要提醒**:
  - **非常弱的保证**: `yield()` 的行为完全取决于调度器的实现。调度器完全可以忽略这个“建议”，让刚刚 `yield` 的线程立刻重新获得 CPU 并继续执行。
  - **不释放锁**: 和 `sleep()` 一样，`yield()` **不会释放锁**。

**使用场景**: 极少使用。它通常用于调试或性能测试的某些特定场景，在大多数并发应用中，我们不应该依赖 `yield()` 来控制程序逻辑。

### 4. 线程等待与通知 (wait / notify / notifyAll)

这一组方法是基于 `Object` 类的，用于实现线程间的协作和通信，是一种更底层的协调机制。

- **相关方法**: `final void wait()`, `final void wait(long timeout)`, `final void notify()`, `final void notifyAll()`。
- **工作机制**:
  - 一个线程在获得了某个对象（比如 `obj`）的锁之后，可以调用 `obj.wait()`。
  - 调用后，该线程会**释放 `obj` 的锁**，并进入 `WAITING` 或 `TIMED_WAITING` 状态，等待被其他线程唤醒。
  - 另一个线程在获得了 `obj` 的锁之后，可以调用 `obj.notify()`（唤醒一个在 `obj` 上等待的线程）或 `obj.notifyAll()`（唤醒所有在 `obj` 上等待的线程）。
  - 被唤醒的线程会从 `WAITING` 状态进入 `BLOCKED` 状态，尝试重新获取 `obj` 的锁，成功后才能继续执行。
- **重要提醒**:
  - **必须在同步块中使用**: 这组方法必须在 `synchronized` 代码块或方法中调用，否则会抛出 `IllegalMonitorStateException`。
  - **`wait()` 释放锁**: 这是 `wait()` 和 `sleep()`/`yield()` 的本质区别。`wait()` 的目的就是为了让出锁，让其他线程有机会进入临界区并改变条件。

**使用场景**: 实现经典的“生产者-消费者”模式，或者任何需要基于特定条件进行线程间协作的场景。

### 5. 线程加入 (Thread.join)

`join()` 方法允许一个线程等待另一个线程执行完成。

- **相关方法**: `final void join()`, `final void join(long millis)`。
- **工作机制**: 假设在线程 A 的代码中调用了 `threadB.join()`，那么线程 A 会进入 `WAITING` 或 `TIMED_WAITING` 状态，直到线程 B 执行完毕（进入 `TERMINATED` 状态），线程 A 才会从 `join()` 方法返回并继续执行。
- **使用场景**: 当一个任务的执行需要依赖另一个任务的结果或完成时。例如，主线程需要等待所有子任务线程都计算完毕后，才能汇总结果。

### 总结

| 方法            | 所属类   | 是否释放锁 | 线程状态变化                                       | 主要作用                   |
| :-------------- | :------- | :--------- | :------------------------------------------------- | :------------------------- |
| `setPriority()` | `Thread` | 不涉及     | 不变                                               | 建议调度器优先调度，不稳定 |
| `sleep()`       | `Thread` | **不释放** | `RUNNABLE` -> `TIMED_WAITING` -> `RUNNABLE`        | 暂停执行，不参与 CPU 调度  |
| `yield()`       | `Thread` | **不释放** | `RUNNABLE` (运行中 -> 就绪)                        | 建议让出 CPU，非常弱的保证 |
| `wait()`        | `Object` | **释放**   | `RUNNABLE` -> `WAITING` -> `BLOCKED` -> `RUNNABLE` | 线程协作，等待特定条件满足 |
| `join()`        | `Thread` | 不涉及     | `RUNNABLE` -> `WAITING` -> `RUNNABLE`              | 等待目标线程执行完毕       |

在现代 Java 并发编程中，我们更倾向于使用 `java.util.concurrent` 包提供的高级工具，如 `Executors` 线程池、`CountDownLatch`、`Semaphore`、`ReentrantLock` 和 `Condition` 等。这些工具封装了底层的 `wait/notify` 等机制，提供了更安全、更强大、更易于使用的 API 来协调和调度线程。

## Java 中的线程上下文切换？

### 1. 什么是线程上下文切换？

首先，我们可以把“上下文”理解为线程在执行过程中所需要的所有状态信息。这包括：

- **程序计数器（Program Counter）**：记录了线程当前正在执行的字节码指令的地址。当线程被切换回来时，就知道从哪里继续执行。
- **虚拟机栈（VM Stack）**：每个线程都有一个私有的 Java 虚拟机栈，里面存放着一个个栈帧（Stack Frame），对应着每次方法调用的信息，包括局部变量表、操作数栈、动态链接、方法出口等。
- **本地方法栈（Native Method Stack）**：与虚拟机栈类似，但服务于本地（Native）方法。
- **寄存器（CPU Registers）**：CPU 内部的高速存储单元，包含了当前线程执行所需的数据，例如栈指针、程序计数器等。
- **其他状态信息**：例如线程的优先级、线程状态（如 Running, Blocked, Waiting 等）、锁信息等。

**线程上下文切换**，简单来说，就是 CPU 暂停当前正在执行的线程，保存这个线程的完整“上下文”信息，然后加载另一个线程的上下文信息，并开始执行这个新的线程。这个“保存-加载”的过程就是一次上下文切换。

### 2. 为什么会发生线程上下文切换？

线程上下文切换是操作系统内核（OS Kernel）来调度的，对于上层 Java 程序来说是透明的。触发上下文切换的常见原因主要有以下几种：

- **CPU 时间片用完（Time Slicing）**：在现代分时操作系统中，为了保证公平性，每个线程会被分配一个很短的时间片（比如几十毫秒）。当线程的时间片用完后，操作系统会强制剥夺其 CPU 使用权，切换到另一个处于就绪（Runnable）状态的线程。这是最常见的上下文切换原因。
- **线程主动让出 CPU**：
  - **`Thread.sleep()`**: 线程调用`sleep()`方法，会进入`TIMED_WAITING`状态，主动放弃 CPU，直到睡眠时间结束。
  - **`Object.wait()`**: 线程调用了某个对象的`wait()`方法，会进入`WAITING`或`TIMED_WAITING`状态，并释放该对象的锁，等待被`notify()`或`notifyAll()`唤醒。
  - **`Thread.yield()`**: 这是一个“建议性”的方法，它暗示调度器当前线程愿意放弃 CPU，但调度器可以忽略这个建议。如果调度器采纳，就会发生切换。
- **线程阻塞（Blocking）**：当线程执行到需要等待某个条件的 I/O 操作（如等待网络数据、读取磁盘文件）或者尝试获取一个已经被其他线程占用的锁时，它会从`RUNNABLE`状态转换到`BLOCKED`或`WAITING`状态，从而触发上下文切换。
- **线程终结或异常**：线程执行完毕（`run`方法执行结束）或者在执行过程中抛出未捕获的异常，都会导致线程终止，从而触发调度，切换到其他线程。
- **更高优先级的线程抢占**：当一个更高优先级的线程进入就绪状态时，操作系统可能会剥夺当前正在运行的低优先级线程的 CPU，把 CPU 分配给高优先级的线程，这被称为“抢占式调度”。

### 3. 上下文切换的开销

上下文切换虽然是实现并发的必要手段，但它并非没有代价。其开销主要体现在两个方面：

1.  **直接开销**：

    - **CPU 时间的消耗**：保存和恢复上下文信息（如寄存器状态、程序计数器等）本身就需要消耗 CPU 指令周期。这个过程完全是内核态操作，需要从用户态切换到内核态再切回用户态，这个切换本身也有开销。

2.  **间接开销**：
    - **CPU 缓存失效**：当一个新线程被加载到 CPU 上时，它所需要的数据很可能不在 CPU 的 L1、L2、L3 缓存中。CPU 需要重新从主内存（RAM）去加载数据，这个过程比直接从缓存中读取要慢几个数量级。 这被称为“缓存冷启动”（Cache Cold Start），是上下文切换带来的最大性能损耗之一。
    - **TLB（Translation Lookaside Buffer）失效**：TLB 是用于加速虚拟地址到物理地址转换的缓存。上下文切换可能会导致 TLB 中的缓存失效，使得地址转换变慢。

频繁且不必要的上下文切换，会大量消耗 CPU 资源，导致系统的整体性能下降，这也是我们在进行并发编程时需要重点关注和优化的问题。

### 4. 如何排查和减少上下文切换？

在实际的 Java 后端开发中，我们可以使用一些工具来监控上下文切换的情况，并采取相应措施进行优化。

- **监控和诊断**：

  - **`vmstat`命令 (Linux)**：`vmstat`是一个常用的 Linux 系统性能分析工具，其输出的`cs`（context switch）列可以显示每秒的上下文切换次数。如果这个数值异常高，就需要警惕了。
  - **`pidstat`命令 (Linux)**：`pidstat -w`可以查看特定 Java 进程的自愿（cswch/s）和非自愿（nvcswch/s）上下文切换次数。非自愿切换次数过多，通常意味着 CPU 资源紧张；自愿切换次数过多，则可能意味着线程在等待资源（如锁、I/O）。
  - **JMX（Java Management Extensions）**：通过 JMX MBeans 可以获取到 JVM 内部的线程状态信息，但通常不直接提供上下文切换次数。
  - **JFR（JDK Flight Recorder）和 JMC（JDK Mission Control）**：这是 JVM 自带的强大性能分析工具，可以记录到非常详细的事件，包括线程的上下文切换事件，并进行可视化分析，是排查这类问题的利器。

- **优化策略**：
  - **合理的线程池大小**：线程并非越多越好。过多的线程会导致大量的线程在就绪状态等待 CPU 时间片，从而引发频繁的上下文切换。线程池的大小应该根据应用的类型（CPU 密集型还是 I/O 密集型）和硬件配置（CPU 核心数）来综合设定。对于 CPU 密集型任务，线程数通常设置为 CPU 核心数+1；对于 I/O 密集型任务，由于线程会大量时间处于等待状态，可以设置更多的线程数，例如 `CPU核心数 * (1 + 平均等待时间 / 平均计算时间)`。
  - **减少锁竞争**：高并发场景下，激烈的锁竞争是导致上下文切换的元凶。当一个线程无法获取锁时，它会被挂起，触发上下文切换。可以通过以下方式减少锁竞争：
    - **使用无锁数据结构**：如`ConcurrentHashMap`、`ConcurrentLinkedQueue`等 JUC 包下的原子类和并发容器。
    - **减少锁的粒度**：将一个大锁拆分成多个小锁，例如使用`ConcurrentHashMap`的分段锁思想。
    - **使用读写锁（`ReadWriteLock`）**：在读多写少的场景下，允许多个读线程同时访问，提高并发性。
    - **使用 CAS（Compare-And-Swap）算法**：像`AtomicInteger`等原子类就是基于 CAS 实现的，它是一种乐观锁机制，避免了线程的阻塞和唤醒。
  - **使用协程（Virtual Threads）**：从 Java 19 开始引入的虚拟线程（Project Loom）是一个重要的进步。虚拟线程的切换不涉及操作系统内核调度，而是在 JVM 层面进行，其上下文切换的开销比传统的平台线程（Platform Threads）小得多。对于大量 I/O 密集型任务的场景，使用虚拟线程可以极大地减少操作系统层面的上下文切换，从而提升系统吞吐量。

## Java 中的线程通信方式？

关于 Java 中的线程通信方式，这是一个并发编程中的核心问题，其本质是解决线程之间如何协作、如何传递数据以及如何同步状态。

### 1. 传统的 `synchronized` 结合 `wait()`, `notify()`, `notifyAll()`

这是 Java 语言最基础、最经典的线程通信机制，它们都属于 `java.lang.Object` 类的方法，因此任何对象都可以作为通信的载体。

- **核心机制**：这套机制依赖于对象的内置锁（也称为监视器锁或 Intrinsic Lock）。一个线程必须首先获得对象的锁，才能调用该对象的 `wait()`, `notify()` 或 `notifyAll()` 方法。

  - **`wait()`**: 当线程 A 调用一个对象 `obj` 的 `wait()` 方法时，它会立即释放 `obj` 的锁，并进入该对象的等待队列（Wait Set）中，线程状态变为 `WAITING` 或 `TIMED_WAITING`。此时，其他线程就有机会获取 `obj` 的锁。
  - **`notify()`**: 当线程 B 获得了 `obj` 的锁，并调用 `obj.notify()` 时，它会从 `obj` 的等待队列中唤醒**一个**正在等待的线程（具体唤醒哪一个是不确定的）。被唤醒的线程并不会立即执行，而是进入锁的竞争队列（Entry Set），只有当它重新获得了 `obj` 的锁之后，才能从 `wait()` 的地方继续执行。
  - **`notifyAll()`**: 与 `notify()` 类似，但它会唤醒等待队列中的**所有**线程，让它们一起去竞争锁。

- **关键点**：

  1.  **必须在同步块中使用**：`wait()`, `notify()`, `notifyAll()` 都必须在 `synchronized` 代码块或方法中调用，否则会抛出 `IllegalMonitorStateException`。
  2.  **`while` 循环判断**：为了防止“虚假唤醒”（Spurious Wakeup），官方强烈建议将 `wait()` 调用放在一个 `while` 循环中，而不是 `if` 语句中。被唤醒后，线程需要再次检查条件是否满足，如果不满足则继续 `wait()`。
  3.  **释放锁**：`wait()` 的一个关键特性是它会**释放锁**，而 `Thread.sleep()` 和 `Thread.yield()` 等方法是不会释放锁的。

- **使用场景**：最典型的就是实现“生产者-消费者”模型。

### 2. 使用 `volatile` 关键字

`volatile` 是一种轻量级的同步机制，它主要用于实现线程间的**可见性**。

- **核心机制**：

  1.  **保证可见性**：当一个线程修改了一个 `volatile` 变量的值，这个新值对其他线程是立即可见的。JVM 会确保每次读取 `volatile` 变量时，都直接从主内存中读取，而不是使用 CPU 缓存；每次写入时，都立即将新值写回主内存。
  2.  **禁止指令重排序**：`volatile` 可以在一定程度上防止编译器和处理器对指令进行重排序，从而保证了代码执行的有序性。

- **如何通信**：通过一个共享的 `volatile` 状态变量。例如，一个线程通过修改一个 `volatile boolean flag = true;` 来通知另一个正在循环检查此标志的线程停止工作。

- **局限性**：`volatile` 只保证可见性和有序性，**不保证原子性**。对于像 `count++` 这样的复合操作，它不是线程安全的。因此，`volatile` 适用于“一写多读”且写操作不依赖于当前值的场景。

### 3. 使用 `java.util.concurrent` (JUC) 包中的工具

自 Java 5 以来，JUC 包提供了更高级、更强大、更易用的线程通信工具。

- **a. `Lock` 和 `Condition`**

  - **定位**：这是对 `synchronized` 和 `wait/notify` 机制的增强。`Lock` 接口提供了比 `synchronized` 更灵活的锁操作，而 `Condition` 接口则提供了更精细的线程等待与唤醒控制。
  - **优势**：
    1.  一个 `Lock` 对象可以关联多个 `Condition` 对象，可以实现对不同条件的线程进行分组等待和选择性唤醒，这是 `notifyAll()` 无法做到的。
    2.  `Lock` 提供了非阻塞的 `tryLock()` 和可中断的 `lockInterruptibly()` 等更丰富的锁获取方式。
  - **用法**：`Condition` 的 `await()`、`signal()`、`signalAll()` 方法分别对应 `Object` 的 `wait()`、`notify()`、`notifyAll()`，但它们提供了更高的灵活性和安全性。

- **b. `BlockingQueue` (阻塞队列)**

  - **定位**：这是实现“生产者-消费者”模式的**最佳实践**。它是一个高级抽象，封装了所有底层的同步和通信细节。
  - **核心机制**：
    - 当生产者向一个已满的队列中添加元素时（`put()`），它会被阻塞，直到队列有空间。
    - 当消费者从一个空的队列中获取元素时（`take()`），它会被阻塞，直到队列中有元素。
  - **实现**：JUC 包提供了多种阻塞队列实现，如 `ArrayBlockingQueue` (基于数组的有界队列)、`LinkedBlockingQueue` (基于链表的、容量可选的队列)、`SynchronousQueue` (不存储元素的队列，用于直接传递) 等。开发者完全不需要关心底层的 `wait/notify` 或 `lock/condition`。

- **c. 并发同步器 (Synchronizers)**
  - **`CountDownLatch` (倒计时门闩)**：允许一个或多个线程等待其他一组线程完成操作。它维护一个计数器，线程调用 `countDown()` 使计数器减一，调用 `await()` 的线程会被阻塞，直到计数器变为零。它是一次性的，不能重置。
  - **`CyclicBarrier` (循环栅栏)**：让一组线程到达一个屏障点时被阻塞，直到最后一个线程到达屏障点，屏障才会打开，所有被屏障拦截的线程才会继续执行。它之所以是“循环”的，是因为在释放等待线程后可以重用。
  - **`Semaphore` (信号量)**：用于控制同时访问某个特定资源的线程数量。通过 `acquire()` 获取一个许可，如果没有许可可用，线程将阻塞；通过 `release()` 释放一个许可。可以用于实现资源池或者流量控制。

### 4. 使用管道流 (Piped Streams)

`PipedInputStream` 和 `PipedOutputStream` 也可以用于线程间通信。

- **核心机制**：一个线程的 `PipedOutputStream` 可以和另一个线程的 `PipedInputStream` 相关联。一个线程通过输出流写入的数据，可以被另一个线程从输入流中读取。
- **通信方式**：这是一种基于字节流的通信方式。
- **使用场景**：虽然可用，但在实际开发中比较少见。因为它的实现涉及到底层的 I/O 和锁，效率不高，且不如 JUC 工具方便。

### 总结

总的来说，Java 的线程通信方式经历了从底层、复杂到高级、易用的演进：

- **`wait/notify`** 是基础和经典，必须深入理解其原理和陷阱。
- **`volatile`** 是轻量级的可见性保障，适用于特定的简单场景。
- **JUC 包** 是现代 Java 并发编程的**首选**。它提供了各种高级抽象：
  - 对于复杂的同步控制，使用 `Lock` 和 `Condition`。
  - 对于生产者-消费者模型，直接使用 `BlockingQueue`。
  - 对于多线程协作完成任务的场景，根据需求选择 `CountDownLatch`、`CyclicBarrier` 或 `Semaphore`。

## 在 Java 中如何保证线程安全？

要保证线程安全，本质上就是要正确地管理**共享的、可变的状态**。如果一个状态不是共享的，或者共享但不可变，那么它天然就是线程安全的。

当共享和可变这两个条件同时满足时，我们就需要采用一系列的手段来保证线程安全。

### 层面一：不共享状态（Stateless or Confinement）

这是实现线程安全最简单、最优雅的方式——从根本上避免问题的发生。如果多个线程之间没有共享数据，那么它们之间就不会有冲突。

1.  **无状态对象（Stateless Objects）**：

    - 对象内部不包含任何成员变量（实例变量），或者成员变量是不可变的（`final`）。这样的对象在多线程环境下是天然安全的，因为它们不存储任何随操作而变化的状态。典型的例子就是 Servlet 或 Spring MVC 中的 Controller，它们通常被设计为单例但无状态的，只包含方法和依赖注入的服务，不包含请求相关的状态数据。

2.  **线程封闭（Thread Confinement / Thread Local）**：
    - **栈封闭（Stack Confinement）**: 这是最简单的线程封闭。方法的局部变量存储在各自线程的虚拟机栈中，是线程私有的，因此永远不会被其他线程访问到。只要我们能确保对象只在方法内部作为局部变量使用，不对外发布（比如作为返回值或传递给其他会保存它的对象），那么它就是线程安全的。
    - **`ThreadLocal` 类**: 这是一个非常强大的工具，用于实现线程封闭。当你创建一个 `ThreadLocal` 变量时，每个线程都会访问到这个变量的独立副本，它们之间互不影响。`ThreadLocal` 为每个线程维护了一个独立的存储空间。
      - **适用场景**：最常见的场景是用来传递贯穿整个调用链路的上下文信息，比如用户信息、事务 ID、数据库连接等。这样可以避免在每个方法参数中都显式传递这些信息。
      - **注意事项**：在使用线程池时，由于线程会被复用，`ThreadLocal` 的值可能会被带到下一个任务中，造成数据混乱。因此，必须在任务执行完毕后，在 `finally` 块中调用 `remove()` 方法清理，以防止内存泄漏和逻辑错误。

### 层面二：不可变（Immutability）

如果共享的数据是不可变的，那么它也是天然线程安全的。因为所有线程只能读取它，不能修改它，所以永远不会产生数据冲突。

- **如何创建不可变对象**：
  1.  类声明为 `final`，防止被继承。
  2.  所有成员变量声明为 `private` 和 `final`。
  3.  不提供任何可以修改状态的 `setter` 方法。
  4.  构造器完成所有成员变量的初始化。
  5.  如果成员变量是可变对象（如 `Date`, `List`），则在构造器和 `getter` 方法中进行“防御性拷贝”（Defensive Copying），确保外部无法通过引用修改内部状态。
- **Java 中的例子**：`String` 类、`Integer` 等基本类型的包装类，以及 `java.time` 包下的所有类都是不可变的经典范例。

**总结层面一和二**：优先考虑使用无状态、线程封闭和不可变性来保证线程安全，因为这种方式没有锁的开销，性能最好，也最不容易出错。

### 层面三：使用同步机制（Synchronization）

当状态必须是共享且可变的时候，我们就必须使用同步机制来确保在同一时刻只有一个线程可以访问和修改这个共享状态。这是最常见也是最核心的保证线程安全的方式。

1.  **`synchronized` 关键字**：

    - 这是 Java 提供的内置锁（Intrinsic Lock 或 Monitor Lock）。它可以修饰方法或代码块。
    - **原理**：`synchronized` 保证了“原子性”和“可见性”。当一个线程进入一个 `synchronized` 保护的代码块时，它会获得锁；其他线程尝试进入时会被阻塞。当线程退出时，会释放锁。JVM 规范要求在解锁之前，必须将该线程对共享变量的所有修改都刷新回主内存，而在加锁时，会清空工作内存，强制从主内存加载最新的值。
    - **优点**：使用简单，是 JVM 级别的实现，经过了大量优化（如偏向锁、轻量级锁、自旋锁），不容易出错。

2.  **`java.util.concurrent.locks.Lock` 接口**：

    - 这是 JUC 包提供的更灵活的锁机制，最常用的实现是 `ReentrantLock`。
    - **与 `synchronized` 的对比**：
      - **功能更强**：`Lock` 提供了 `synchronized` 不具备的功能，如：
        - **可中断的获取锁** (`lockInterruptibly()`)：等待锁的线程可以响应中断。
        - **可超时的获取锁** (`tryLock(long time, TimeUnit unit)`)：在指定时间内尝试获取锁，避免死等。
        - **非阻塞的获取锁** (`tryLock()`)：立即返回，不管是否能获取到锁。
        - **公平锁/非公平锁**：可以在构造时指定。
      - **使用方式**：`Lock` 需要手动加锁和解锁。为了保证锁一定会被释放，必须将 `unlock()` 操作放在 `finally` 代码块中。
    - **选择**：除非你需要 `Lock` 提供的那些高级功能，否则优先推荐使用更简单、更安全的 `synchronized`。

3.  **`volatile` 关键字**：
    - 这是一个轻量级的同步机制，它只保证“可见性”和一定程度的“有序性”，**但不保证“原子性”**。
    - 当一个变量被声明为 `volatile`，线程对它的写操作会立刻刷新到主内存，而读操作会直接从主内存读取。这确保了所有线程看到的都是该变量的最新值。
    - **适用场景**：适用于“一写多读”或者写入值不依赖于当前值的场景。例如，一个布尔标志位，由一个线程写入，其他线程读取并根据它来决定是否终止循环。

### 层面四：使用原子类和并发容器

Java 的 JUC 包提供了一系列专门为并发设计的工具类，它们内部已经封装了复杂的同步逻辑，是保证线程安全的首选高级工具。

1.  **原子类 (`java.util.concurrent.atomic`)**：

    - 这个包下提供了一系列的原子操作类，如 `AtomicInteger`, `AtomicLong`, `AtomicReference` 等。
    - **核心原理**：它们底层利用了处理器的 CAS（Compare-And-Swap，比较并交换）指令。这是一个乐观锁的实现，它在不加锁的情况下尝试更新变量，如果发现值被其他线程修改了，就重试，直到成功。
    - **优点**：在高并发下，其性能通常优于基于锁的同步机制，因为它避免了线程的阻塞和上下文切换。

2.  **并发容器 (`java.util.concurrent`)**：
    - JUC 包提供了大量线程安全的容器，它们的性能远超于使用 `Collections.synchronizedXXX()` 包装的传统容器。
    - `ConcurrentHashMap`：一个高性能的线程安全的哈希表。它使用分段锁或更先进的 CAS+`synchronized`技术，允许多个线程同时进行读写操作，并发度非常高。
    - `CopyOnWriteArrayList` / `CopyOnWriteArraySet`：适用于“读多写少”的场景。读操作完全不加锁，性能极高。写操作（add, set, remove）时，它会复制一份底层数组，在新数组上进行修改，然后将引用指向新数组。写操作成本较高，但保证了读操作的无锁并发。
    - `BlockingQueue` (阻塞队列)：如 `ArrayBlockingQueue`, `LinkedBlockingQueue` 等，它们是线程安全的，并且提供了阻塞的 `put()` 和 `take()` 方法，是实现生产者-消费者模式的利器。

### 总结

1.  **优先考虑不可变、无状态和线程封闭**，从设计上规避线程安全问题。
2.  如果必须共享可变状态，**优先使用 JUC 包提供的并发容器和原子类**，例如用 `ConcurrentHashMap` 代替 `HashMap`，用 `AtomicInteger` 代替 `int` 做计数器。
3.  如果业务逻辑非常复杂，上述工具无法满足，才需要**使用 `synchronized` 或 `Lock` 进行显式同步**。在这种情况下，优先选择更简单、不易出错的 `synchronized`，只有在需要其高级特性时才使用 `Lock`。

通过合理地运用这些策略，我们可以在保证程序正确性的前提下，最大化程序的并发性能。

## Java 中的 ThreadLocal 类？

### 1. 什么是 ThreadLocal？

`ThreadLocal` 从字面上看是“线程局部变量”，但更准确的描述应该是：**它提供了一种创建变量副本的机制，每个线程都可以独立地访问和修改自己的那份副本，而不会影响其他线程的副本**。

简单来说，`ThreadLocal` 就像一个保险箱的管理员。你（一个线程）可以向管理员存取一个属于你自己的东西（变量值），其他线程也可以向管理员存取它们自己的东西。管理员能够清楚地记得哪个东西是哪个线程的，确保你只能拿到你自己的，而拿不到别人的。这样，即使这个“管理员”（`ThreadLocal` 实例）是共享的，但它所管理的数据在线程之间是隔离的。

这就从根本上解决了多线程环境下共享变量的并发访问问题，因为它把共享变量变成了“线程私有”的变量，是一种“**以空间换时间**”的思路，避免了使用锁带来的性能开销。

### 2. ThreadLocal 的核心原理与源码结构

要理解 `ThreadLocal` 的工作原理，必须先了解三个核心组件：`Thread`、`ThreadLocal` 和 `ThreadLocalMap`。

- **`Thread` 类**：每个 `Thread` 对象内部都有一个成员变量 `threadLocals`，它的类型是 `ThreadLocal.ThreadLocalMap`。这个 `threadLocals` 变量默认是 `null`，只有在线程第一次调用 `ThreadLocal` 的 `set()` 或 `get()` 方法时才会被创建。
- **`ThreadLocalMap` 类**：这是 `ThreadLocal` 的一个静态内部类，它才是真正存储数据的地方。它类似于一个 `HashMap`，但其设计专为 `ThreadLocal` 服务。
  - 它的 **Key** 是 `ThreadLocal` 对象本身（更准确地说是它的弱引用）。
  - 它的 **Value** 就是我们想要存储的线程局部变量的值。
- **`ThreadLocal` 类**：`ThreadLocal` 实例本身并不存储任何数据，它只作为一个“钥匙”（Key），用来在当前线程的 `ThreadLocalMap` 中存取数据。

**工作流程可以总结如下**：

1.  **`set(T value)` 方法**：

    - 首先，获取当前线程 `Thread.currentThread()`。
    - 然后，通过当前线程获取其内部的 `ThreadLocalMap` 对象 `t.threadLocals`。
    - 如果 `map` 存在，就以当前的 `ThreadLocal` 实例为 Key，要设置的 `value` 为 Value，调用 `map.set(this, value)` 方法存入数据。
    - 如果 `map` 不存在（即该线程第一次使用 `ThreadLocal`），就为该线程创建一个新的 `ThreadLocalMap` 并存入数据。

2.  **`get()` 方法**：
    - 同样，先获取当前线程及其 `ThreadLocalMap`。
    - 如果 `map` 存在，就以当前的 `ThreadLocal` 实例为 Key，从中获取对应的 `Entry`，并返回其 `value`。
    - 如果 `map` 不存在，或者 `map` 中没有这个 Key，它会返回一个初始值。这个初始值是通过调用 `initialValue()` 方法得到的（该方法默认返回 `null`，我们可以通过继承 `ThreadLocal` 并重写该方法来提供自定义的初始值）。

**关键设计点：弱引用（WeakReference）**

`ThreadLocalMap` 中的 `Entry` 继承了 `WeakReference`。它的 Key，也就是 `ThreadLocal` 实例，是被弱引用持有的。

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;

    Entry(ThreadLocal<?> k, Object v) {
        super(k); // k (ThreadLocal实例) 被弱引用持有
        value = v;
    }
}
```

- **为什么用弱引用？**
  这是为了防止内存泄漏。如果 Key（`ThreadLocal` 实例）是强引用，那么即使外部代码已经不再持有 `ThreadLocal` 实例的引用（比如 `tl = null;`），在线程生命周期内，`ThreadLocalMap` 依然会强引用着这个 `ThreadLocal` 实例，导致它无法被垃圾回收（GC），从而引发内存泄漏。
  使用弱引用后，当 `ThreadLocal` 实例在外部没有强引用时，下一次 GC 发生时，这个弱引用就会被回收，`Entry` 中的 Key 就会变为 `null`。

### 3. ThreadLocal 的典型应用场景

`ThreadLocal` 主要用于在同一个线程的执行周期内，跨越多个方法或组件传递共享数据，而无需通过方法参数层层传递。

1.  **数据库连接管理**：在服务层（Service）打开数据库连接，在数据访问层（DAO）使用它，最后在服务层关闭。通过将 `Connection` 对象存入 `ThreadLocal`，可以保证在同一个线程处理的请求中，从头到尾使用的都是同一个数据库连接，从而方便地管理事务（提交或回滚）。许多持久化框架如 Hibernate、MyBatis 都使用这种机制来管理 Session 或 Connection。

2.  **用户身份信息传递**：在 Web 应用中，用户的身份信息（如 UserID、Session）通常在请求开始时（例如在一个 Filter 或 Interceptor 中）被解析出来。将这些信息存入 `ThreadLocal`，那么在这个请求处理链路上的任何地方（Service, Controller, DAO 等），都可以方便地获取到当前用户信息，而不需要在每个方法签名上都加上 `User user` 参数。

3.  **Spring 事务管理**：Spring 框架通过 `TransactionSynchronizationManager` 使用 `ThreadLocal` 来管理和传播事务上下文，确保在同一个事务方法调用链中，所有数据库操作都绑定到同一个事务上。

### 4. 使用 ThreadLocal 必须注意的陷阱：内存泄漏

虽然 `ThreadLocalMap` 的 Key 使用了弱引用来避免一部分内存泄漏问题，但在某些场景下，内存泄漏的风险依然存在，并且这是面试中经常被问到的重点。

- **泄漏的根源**：
  当 Key (`ThreadLocal` 实例) 被 GC 回收后，`ThreadLocalMap` 中就出现了 Key 为 `null` 的 `Entry`。虽然 Key 没了，但 `Entry` 中的 **Value 依然被 `ThreadLocalMap` 这个 `Entry` 数组强引用着**。只要线程本身不销毁，这个 `Entry` 数组就一直存在，那么这个 Value 就永远无法被回收，从而导致了内存泄漏。

- **`ThreadLocal` 的自我补救措施**：
  设计者考虑到了这一点。在调用 `get()`, `set()`, `remove()` 方法时，`ThreadLocalMap` 会顺便检查并清理那些 Key 为 `null` 的 `Entry`（这个过程被称为 "expungeStaleEntry"），从而释放 Value 的引用。但这是一种**被动清理**，不保证及时性。如果一个 `ThreadLocal` 长时间不被再次访问，这些垃圾数据就可能一直存在。

- **正确的解决方案：手动清理**
  **最佳实践是：在使用完 `ThreadLocal` 后，务必在 `finally` 块中调用其 `remove()` 方法。**
  `remove()` 方法会从当前线程的 `ThreadLocalMap` 中移除这个 `ThreadLocal` 对应的 `Entry`，从而彻底断开引用链，避免内存泄漏。

  ```java
  ThreadLocal<MyObject> myThreadLocal = new ThreadLocal<>();
  try {
      myThreadLocal.set(new MyObject());
      // ... business logic ...
  } finally {
      myThreadLocal.remove(); // 必须执行的操作！
  }
  ```

  这一点在使用**线程池**的场景下尤为重要。线程池中的线程是会被复用的。如果上一个任务设置了 `ThreadLocal` 的值但没有清理，那么下一个任务执行时，这个线程会带着上一个任务的“脏”数据继续执行，这不仅会导致内存泄漏，还可能引发严重的业务逻辑错误。

### 总结

- **`ThreadLocal`是一个为每个线程提供独立变量副本的工具，实现了线程间的数据隔离。**
- **其核心原理是，每个 `Thread` 都有一个 `ThreadLocalMap`，`ThreadLocal` 实例作为 Key 在这个 Map 中存取数据。**
- **它的关键应用在于跨组件传递线程上下文信息，如数据库连接、用户信息等。**
- **它最大的风险是内存泄漏，根源在于 `ThreadLocalMap` 中 Key 为 `null` 的 Entry 的 Value 依然被强引用。最可靠的避免方式是养成在使用完毕后，在 `finally` 中调用 `remove()` 方法的好习惯。**

## Java 中的内存模型？

Java 内存模型（Java Memory Model, JMM）是 Java 并发编程中一个非常底层且至关重要的概念。它是一个抽象的规范，而非物理硬件的真实描述。如果说`synchronized`, `volatile`, `Lock` 等是解决并发问题的“药方”，那么 JMM 就是阐述这些药方为何有效的“药理学”。

### 1. 为什么需要 Java 内存模型（JMM）？

这个问题的根源在于现代计算机硬件的性能优化机制与理想编程模型之间的差距。

- **硬件**：为了提升性能，CPU 和内存之间加入了多级高速缓存（CPU Caches）。一个 CPU 核心在执行计算时，会先将数据从主内存（Main Memory）加载到自己的高速缓存中，操作完成后再择机写回主内存。此外，为了让 CPU 内部的计算单元不空闲，编译器和处理器还会对指令进行重排序（Instruction Reordering），在不改变单线程执行结果的前提下，调整指令的执行顺序。

- **并发的麻烦**：这些优化在单线程环境下是完全没有问题的。但在多线程环境下，就会导致严重的问题：
  1.  **可见性（Visibility）问题**：一个线程在自己的 CPU 缓存中修改了某个共享变量的值，但没有及时写回主内存，导致其他线程从主内存中读取到的仍然是旧值。
  2.  **有序性（Ordering）问题**：指令重排序可能导致一个线程观察到另一个线程的执行顺序与代码中定义的顺序不一致，从而产生意想不到的后果。

为了解决这个问题，Java 语言设计者提出了一套统一的、跨平台的规范，这就是 JMM。**JMM 的目标是屏蔽底层硬件和操作系统的内存访问差异，为 Java 开发者提供一个一致的、可预测的内存可见性保证。** 它定义了在多线程程序中，变量（特指实例字段、静态字段和数组元素）的读写操作何时对其他线程可见。

### 2. JMM 的抽象模型

JMM 定义了一个抽象的结构，主要包含两个部分：

- **主内存（Main Memory）**：这是所有线程共享的区域，存储了所有的实例字段、静态字段和数组元素。
- **工作内存（Working Memory）**：这是每个线程私有的区域。线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，不能直接读写主内存中的变量。工作内存中存储了主内存中共享变量的副本。

**线程间变量的交互流程如下**：

1.  **Load（加载）**: 线程需要读取一个变量时，会从主内存中将该变量的值加载到自己的工作内存中。
2.  **Use（使用）**: 线程使用工作内存中的变量副本进行计算。
3.  **Assign（赋值）**: 线程将计算结果赋值给工作内存中的变量副本。
4.  **Store（存储）**: 线程将工作内存中修改后的变量副本的值存储回主内存。

**注意**：这个模型是纯粹抽象的，工作内存可以对应于 CPU 的高速缓存，也可以是寄存器或其他硬件优化。JMM 关注的是这一系列操作的规则和效果，而不是具体的物理实现。

### 3. JMM 如何解决并发三大问题

JMM 主要围绕并发编程中的**原子性、可见性和有序性**这三大特性来建立规则。

- **原子性（Atomicity）**：JMM 本身只保证了基本数据类型的读取和赋值操作是原子的（如 `int a = 5;`）。对于更大范围的原子性（如 `i++` 或一个代码块），JMM 通过 `synchronized` 和 `Lock` 来保证。`synchronized` 块之间的操作是互斥的，在外界看来，这个代码块内的操作就像一个不可分割的原子操作。

- **可见性（Visibility）**：这是 JMM 着重解决的问题。

  - **`volatile`**：当一个变量被声明为 `volatile` 后，JMM 会保证：
    1.  线程对该变量的修改会立即被刷新到主内存。
    2.  线程每次读取该变量时，都会使工作内存中的副本失效，强制从主内存重新加载。
  - **`synchronized` 和 `Lock`**：它们同样能保证可见性。JMM 规定，在一个线程解锁（`unlock`）之前，必须将共享变量的最新值刷新到主内存；一个线程加锁（`lock`）时，会清空工作内存，强制从主内存加载最新的值。
  - **`final`**：被 `final` 修饰的字段在构造器中一旦初始化完成，并且构造器没有把 `this` 引用传递出去，那么在其他线程中就能看见 `final` 字段的值。

- **有序性（Ordering）**：
  - **`volatile`**：`volatile` 关键字本身包含“禁止指令重排序”的语义。具体来说，它会插入内存屏障（Memory Barrier），确保 `volatile` 写操作之前的所有操作都已经完成，且其结果对其他线程可见；同时确保 `volatile` 读操作之后的所有操作都能看到 `volatile` 变量的最新值。
  - **`synchronized` 和 `Lock`**：它们通过“一个时刻只有一个线程能持有锁”的特性，天然地保证了持有同一个锁的多个同步块之间的执行是串行的，从而保证了有序性。
  - **Happens-Before 规则**：这是 JMM 有序性的基石，下面会详细讲。

### 4. JMM 的核心：Happens-Before 原则

为了让开发者更容易地理解和利用有序性，而不用去记忆复杂的重排序规则和内存屏障，JMM 提出了 `happens-before` 原则。这个原则是判断数据是否存在竞争、线程是否安全的主要依据。

**`happens-before` 的定义**：如果操作 A `happens-before` 操作 B，那么 A 操作的执行结果对 B 操作是可见的，并且 A 操作的执行顺序在 B 操作之前。

这两个操作可以在同一个线程，也可以在不同线程。JMM 天然定义了以下几条 `happens-before` 规则：

1.  **程序次序规则（Program Order Rule）**：在一个线程内，按照代码的顺序，书写在前面的操作 `happens-before` 书写在后面的操作。这是最符合我们编程直觉的规则。

2.  **管程锁定规则（Monitor Lock Rule）**：一个 `unlock` 操作 `happens-before` 于后面对**同一个锁**的 `lock` 操作。

3.  **volatile 变量规则（Volatile Variable Rule）**：对一个 `volatile` 变量的写操作 `happens-before` 于后面对这个变量的读操作。

4.  **线程启动规则（Thread Start Rule）**：`Thread` 对象的 `start()` 方法 `happens-before` 于此线程的任何一个动作。

5.  **线程终止规则（Thread Termination Rule）**：线程中的所有操作都 `happens-before` 于对此线程的终止检测，例如我们可以通过 `Thread.join()` 方法结束、`Thread.isAlive()` 的返回值等手段检测到线程已经终止执行。

6.  **线程中断规则（Thread Interruption Rule）**：对线程 `interrupt()` 方法的调用 `happens-before` 于被中断线程的代码检测到中断事件的发生。

7.  **对象终结规则（Finalizer Rule）**：一个对象的初始化完成（构造函数执行结束）`happens-before` 于它的 `finalize()` 方法的开始。

8.  **传递性（Transitivity）**：如果操作 A `happens-before` 操作 B，操作 B `happens-before` 操作 C，那么可以得出操作 A `happens-before` 操作 C。

**如何应用？**
当我们编写并发代码时，我们可以通过分析代码中的操作，看看能否利用这些 `happens-before` 规则串联起来，形成一个偏序关系。如果两个操作之间不存在任何 `happens-before` 关系，那么 JVM 就可以对它们进行任意重排序，它们的操作结果也可能互相不可见，我们就称这段代码是线程不安全的。

### 总结

- Java 内存模型是一个**规范**，旨在解决由 CPU 缓存、指令重排序等硬件优化引起的多线程**可见性**和**有序性**问题。
- Java 内存模型提出了**主内存**和**工作内存**的抽象模型来解释线程如何与变量交互。
- Java 内存模型的核心是 **`happens-before` 原则**，为我们开发者提供了一套清晰的规则，通过使用 `synchronized`, `volatile`, `Lock` 等同步工具来建立 `happens-before` 关系，从而编写出正确、可靠的并发程序。

## Java 中的指令重排？

### 1. 什么是指令重排？

指令重排是指，为了提高程序的执行性能，编译器和处理器在不改变**单线程程序**语义的前提下，对输入的代码指令进行重新排序执行。

也就是说，我们写的代码顺序（Code Order）和最终在 CPU 上实际执行的指令顺序（Execution Order）可能是不一致的。

例如，我们写了如下代码：

```java
int a = 1; // 语句1
int b = 2; // 语句2
int c = a + b; // 语句3
```

在实际执行时，只要保证`c`的计算结果是 3，那么语句 1 和语句 2 的执行顺序是可以互换的，编译器或 CPU 可能会先执行语句 2 再执行语句 1。

### 2. 为什么会发生指令重排？

指令重排的根本目的是**提升性能**。它主要发生在三个层面：

1.  **编译器优化重排**：编译器（例如 `javac` 编译成字节码，或 JIT 编译器在运行时编译成机器码）在分析代码时，发现某些指令的执行顺序调换后，可以减少寄存器的使用、或者更好地利用 CPU 的缓存，从而进行重排。

2.  **处理器指令级并行重排（Instruction-Level Parallelism）**：现代的 CPU 都采用了复杂的指令流水线（Pipeline）和超标量（Superscalar）技术。CPU 可以同时执行多条指令。如果一条指令因为等待数据（比如从主内存加载数据，这非常慢）而阻塞，CPU 的执行单元就会空闲。为了不浪费这些宝贵的时钟周期，CPU 会“乱序执行”（Out-of-Order Execution），即选择后面不依赖于当前阻塞指令的指令来提前执行。

3.  **内存系统重排**：这与 CPU 的缓存和写缓冲器（Store Buffer）有关。由于 CPU 和主内存之间速度差异巨大，CPU 的写操作实际上是先写入一个高速的写缓冲器，然后由这个缓冲器在未来的某个时间点同步到主内存。这就会导致，从其他 CPU 核心的视角看，写操作的顺序和实际执行的顺序可能不一致。

### 3. 指令重排的原则：as-if-serial

指令重排并不是随心所欲的，它必须遵守一个基本原则：**`as-if-serial` 语义**。

这个语义的核心是：**不管编译器和处理器如何对指令进行重排，它们必须保证在单线程环境下，程序的执行结果与代码顺序执行的结果是一致的。**

这个原则保护了单线程程序的正确性。我们作为程序员，在编写单线程代码时，完全可以不用关心指令重排的问题，因为`as-if-serial`语义为我们兜底了。

### 4. 指令重排在多线程下的问题

然而，`as-if-serial` 语义只保证单线程的正确性。一旦进入多线程环境，指令重排就可能导致严重的问题，最经典的就是**双重检查锁定（Double-Checked Locking, DCL）实现的单例模式**失效问题。

让我们看一个有问题的 DCL 单例实现：

```java
public class Singleton {
    private static Singleton instance = null;

    private Singleton() {}

    public static Singleton getInstance() {
        if (instance == null) { // 第一次检查
            synchronized (Singleton.class) {
                if (instance == null) { // 第二次检查
                    instance = new Singleton(); // 问题根源
                }
            }
        }
        return instance;
    }
}
```

问题出在 `instance = new Singleton();` 这一行代码。在 JVM 中，这行代码大致可以分解为三个步骤：

1.  **`memory = allocate();`** // 1. 分配对象的内存空间
2.  **`ctorInstance(memory);`** // 2. 初始化对象（调用构造函数）
3.  **`instance = memory;`** // 3. 将 instance 引用指向分配的内存地址

在单线程下，这三个步骤的顺序是固定的。但在多线程下，由于指令重排，执行顺序可能会变成 **1-3-2**：

1.  分配内存空间。
2.  将 `instance` 引用指向该内存地址（此时 `instance` 已经 **不为 `null`** 了）。
3.  初始化对象。

**现在设想以下并发场景：**

- **线程 A** 进入 `synchronized` 代码块，执行了重排后的步骤 1 和步骤 3。此时 `instance` 已经指向了一块内存，不再是 `null`，但对象还**没有被初始化**。
- **此时发生线程切换，线程 B** 调用 `getInstance()` 方法，执行到第一次检查 `if (instance == null)`。
- 线程 B 发现 `instance` 已经不是 `null` 了，于是它跳过了 `synchronized` 块，直接返回 `instance`。
- 线程 B 拿到的是一个**尚未初始化完成的对象**，如果它此时去使用这个对象的成员变量或方法，就会发生不可预知的错误（例如 `NullPointerException`）。

这就是指令重排在多线程环境下破坏程序正确性的典型例子。

### 5. 如何禁止指令重排？

为了解决这个问题，Java 内存模型（JMM）提供了一些机制，允许开发者在需要时显式地禁止特定区域的指令重排。

1.  **`volatile` 关键字**：
    `volatile` 是解决指令重排问题的核心武器。它有两个作用：保证内存可见性，以及**禁止指令重排**。
    JMM 为`volatile`的读写操作定义了特殊的内存屏障（Memory Barrier）：

    - **在每个`volatile`写操作的前面**，插入一个 StoreStore 屏障，保证前面的普通写操作对所有处理器可见。
    - **在每个`volatile`写操作的后面**，插入一个 StoreLoad 屏障，防止`volatile`写与后面可能有的`volatile`读/写操作重排序。
    - **在每个`volatile`读操作的后面**，插入一个 LoadLoad 屏障和一个 LoadStore 屏障，禁止`volatile`读与后面的普通读/写重排序。

    回到 DCL 的例子，我们只需要将 `instance` 声明为 `volatile` 即可解决问题：

    ```java
    private static volatile Singleton instance = null;
    ```

    加上 `volatile` 后，`instance = new Singleton();` 这行代码的内部执行顺序（1-2-3）得到了保证，步骤 3（赋值）一定会在步骤 2（初始化）之后发生，从而避免了其他线程获取到半初始化的对象。

2.  **`synchronized` 和 `Lock`**：
    `synchronized` 和 `Lock` 同样可以禁止指令重排。它们通过管程锁定规则（Monitor Lock Rule）来保证。一个 `unlock` 操作 `happens-before` 后面对同一个锁的 `lock` 操作。这意味着，`synchronized` 块内部的代码不能被重排到块的外部，并且在退出同步块时，所有在块内对共享变量的修改都会被刷新到主内存，保证了可见性和有序性。

### 总结

- **指令重排是一种为了提升性能而对代码执行顺序进行优化的技术**，发生在编译器、处理器等多个层面。
- 指令重排遵循 **`as-if-serial`语义**，保证单线程程序的正确性。
- 但在**多线程环境下，指令重排会破坏程序的有序性**，导致像 DCL 单例失效这样的严重问题。
- Java 提供了 **`volatile`和`synchronized`/`Lock`等机制来禁止指令重排**，其中`volatile`通过插入内存屏障来精确控制，是解决这类问题的关键工具。

## Java 中的 volatile 关键字？

`volatile`被称为“轻量级的`synchronized`”，因为它在多线程环境下提供了一种比锁开销更低的同步机制。

### 1. `volatile` 的两大核心作用

`volatile`关键字主要解决了多线程环境下的两个关键问题：**可见性（Visibility）**和**有序性（Ordering）**。但是，它**不保证原子性（Atomicity）**。

#### a. 保证可见性

这是`volatile`最核心的功能。在一个多线程应用中，当一个线程修改了一个共享变量的值，这个新值对其他线程来说是立即可见的。

- **问题背景**：在没有`volatile`的情况下，根据 Java 内存模型（JMM），每个线程都有自己的工作内存（通常对应 CPU 的高速缓存）。当一个线程修改共享变量时，它首先是在自己的工作内存中修改副本，然后才在某个不确定的时间点将这个修改刷新回主内存。 这就导致其他线程可能无法立即看到这个变更，它们可能会继续使用自己工作内存中的旧值，从而导致数据不一致。

- **`volatile`如何保证可见性**：
  1.  **写操作**：当一个线程修改一个`volatile`变量时，JMM 会强制该线程立即将这个新值刷新到主内存中。
  2.  **读操作**：当一个线程读取一个`volatile`变量时，JMM 会强制该线程让其工作内存中对应的副本失效，然后直接从主内存中重新加载最新的值。

通过这一写一读的强制规定，`volatile`确保了任何时刻，所有线程读取到的`volatile`变量值都是最新的。

#### b. 禁止指令重排

这是`volatile`另一个至关重要的作用，它保证了程序的有序性。

- **问题背景**：为了提升性能，编译器和处理器可能会对代码指令进行重排序。这种重排序在单线程下能保证最终结果一致（遵循`as-if-serial`语义），但在多线程环境下，可能会破坏程序逻辑，导致意想不到的错误。

- **`volatile`如何禁止指令重排**：
  `volatile`关键字通过在底层插入**内存屏障（Memory Barriers）** 来实现。 内存屏障是一种 CPU 指令，它有以下效果：

  1.  **屏障之前的所有写操作**必须在屏障之后的读写操作执行前完成，并对其他处理器可见。
  2.  **屏障之后的读写操作**不能被重排到屏障之前。
  3.  简而言之，内存屏障就像一道“栅栏”，严格限制了它前后指令的执行顺序。

  具体来说，JMM 会对`volatile`变量的读写操作施加以下内存屏障策略：

  - 在每个`volatile`写操作之前插入一个`StoreStore`屏障。
  - 在每个`volatile`写操作之后插入一个`StoreLoad`屏障。
  - 在每个`volatile`读操作之后插入一个`LoadLoad`屏障和一个`LoadStore`屏障。

  正是这些内存屏障，确保了`volatile`变量相关的代码会严格按照我们书写的顺序执行，避免了因重排序引发的并发问题。

### 2. `volatile` 的应用场景

了解了`volatile`的作用后，它的应用场景也就很清晰了。它适用于那些**一个线程写入，多个线程读取**，且**写入操作不依赖于变量当前值**的场景。

1.  **状态标志位（Status Flag）**
    这是最典型的应用场景。一个线程负责修改一个布尔标志位来控制流程，其他线程则不断地读取这个标志位来判断是否继续执行。

    ```java
    class Worker implements Runnable {
        private volatile boolean running = true; // 必须是 volatile

        public void stop() {
            running = false;
        }

        @Override
        public void run() {
            while (running) {
                // ... do work ...
            }
            System.out.println("Worker thread has stopped.");
        }
    }
    ```

    如果没有`volatile`，`run`方法所在的线程可能永远无法看到`running`变为`false`，导致循环无法终止。

2.  **双重检查锁定（Double-Checked Locking, DCL）的单例模式**
    在 DCL 单例模式中，为了防止指令重排导致其他线程获取到未完全初始化的对象，必须使用`volatile`来修饰单例实例。

    ```java
    public class Singleton {
        private static volatile Singleton instance; // 必须是 volatile

        private Singleton() {}

        public static Singleton getInstance() {
            if (instance == null) {
                synchronized (Singleton.class) {
                    if (instance == null) {
                        instance = new Singleton(); // volatile禁止了这里的重排序
                    }
                }
            }
            return instance;
        }
    }
    ```

    `volatile`确保了`instance = new Singleton()`这个操作的原子性（分配内存、初始化、赋值这三步不会被重排），从而保证了线程安全。

### 3. `volatile` vs. `synchronized`

`volatile`和`synchronized`是 Java 中两种重要的同步机制，但它们的定位和开销完全不同。

| 特性           | `volatile`                                    | `synchronized`                             |
| -------------- | --------------------------------------------- | ------------------------------------------ |
| **保证的特性** | 可见性、有序性                                | 原子性、可见性、有序性                     |
| **原子性**     | **不保证**。对于`i++`这种复合操作非线程安全。 | **保证**。代码块内的操作是原子的。         |
| **阻塞**       | 不会引起线程阻塞。                            | 会引起线程阻塞，未获取到锁的线程会等待。   |
| **实现机制**   | 基于 JMM 的内存屏障和 CPU 指令。              | 基于 JVM 内置的监视器锁（Monitor）。       |
| **使用范围**   | 只能修饰变量。                                | 可以修饰方法和代码块。                     |
| **性能开销**   | 较低，属于轻量级同步。                        | 较高，涉及线程的阻塞和唤醒，是重量级操作。 |

**总结**：`volatile`是一个变量修饰符，它用较低的成本保证了共享变量的可见性和有序性，但不保证原子性。而`synchronized`是一个更重的锁机制，它通过保证代码块的互斥执行，同时保证了原子性、可见性和有序性。

**如何选择？**

- 当只需要保证共享变量的可见性，或者用作状态标志时，使用`volatile`。
- 当需要保证一个代码块的原子性（例如`i++`或更复杂的逻辑），或者需要保护多个资源的一致性时，必须使用`synchronized`或`Lock`。

## Java 中的 synchronized 关键字？

### 1. `synchronized` 的核心作用与三大保证

`synchronized` 的核心作用是提供**互斥访问（Mutual Exclusion）**，确保同一时刻只有一个线程可以执行被它修饰的代码块或方法。为了实现这一目标，它为我们提供了并发编程中的三大关键保证：

1.  **原子性（Atomicity）**: `synchronized` 保证了其修饰的代码块（同步块）在执行期间是不可分割的。一个线程一旦进入同步块，在它执行完毕并释放锁之前，其他任何线程都无法进入该同步块。这使得同步块内的所有操作在其他线程看来，要么都完成了，要么都还没开始，形成了一个原子操作。

2.  **可见性（Visibility）**: `synchronized` 不仅仅是管“锁门”，它还管“通风报信”。根据 JMM（Java 内存模型）的`happens-before`原则中的管程锁定规则：对一个锁的`unlock`操作 `happens-before` 于后面对同一个锁的`lock`操作。这意味着：

    - 当一个线程释放锁时（退出`synchronized`块），JMM 会强制它将工作内存中对共享变量的所有修改都刷新回主内存。
    - 当一个线程获取锁时（进入`synchronized`块），JMM 会强制它清空工作内存，并从主内存中加载共享变量的最新值。
      这一进一出的操作，确保了线程之间对共享变量状态的可见性。

3.  **有序性（Ordering）**: `synchronized` 同样可以保证有序性。由于`as-if-serial`语义的存在，单线程内的指令重排不会影响最终结果。而`synchronized`保证了同一时刻只有一个线程能执行同步代码，这使得多个线程对同步代码的访问是串行的。在“一个线程的`unlock`”和“另一个线程的`lock`”之间，JMM 禁止了关键的指令重排序，从而保证了线程间的有序性。

### 2. `synchronized` 的四种使用方式

`synchronized` 的使用非常灵活，根据其修饰的对象，可以分为四种情况，其锁定的对象（也称为“锁监视器”或“Monitor”）各不相同：

1.  **修饰实例方法**: 锁住的是**当前实例对象 (`this`)**。

    ```java
    public synchronized void instanceMethod() {
        // ...
    }
    ```

    当一个线程进入这个方法时，它获取了`this`对象的锁。其他线程如果想调用**同一个实例**的任何`synchronized`实例方法，都必须等待。

2.  **修饰静态方法**: 锁住的是**当前类的 `Class` 对象**。

    ```java
    public static synchronized void staticMethod() {
        // ...
    }
    ```

    由于静态方法不属于任何实例，所以锁是加在类本身的`Class`对象上。这会影响所有尝试调用该类任何`synchronized`静态方法的线程。

3.  **修饰代码块（锁实例对象）**: 锁住的是**括号中指定的对象**。最常见的是锁`this`。

    ```java
    public void blockMethod() {
        synchronized (this) {
            // ...
        }
    }
    ```

    这种方式比直接修饰方法更灵活，因为它可以减小锁的粒度，只对必要的代码进行同步，从而提高性能。

4.  **修饰代码块（锁类对象）**: 锁住的是**指定的类的 `Class` 对象**。
    ```java
    public void blockMethod() {
        synchronized (MyClass.class) {
            // ...
        }
    }
    ```
    效果与修饰静态方法相同，但提供了更高的灵活性。

### 3. `synchronized` 的底层原理：Monitor

`synchronized` 是基于 JVM 层面实现的，其背后依赖于一个叫做 **Monitor（监视器锁）** 的概念。

- **Monitor 是什么**：在 HotSpot 虚拟机中，Monitor 是由 C++的`ObjectMonitor`类实现的。每个 Java 对象在创建时，其对象头（Object Header）中都包含一个指向 Monitor 的指针。
- **工作机制**：
  1.  当一个线程尝试进入一个`synchronized`保护的代码时，它会尝试获取该对象对应的 Monitor 的所有权。
  2.  如果获取成功，Monitor 内部的计数器（`_recursions`）会加 1，并且`_owner`字段会记录下当前线程的 ID。
  3.  如果获取失败，说明锁已被其他线程持有，当前线程会被阻塞，并放入一个叫 `_EntryList` 的等待队列中，进入`BLOCKED`状态。
  4.  当持有锁的线程执行完毕退出同步块时，它会释放 Monitor 的所有权（计数器减 1，`_owner`清空），并唤醒`_EntryList`中的一个或多个等待线程，让它们重新竞争锁。
- **字节码层面**：通过`javap`反编译可以看到，`synchronized`代码块在字节码层面被`monitorenter`和`monitorexit`这两个指令所包裹。`monitorenter`对应获取锁，`monitorexit`对应释放锁。编译器会确保即使代码块中发生异常，`monitorexit`也一定会被执行，从而保证锁的释放。

### 4. `synchronized` 的锁优化（JDK 1.6 之后）

早期的`synchronized`被称为“重量级锁”，因为它直接依赖于操作系统的互斥量（Mutex），线程的阻塞和唤醒都需要从用户态切换到内核态，开销巨大。从 JDK 1.6 开始，JVM 引入了一系列的锁优化，使得`synchronized`的性能大幅提升。这些优化是一个**锁升级**的过程：

1.  **偏向锁（Biased Locking）**：

    - **场景**：适用于只有一个线程访问同步代码的场景。
    - **原理**：当一个线程第一次获取锁时，JVM 会把对象头中的“偏向锁标记”设为 1，并用 CAS 操作把获取锁的线程 ID 记录在对象头中。此后，该线程每次进出同步块时，不再需要进行任何同步操作，只需简单检查线程 ID 是否匹配，极大地降低了开销。
    - **升级**：当有第二个线程尝试竞争锁时，偏向锁就会撤销（revoke），并升级为轻量级锁。

2.  **轻量级锁（Lightweight Locking）**：

    - **场景**：适用于多个线程交替执行同步代码，但几乎没有实际竞争（即很少有线程在等待锁）的场景。
    - **原理**：线程在获取锁时，JVM 会尝试使用 CAS 操作将对象的 Mark Word（对象头的一部分）更新为指向当前线程栈中锁记录（Lock Record）的指针。如果成功，则获取锁。如果失败，表示有其他线程持有了锁，当前线程会进行**自旋（Spinning）**，即执行一个空循环，等待一小段时间，而不是立即被挂起。
    - **升级**：如果自旋一定次数后（或有其他线程也在自旋），锁仍未被释放，轻量级锁就会膨胀为重量级锁。

3.  **重量级锁（Heavyweight Locking）**：
    - **场景**：适用于存在激烈锁竞争的场景。
    - **原理**：这就是传统的 Monitor 机制，竞争失败的线程会被阻塞并放入等待队列，依赖操作系统进行调度和唤醒，涉及用户态到内核态的切换。

这个锁升级过程是单向的，只能从偏向锁 -> 轻量级锁 -> 重量级锁，不能降级（但在某些 JVM 实现中，全局安全点时可能有锁降级的过程，但这很复杂且不常见）。

### 5. `synchronized` 的可重入性

`synchronized`是**可重入（Reentrant）** 的。这意味着一个线程在持有了一个对象的锁之后，可以再次进入任何其他使用该对象作为锁的`synchronized`代码块，而不会被自己阻塞。

- **原理**：Monitor 内部维护了一个计数器。线程每获取一次锁，计数器加 1；每释放一次锁，计数器减 1。只有当计数器变为 0 时，锁才被真正释放。这避免了因为内部方法调用而导致的死锁。

### 总结

- `synchronized`是一个**内置的、简单易用的同步关键字**，能保证并发环境下的**原子性、可见性和有序性**。
- `synchronized`有**四种使用方式**，锁的对象分别是`this`实例或类的`Class`对象。
- `synchronized`底层依赖于每个 Java 对象都关联的**Monitor**实现，通过`monitorenter`和`monitorexit`指令操作。
- 现代 JVM 对`synchronized`进行了大量优化，引入了从**偏向锁、轻量级锁到重量级锁的升级机制**，以及自旋等待等技术，使其性能在大多数情况下都非常优秀。
- `synchronized`也是一个**可重入锁**，避免了自身死锁的问题。

在开发中，如果并发场景不复杂，或者不需要`ReentrantLock`提供的高级功能（如公平锁、可中断锁、条件变量等），`synchronized`因其简单、不易出错的特性，通常是首选的同步方案。

## Java 中的 ReentrantLock？

`ReentrantLock` 是 Java 并发包（`java.util.concurrent.locks`）中一个非常核心的工具，它是对传统 `synchronized` 关键字的有力补充和扩展。作为一个可重入的互斥锁，它提供了比 `synchronized` 更高的灵活性和更丰富的功能。

### 1. 核心实现原理：AQS (AbstractQueuedSynchronizer)

要深入理解 `ReentrantLock`，就必须先理解它的基石——**AQS（抽象队列同步器）**。

- **AQS 是什么**：AQS 是 JUC 包中绝大多数同步器（如 `Semaphore`, `CountDownLatch`, `ReentrantLock`）的基础框架。它内部封装了实现同步状态管理、线程的排队、阻塞与唤醒等一系列底层操作。
- **AQS 的核心思想**：

  1.  **状态（State）**：AQS 内部维护了一个 `private volatile int state` 变量。这个 `state` 变量的具体含义由子类决定。在 `ReentrantLock` 中，`state` 表示锁的持有状态：`0` 表示锁未被持有，大于 `0` 表示锁已被某个线程持有。
  2.  **CLH 队列（FIFO 双向队列）**：AQS 内部维护了一个虚拟的、先进先出的双向队列。当多个线程竞争锁失败时，它们会被封装成节点（Node）并加入到这个队列的尾部进行排队等待。
  3.  **模板方法模式**：AQS 提供了 `acquire()` 和 `release()` 等模板方法来供外部调用，这些方法定义了获取和释放资源的顶层逻辑。而具体的获取资源（`tryAcquire`）和释放资源（`tryRelease`）的细节则由子类去实现。

- **`ReentrantLock` 如何利用 AQS**：
  `ReentrantLock` 内部有两个关键的静态内部类 `FairSync` 和 `NonfairSync`，它们都继承自 AQS。
  - 当一个线程调用 `lock()` 方法时，实际上是调用了 `Sync` 对象的 `acquire(1)` 方法。
  - `acquire(1)` 会尝试调用子类实现的 `tryAcquire(1)`。如果 `tryAcquire` 成功（比如锁是空闲的），则线程直接获取锁，并将 `state` 设置为 1。
  - 如果 `tryAcquire` 失败（比如锁已被其他线程持有），AQS 就会将当前线程加入到等待队列中，并将其挂起（park），使其进入阻塞状态。
  - 当持有锁的线程调用 `unlock()` 方法时，实际上是调用了 `release(1)` 方法，这会修改 `state` 的值（减 1），并在 `state` 变为 0 时，唤醒等待队列中的下一个线程。

### 2. `ReentrantLock` 的关键特性

`ReentrantLock` 之所以强大，是因为它提供了 `synchronized` 不具备的一系列高级功能。

1.  **可重入性（Reentrancy）**：
    这和 `synchronized` 一样，是其基本特性。一个线程在已经持有锁的情况下，可以再次成功获取该锁而不会被自己阻塞。

    - **原理**：AQS 的 `state` 变量在这里扮演了“重入计数器”的角色。线程每成功获取一次锁，`state` 就加 1。每次调用 `unlock()`，`state` 就减 1。只有当 `state` 减到 0 时，锁才被真正释放，其他线程才能获取它。

2.  **可中断的锁获取（Interruptible Lock Acquisition）**：

    - **方法**：`lockInterruptibly()`
    - **作用**：`lock()` 方法会一直等待直到获取锁，期间不响应中断。而 `lockInterruptibly()` 在等待锁的过程中，如果其他线程调用了该等待线程的 `interrupt()` 方法，它会立即停止等待并抛出 `InterruptedException`。
    - **应用**：这个特性对于处理可能长时间阻塞的任务非常有用，可以避免线程死等，提高系统的响应性。

3.  **可超时的锁获取（Timed Lock Acquisition）**：

    - **方法**：`tryLock(long timeout, TimeUnit unit)`
    - **作用**：在指定的时间内尝试获取锁。如果在规定时间内成功获取锁，则返回 `true`；如果超时仍未获取，则返回 `false`，而不会永久等待。
    - **应用**：可以有效避免死锁。当两个线程互相等待对方释放锁时，如果使用 `tryLock`，它们可以在超时后放弃等待，并执行回退策略。

4.  **非阻塞的锁获取（Non-blocking Lock Acquisition）**：

    - **方法**：`tryLock()`
    - **作用**：“试一下就行”，立即尝试获取锁，如果锁可用则立即获取并返回 `true`，否则立即返回 `false`，线程不会被阻塞。
    - **应用**：适用于那些“如果能拿到锁就做，拿不到就先做别的事”的场景。

5.  **公平/非公平策略选择**：这是 `ReentrantLock` 的一个显著特点，下面会详细说明。

6.  **条件变量（Condition）**：
    - `ReentrantLock` 可以与 `Condition` 接口配合使用。一个 `Lock` 对象可以创建多个 `Condition` 实例。
    - `Condition` 的 `await()`、`signal()`、`signalAll()` 方法分别对应 `Object` 的 `wait()`、`notify()`、`notifyAll()`。
    - **优势**：`synchronized` 只有一个等待队列，而 `ReentrantLock` 可以有多个独立的等待队列（每个 `Condition` 对象一个）。这使得实现更复杂的线程通信模式成为可能，例如，可以精确唤醒特定条件的线程组，而不是像 `notifyAll()` 那样唤醒所有等待的线程，从而避免了“惊群效应”，提高了效率。

### 3. 公平锁 vs. 非公平锁

`ReentrantLock` 的构造函数允许我们指定锁的公平策略：

- `public ReentrantLock()`：默认创建**非公平锁**。
- `public ReentrantLock(boolean fair)`：`true` 表示创建**公平锁**。

1.  **公平锁 (FairSync)**：

    - **策略**：严格遵循“先来后到”的原则。等待时间最长的线程将优先获取锁。
    - **实现**：在 `tryAcquire` 时，会检查等待队列中是否有比自己等待时间更长的线程。如果有，即使锁是空闲的，自己也不能获取，必须排队。
    - **优缺点**：优点是能避免线程“饥饿”，保证所有线程都有机会执行。缺点是吞吐量较低，因为频繁的线程上下文切换会带来巨大开销。

2.  **非公平锁 (NonfairSync)**：
    - **策略**：允许“插队”。当一个线程请求锁时，如果恰好锁被释放，那么这个新来的线程可以直接尝试获取锁，而不用去排队。
    - **实现**：`tryAcquire` 时，会先尝试用 CAS 操作抢占锁，抢占失败再进入队列。
    - **优缺点**：优点是吞吐量远高于公平锁，因为它减少了线程挂起和唤醒的次数。缺点是可能导致队列中的某些线程长时间得不到执行，即“饥饿”。

**注意**：`synchronized` 关键字实现的锁是**非公平**的。

### 4. 正确的使用规范与 `synchronized` 对比

- **使用规范**：`ReentrantLock` 需要**手动释放锁**，这是一个非常重要的注意点。为了保证在任何情况下（包括发生异常）都能释放锁，必须将 `unlock()` 操作放在 `finally` 代码块中。

  ```java
  Lock lock = new ReentrantLock();
  lock.lock(); // 获取锁
  try {
      // ... 受保护的业务逻辑 ...
  } finally {
      lock.unlock(); // 必须在finally块中释放锁
  }
  ```

- **与 `synchronized` 的对比总结**：

| 特性         | `synchronized`                                      | `ReentrantLock`                             |
| :----------- | :-------------------------------------------------- | :------------------------------------------ |
| **实现层面** | JVM 内置关键字                                      | JUC 包中的 API (基于 AQS)                   |
| **锁的释放** | 自动释放（代码块结束或异常）                        | 手动释放（必须在`finally`中调用`unlock()`） |
| **公平性**   | 非公平                                              | 可选（默认非公平）                          |
| **高级功能** | 无                                                  | 可中断、可超时、`tryLock`、多条件变量       |
| **性能**     | JDK 1.6 后性能大幅优化，与 `ReentrantLock` 差距不大 | 性能非常高，吞吐量通常略优                  |
| **易用性**   | 简单，不易出错                                      | 相对复杂，忘记`unlock`会导致严重问题        |

**如何选择？**

- **首选 `synchronized`**：在并发竞争不激烈，或者不需要 `ReentrantLock` 提供的高级功能的场景下，优先使用 `synchronized`。因为它更简单，JVM 会自动管理锁的释放，不容易出错。
- **选择 `ReentrantLock`**：当需要以下功能时，必须使用 `ReentrantLock`：
  1.  需要实现公平锁。
  2.  需要可中断、可超时的锁获取。
  3.  需要使用多个条件变量进行复杂的线程通信。

## Java 中的 ReentrantReadWriteLock？

`ReentrantReadWriteLock`（可重入读写锁）是 JUC 包中提供的一个更高级、更专业化的锁。它不是为了替代 `ReentrantLock` 或 `synchronized`，而是为了解决一类非常特定的并发场景：**读多写少**。

如果说 `ReentrantLock` 是一个独占锁（同一时刻只允许一个线程进入），那么 `ReentrantReadWriteLock` 就是一个**共享/独占锁**的组合体。它巧妙地将锁的功能一分为二：一个**读锁（共享锁）**和一个**写锁（独占锁）**。

### 1. 核心设计思想：读写分离

`ReentrantReadWriteLock` 的设计初衷是为了在保证线程安全的前提下，最大化地提升程序的并发性能。它遵循以下核心原则：

- **读-读共享**：多个线程可以**同时**持有读锁，并访问被保护的资源。因为读操作不会修改数据，所以它们之间是安全的。
- **读-写互斥**：当一个线程持有读锁时，其他任何线程都**不能**获取写锁。
- **写-读互斥**：当一个线程持有写锁时，其他任何线程都**不能**获取读锁。
- **写-写互斥**：当一个线程持有写锁时，其他任何线程都**不能**再获取写锁。

简单总结就是：“**读锁大家可以一起拿，但只要有人想写，或者正在写，那么谁都别想再进来，无论是读还是写。**”

这种设计使得在读操作远多于写操作的场景下，系统的并发能力能够得到极大的提升。例如，对于一个缓存系统，绝大多数操作都是查询（读），只有少量操作是更新（写），使用读写锁就非常合适。

### 2. 底层实现原理：依然是 AQS

和 `ReentrantLock` 一样，`ReentrantReadWriteLock` 的底层也是基于 **AQS (AbstractQueuedSynchronizer)** 实现的。但它的实现更为精巧，因为它需要用 AQS 中**一个 `state` 变量来同时管理两种不同类型的锁**。

`ReentrantReadWriteLock` 通过“按位切分”的方式来使用 `state` 变量：

- `state` 是一个 32 位的 `int` 变量。
- **高 16 位**：用来表示**读锁**的持有数量（也叫共享计数）。
- **低 16 位**：用来表示**写锁**的重入次数（也叫独占计数）。

**获取锁的逻辑大致如下**：

- **获取写锁 (`writeLock.lock()`)**：
  1.  检查 `state` 的高 16 位（读锁计数）和低 16 位（写锁计数）是否都为 0。
  2.  如果都为 0，则尝试用 CAS 操作将低 16 位的写锁计数加 1，并把锁的持有者设为当前线程。
  3.  如果不为 0，或者当前持有锁的线程不是自己（非重入情况），则获取失败，进入 AQS 的等待队列。
- **获取读锁 (`readLock.lock()`)**：
  1.  检查 `state` 的低 16 位（写锁计数）是否为 0。如果有任何线程持有写锁（或者有写线程在等待），则当前线程不能获取读锁，必须进入等待队列。
  2.  如果写锁未被持有，则尝试用 CAS 操作将高 16 位的读锁计数加 1。

通过这种方式，`ReentrantReadWriteLock` 在 AQS 框架的基础上，巧妙地实现了读写两种状态的管理。

### 3. 关键特性

1.  **公平性策略**：和 `ReentrantLock` 一样，`ReentrantReadWriteLock` 也支持公平和非公平两种策略。

    - **非公平（默认）**：读锁和写锁的获取机会是随机的，可能导致“写饥饿”。
    - **公平**：等待队列中的线程严格按照 FIFO 顺序获取锁。如果队列头部是写线程，则后来的读线程必须等待；如果队列头部是读线程，则后来的写线程必须等待。

2.  **可重入性**：

    - **写锁可重入**：持有写锁的线程可以再次获取写锁。
    - **读锁可重入**：持有读锁的线程可以再次获取读锁。
    - **写锁中可获取读锁**：持有写锁的线程**可以**在不释放写锁的情况下，继续获取读锁。

3.  **锁降级（Lock Downgrading）**：这是一个非常重要且独特的特性。它指的是，一个线程在**持有写锁**的情况下，**可以继续获取读锁**，然后**释放写锁**。这样，线程就从一个独占的写模式，安全地“降级”为了一个共享的读模式。

    **为什么要锁降级？**
    考虑一个场景：你需要更新一个共享数据，并在更新后立即读取这个数据进行后续操作，同时不希望在读取期间有其他线程来修改它。

    ```java
    WriteLock writeLock = rwl.writeLock();
    ReadLock readLock = rwl.readLock();

    writeLock.lock(); // 1. 获取写锁
    try {
        // ... 修改数据 ...

        readLock.lock(); // 2. 在不释放写锁的情况下，获取读锁（锁降级）

    } finally {
        writeLock.unlock(); // 3. 释放写锁
    }

    // 此刻线程依然持有读锁
    try {
        // ... 使用刚才更新的数据进行只读操作 ...
        // 因为还持有读锁，所以数据不会被其他写线程修改
    } finally {
        readLock.unlock(); // 4. 最后释放读锁
    }
    ```

    如果没有锁降级，那么在步骤 1 之后释放写锁，到步骤 4 获取读锁之间，可能会有另一个写线程插入，导致数据不一致。

4.  **不支持锁升级（Lock Upgrading）**：与锁降级相反，`ReentrantReadWriteLock` **不允许**一个持有读锁的线程去获取写锁。这样做是为了**避免死锁**。
    - **死锁场景**：如果支持锁升级。线程 A 和线程 B 都持有了读锁，然后它们都想升级为写锁。线程 A 需要等待线程 B 释放读锁，而线程 B 也需要等待线程 A 释放读锁，两者互相等待，形成死锁。

### 4. 潜在问题：写锁饥饿

在非公平模式下，如果读操作非常密集，源源不断地有线程来获取读锁，那么写线程可能会一直等待，永远无法获取到写锁，导致“饥饿”。公平模式在一定程度上可以缓解这个问题，因为它会给予等待队列头部的写线程优先权。

### 5. 使用场景与规范

- **适用场景**：明确的**读多写少**的并发场景。例如：
  - 内存缓存的实现
  - 系统配置信息的读取与修改
  - 排行榜等数据查询远多于更新的业务
- **使用规范**：与 `ReentrantLock` 完全相同，获取读锁或写锁后，必须在 `finally` 块中调用对应的 `unlock()` 方法来保证锁的释放。

### 总结

- **`ReentrantReadWriteLock` 是一种为“读多写少”场景量身定制的性能优化锁**，通过读写分离，允许多线程并发读取，从而提高吞吐量。
- **底层基于 AQS 实现**，通过按位切分一个`state`变量来同时管理读锁（共享）和写锁（独占）的状态。
- 具备**公平/非公平策略**、**可重入性**等特性。
- 其最独特的特性是支持**锁降级**（写锁 -> 读锁），但**不支持锁升级**（读锁 -> 写锁）以避免死锁。
- 在使用时，需要注意在非公平模式下可能引发的**写锁饥饿**问题，并严格遵守在`finally`块中释放锁的规范。

在选择锁时，如果业务场景符合读多写少的特征，使用`ReentrantReadWriteLock`将会比使用`synchronized`或`ReentrantLock`带来显著的性能提升。

## Java 中的原子操作类？

Java 中的原子操作类是 JUC（`java.util.concurrent`）包的核心组成部分，它们提供了一种比使用锁（`synchronized`或`ReentrantLock`）更高效、更轻量级的线程安全方式，用于在多线程环境下对**单个变量**进行原子操作。

### 1. 为什么需要原子操作类？

我们来看一个最经典的并发问题：`count++`。
即使你用 `volatile` 修饰 `count` 变量，`count++` 这个操作也不是线程安全的。

```java
private volatile int count = 0;

public void increment() {
    count++; // 这不是原子操作
}
```

因为 `count++` 实际上包含了三个独立的步骤：

1.  **读取** `count` 的当前值。
2.  将读取到的值**加 1**。
3.  将计算后的结果**写回** `count`。

在多线程环境下，线程 A 可能刚执行完步骤 1，就被挂起；然后线程 B 也执行了步骤 1、2、3，将 `count` 变成了 1。之后线程 A 恢复执行，它基于自己之前读取的旧值（0）进行加 1 操作，再写回，最终 `count` 还是 1。两次 `increment` 调用，结果只增加了 1，造成了数据不一致。

要解决这个问题，传统的方式是使用 `synchronized` 或 `Lock`：

```java
public synchronized void increment() {
    count++;
}
```

但这是一种**悲观锁**策略。无论是否存在竞争，它都会加锁，导致线程阻塞、上下文切换，在高并发下性能开销较大。

原子操作类则提供了一种**乐观锁**的解决方案，它在无竞争时开销极小，在有竞争时通过一种高效的机制重试，避免了线程阻塞。

### 2. 核心实现原理：CAS (Compare-And-Swap)

原子类的神奇之处在于其底层的 **CAS（比较并交换）** 机制。

- **CAS 是什么**：CAS 是一种硬件级别的原子指令，它允许 CPU 在一条指令内完成“比较-交换”的操作。这个操作是不可中断的，因此是原子的。
- **CAS 的三个操作数**：
  1.  **V**：要更新的内存地址（变量）。
  2.  **A**：期望的旧值（Expected Value）。
  3.  **B**：要更新的新值（New Value）。
- **执行逻辑**：当执行 CAS 指令时，处理器会自动比较内存地址 `V` 处的当前值是否等于期望的旧值 `A`。
  - 如果**相等**，说明在此期间没有其他线程修改过这个变量，就将 `V` 处的值更新为新值 `B`。
  - 如果**不相等**，说明变量已经被其他线程修改了，那么本次操作失败，不做任何修改。
- **自旋/循环 (Spinning)**：Java 中的原子类，如`AtomicInteger`的`getAndIncrement()`方法，通常会将 CAS 操作放在一个 `do-while` 循环中。
  ```java
  // 伪代码
  public final int getAndIncrement() {
      do {
          int current = get(); // 1. 获取当前值
          int next = current + 1; // 2. 计算新值
      } while (!compareAndSet(current, next)); // 3. CAS操作，如果失败就循环重试
      return current;
  }
  ```
  这种不断重试的方式被称为“自旋”。因为它不涉及线程的阻塞和唤醒，所以开销比锁小得多。

### 3. 原子操作类的四大分类

`java.util.concurrent.atomic`包下的原子类可以分为四类：

#### a. 基本类型原子类

这是最常用的一类，用于对单个基本类型或引用进行原子操作。

- `AtomicInteger`：原子更新整型。
- `AtomicLong`：原子更新长整型。
- `AtomicBoolean`：原子更新布尔值。
- `AtomicReference<V>`：原子更新引用类型。

#### b. 数组类型原子类

用于对数组中的某个元素进行原子操作。

- `AtomicIntegerArray`
- `AtomicLongArray`
- `AtomicReferenceArray<E>`
  **注意**：这些类保证的是数组中**单个元素**的原子性，而不是对整个数组的操作。

#### c. 引用字段更新原子类 (Updater)

这是一个比较特殊的类别，它允许我们以一种非侵入的方式，对一个普通类中被 `volatile` 修饰的字段进行原子操作，而无需修改这个类的源码。

- `AtomicIntegerFieldUpdater<T>`
- `AtomicLongFieldUpdater<T>`
- `AtomicReferenceFieldUpdater<T, V>`
  **使用场景**：当你需要为一个已有的类（可能是第三方库的类）中的某个字段提供原子更新能力时，这个工具非常有用。

#### d. 累加器 (Java 8 新增)

在高并发下，如果大量线程同时对一个`AtomicLong`进行 CAS 操作，由于只有一个线程能成功，其他线程会不断自旋，导致 CPU 资源浪费。累加器就是为了解决这个问题。

- **`LongAdder` / `DoubleAdder`**：
  - **原理**：`LongAdder`内部维护了一个`base`值和一个`Cell[]`数组。在低竞争时，直接更新`base`值。在高竞争时，它会为不同线程分配到不同的`Cell`上进行更新，从而**分散热点**，减少 CAS 冲突。
  - **取值**：当调用`sum()`方法获取总和时，它会返回`base`值与所有`Cell`中值的总和。
  - **特点**：`LongAdder`的`add()`性能远高于`AtomicLong`，但`sum()`因为需要遍历累加，性能稍差。它适用于“写多读少”的统计场景，是一种“空间换时间”的策略。
- **`LongAccumulator` / `DoubleAccumulator`**：
  `LongAdder`是`LongAccumulator`的一个特例。`LongAccumulator`可以提供一个自定义的二元运算函数，使其不仅仅局限于加法，可以实现更复杂的累积操作（如求最大值）。

### 4. 潜在问题：ABA 问题

CAS 机制看似完美，但存在一个著名的逻辑漏洞——**ABA 问题**。

- **什么是 ABA 问题**：一个线程准备对变量 V（初始值为 A）进行 CAS 操作。在它操作之前，另一个线程将 V 的值从 A 改为了 B，然后又改回了 A。当第一个线程执行 CAS 时，它发现内存值仍然是 A，与期望值相同，于是操作成功。但实际上，这个变量的值已经经历了一个变化过程，这在某些业务场景下是不能接受的。
- **例子**：一个账户余额是 100 元。你想用 100 元去消费。在你消费前，有人给你转了 50 元（余额变为 150），然后你又有一笔 50 元的自动扣款（余额变回 100）。当你进行消费 CAS（期望值 100，新值 0）时，操作成功了。但实际上账户已经发生了流水变化。
- **解决方案**：JUC 包提供了**版本号**机制来解决 ABA 问题。
  - **`AtomicStampedReference<V>`**：这个类在内部不仅维护了对象值，还维护了一个“时间戳”（stamp），可以理解为版本号。
  - 每次更新时，不仅要比较值，还要比较时间戳。更新成功后，会同时更新值和时间戳。
  - 这样，即使值从 A 变 B 再变回 A，它的时间戳（版本号）已经不同了，原来的 CAS 操作就会失败，从而避免了 ABA 问题。

### 总结

- Java 原子操作类是 JUC 提供的**轻量级、无锁的线程安全工具**，专用于**单个变量**的原子操作。
- 核心原理是基于硬件支持的**CAS（比较并交换）**指令，结合**自旋**实现乐观锁。
- 根据功能可分为**基本类型、数组、字段更新器和累加器**四大类，其中`LongAdder`是高并发统计场景下的性能利器。
- 使用 CAS 需要注意**ABA 问题**，可以通过`AtomicStampedReference`引入版本号来解决。

在开发中，凡是涉及到对单个变量（如计数器、状态标志、配置引用等）的线程安全更新，都应该优先考虑使用原子类，因为它通常比`synchronized`有更好的性能表现。

## Java 中的同步辅助工具？

Java 中的同步辅助工具，通常指的是 JUC（`java.util.concurrent`）包下，除锁和原子类之外，用于协调和控制多线程协作的工具类。它们极大地简化了复杂并发场景的编程，让开发者可以专注于业务逻辑，而不是底层的线程通信细节。

### 1. `CountDownLatch` (倒计时门闩)

`CountDownLatch` 是一个非常实用的“一次性”同步工具。它允许一个或多个线程等待，直到在其他线程中执行的一系列操作完成。

- **核心概念与比喻**：
  可以把 `CountDownLatch` 想象成一个**火箭发射倒计时**。

  - 发射控制中心（主线程）调用 `latch.await()`，它会一直等待，直到倒计时结束。
  - 各个岗位的工程师（工作线程）完成自己的检查任务后，就调用一次 `latch.countDown()`，倒计时计数器减 1。
  - 当所有工程师都报告完成（计数器减到 0），控制中心的 `await()` 方法就会返回，火箭可以发射。

- **关键方法**：

  - `public CountDownLatch(int count)`：构造函数，初始化一个值为 `count` 的计数器。
  - `public void await()`：调用该方法的线程会被阻塞，直到计数器的值变为 0。
  - `public void countDown()`：将计数器的值减 1。

- **核心特性**：

  1.  **一次性**：计数器减到 0 后，`CountDownLatch` 就完成了它的使命，不能被重置或复用。
  2.  **非对称性**：它通常用于一个线程等待多个线程的场景，或者多个线程等待一个线程发出“开始”信号的场景。

- **典型应用场景**：
  1.  **主线程等待所有子任务完成**：一个主任务需要分解成多个子任务并行执行，主任务必须等待所有子任务都执行完毕后才能继续。
  2.  **并行初始化**：一个服务启动时，需要并行初始化多个资源（如数据库连接池、缓存、配置加载），主启动流程需要等待所有资源都初始化成功。
  3.  **模拟高并发**：在性能测试中，可以让多个请求线程都调用`await()`等待一个“发令枪”，然后主线程调用一次`countDown()`，所有线程同时开始执行，模拟瞬时并发。

### 2. `CyclicBarrier` (循环栅栏)

`CyclicBarrier` 与 `CountDownLatch` 有些相似，但用途和设计思想完全不同。它让一组线程能够互相等待，直到所有线程都到达一个公共的屏障点（barrier point），然后再一起继续执行。

- **核心概念与比喻**：
  可以把它想象成一群朋友**约好一起去春游**。

  - 大家（一组线程）各自从家里出发，到达集合点（调用 `barrier.await()`）。
  - 先到的人不能先走，必须在集合点等待其他人。
  - 当最后一个人到达集合点时，“栅栏”打开，大家再一起出发去目的地。
  - 到达第一个景点后，大家又可以约定下一个集合点，再次使用这个“栅栏”。

- **关键方法**：

  - `public CyclicBarrier(int parties)`：构造函数，`parties` 指的是需要在此栅栏处等待的线程数量。
  - `public CyclicBarrier(int parties, Runnable barrierAction)`：一个更高级的构造函数，当所有线程都到达栅栏时，会优先执行 `barrierAction` 这个任务，然后再释放所有等待的线程。
  - `public int await()`：线程调用此方法表示自己已到达栅栏，并开始等待。

- **核心特性**：

  1.  **可循环使用 (Cyclic)**：当所有等待线程被释放后，`CyclicBarrier` 可以被重置并用于下一轮。
  2.  **对称性**：它用于一组线程**互相等待**，直到所有成员都准备就绪。

- **典型应用场景**：
  1.  **多线程数据处理**：在进行大规模数据计算时，可以将数据分成多块，每个线程处理一块。当所有线程都完成第一阶段的计算后，通过 `CyclicBarrier` 同步，然后利用 `barrierAction` 将各部分结果合并，再进入第二阶段的计算。
  2.  **并行迭代算法**：很多科学计算算法（如遗传算法）需要分代迭代，每一代的计算都需要所有线程完成其子任务后才能开始下一代的计算。

#### `CountDownLatch` vs. `CyclicBarrier` 对比

| 特性            | `CountDownLatch`                       | `CyclicBarrier`                          |
| :-------------- | :------------------------------------- | :--------------------------------------- |
| **作用对象**    | 一个或多个线程等待**其他**线程完成操作 | 一组线程**互相**等待对方到达某个点       |
| **可重用性**    | 不可重用                               | 可循环重用                               |
| **核心方法**    | `countDown()` (计数减一)               | `await()` (线程等待)                     |
| **与 AQS 关系** | 基于 AQS 的共享模式实现                | 基于 `ReentrantLock` 和 `Condition` 实现 |
| **附加动作**    | 无                                     | 可在栅栏打破时执行一个 `barrierAction`   |

### 3. `Semaphore` (信号量)

`Semaphore` 用于控制**同时访问某个特定资源的线程数量**，它通过协调各个线程，以保证合理地使用公共资源。

- **核心概念与比喻**：
  可以把它想象成一个**停车场**。

  - 停车场有固定数量的车位（`permits`）。
  - 一辆车想进入停车场，需要先获取一个车位（调用 `semaphore.acquire()`），如果没车位了，就得在门口排队等着。
  - 一辆车离开停车场，会释放出一个车位（调用 `semaphore.release()`），门口等待的下一辆车就可以进来了。

- **关键方法**：

  - `public Semaphore(int permits)`：构造函数，`permits` 表示可用许可（资源）的数量。
  - `public Semaphore(int permits, boolean fair)`：可以指定是公平模式还是非公平模式。
  - `public void acquire()`：获取一个许可，如果无可用许可，线程将阻塞。
  - `public void release()`：释放一个许可。

- **核心特性**：

  1.  **资源数量控制**：它的核心是控制并发访问特定资源的线程上限。
  2.  **非持有性**：`release()` 可以在任何线程中调用，不一定非得是之前 `acquire()` 的那个线程（尽管这不常见）。

- **典型应用场景**：
  1.  **流量控制与限流**：例如，一个服务最多只能同时处理 100 个请求，可以用一个 `Semaphore(100)` 来限制并发请求数。
  2.  **资源池管理**：如数据库连接池、线程池等。当池中资源有限时，`Semaphore` 可以控制同时获取资源的线程数。

### 4. `Exchanger` (交换器)

`Exchanger` 是一个用于**两个线程之间交换数据**的同步工具。它提供一个同步点，在这个同步点，两个线程可以互相交换彼此的数据。

- **核心概念与比喻**：
  可以把它想象成**间谍交换情报**。

  - 间谍 A 和间谍 B 约定在某个桥下见面。
  - 间谍 A 带着自己的情报（数据）先到了（调用 `exchange(dataA)`），他必须在桥下等待。
  - 当间谍 B 也带着他的情报（数据）到达桥下时（调用 `exchange(dataB)`），交换发生。
  - 间谍 A 拿到了间谍 B 的数据，间谍 B 拿到了间谍 A 的数据，然后各自离开。

- **关键方法**：

  - `public V exchange(V x)`：等待另一个线程到达此交换点，然后将给定的对象传送给该线程，并接收该线程的对象。

- **典型应用场景**：
  - **生产者-消费者模型**：一个线程生产数据（例如放入一个缓冲区），另一个线程消费数据（从缓冲区取出）。当生产者填满缓冲区，消费者清空缓冲区后，两者可以通过`Exchanger`交换空的和满的缓冲区，避免了数据的复制，提高了效率。
  - **算法中的数据校对**：两个线程使用不同算法计算同一个任务，计算完成后通过`Exchanger`交换结果，进行比对和验证。

### 总结

- 用 `CountDownLatch` 实现“**等待多个任务完成**”。
- 用 `CyclicBarrier` 实现“**多个线程同步执行**”。
- 用 `Semaphore` 实现“**流量控制和资源数限制**”。
- 用 `Exchanger` 实现“**两个线程间的数据交换**”。

## Java 中的 ConcurrentHashMap？

`ConcurrentHashMap` 是 JUC 包中最重要的并发容器之一，它是一个高性能、线程安全的哈希表实现，是解决高并发场景下 Map 需求的事实标准。

### 1. 为什么需要 `ConcurrentHashMap`？

在 `ConcurrentHashMap` 出现之前，要实现一个线程安全的 Map，我们通常有两种选择：

1.  **`Hashtable`**：这是一个“远古”的线程安全类。它的实现方式非常粗暴：在**所有**公开方法上都加上 `synchronized` 关键字。这意味着，无论你是在进行 `put`, `get`, `remove` 还是 `size` 操作，都会锁住整个 `Hashtable` 对象。这种全局锁（也叫表锁）导致其并发性能极差，只要有一个线程在操作 `Hashtable`，其他所有线程都必须等待。

2.  **`Collections.synchronizedMap(new HashMap<>())`**：这是通过装饰器模式创建的一个线程安全的 Map。它的底层原理和 `Hashtable` 类似，也是通过在 Map 对象上加一个全局的 `synchronized` 锁来实现。所以，它同样存在严重的性能瓶颈。

`ConcurrentHashMap` 的诞生，就是为了解决这种“一锁到底”的低效问题，它的核心目标是：**在保证线程安全的前提下，提供尽可能高的并发读写性能。**

### 2. JDK 1.7 的实现原理：分段锁 (Segment Locking)

JDK 1.7 版本的 `ConcurrentHashMap` 是并发容器设计史上的一个里程碑，它创造性地引入了**分段锁**技术。

- **核心思想：化整为零，降低锁粒度。**
  `ConcurrentHashMap` 不再锁住整个 Map，而是将 Map 内部的数据在逻辑上分割成多个独立的**段（Segment）**。每个 `Segment` 本身就像一个小型的 `Hashtable`，它拥有自己的锁。
- **数据结构**：
  `ConcurrentHashMap` -> `Segment[] segments` -> `HashEntry<K,V>[] table`
  1.  `ConcurrentHashMap` 内部维护一个 `Segment` 数组。`Segment` 继承自 `ReentrantLock`，所以每个 `Segment` 都是一个可重入锁。
  2.  每个 `Segment` 内部又包含一个 `HashEntry` 数组，`HashEntry` 数组的每个元素是一个链表的头节点（用于解决哈希冲突）。
- **操作流程**：
  - **`put` 操作**：
    1.  通过哈希算法计算出 Key 应该被存放在哪个 `Segment` 中。
    2.  调用该 `Segment` 的 `lock()` 方法，**只锁住这个 `Segment`**。
    3.  在 `Segment` 内部进行标准的 `put` 操作（定位到 `HashEntry` 数组，处理链表）。
    4.  操作完成后，调用 `unlock()` 释放该 `Segment` 的锁。
  - **`get` 操作**：
    `get` 操作**大部分情况下是无锁的**，这也是其性能高的关键。
    1.  `Segment` 和 `HashEntry` 中的共享变量（如 `value`, `next` 指针）都被 `volatile` 修饰，保证了内存可见性。
    2.  `get` 时，只需通过哈希定位到 `Segment`，再定位到 `HashEntry` 数组，然后顺着链表查找即可，整个过程不需要加锁。
  - **`size` 操作**：
    `size()` 的计算比较复杂。它会先尝试在不加锁的情况下，将所有 `Segment` 的 `count` 值（每个 `Segment` 维护自己的元素个数）累加两次。如果两次结果相同，就认为是一个相对准确的值并返回。如果不同，才会把所有 `Segment` 都锁住，再进行一次精确的累加。

**优点**：通过分段锁，多个线程可以同时操作不同的 `Segment`，并发度大大提升。默认情况下，`Segment` 的数量是 16，这意味着理论上可以支持 16 个线程的并发写入。
**缺点**：分段锁的粒度还是基于 `Segment`，而不是每个具体的元素，在某些场景下仍然有优化空间。`size()` 计算相对复杂且可能不精确。

### 3. JDK 1.8 的实现原理：CAS + `synchronized`

到了 JDK 1.8，`ConcurrentHashMap` 的实现被彻底重构，放弃了 `Segment` 的设计，转而采用了更先进、粒度更细的 **CAS + `synchronized`** 方案。其数据结构也变得和 JDK 1.8 的 `HashMap` 非常相似（数组 + 链表 / 红黑树）。

- **核心思想：进一步降低锁粒度，只在必要时加锁。**
  锁的粒度从“段（Segment）”缩小到了“**哈希桶的头节点**”。
- **数据结构**：
  `Node<K,V>[] table`，其中 `Node` 可以是链表节点，也可以是红黑树节点（`TreeNode`）。
- **操作流程**：
  - **`put` 操作**：
    1.  根据 Key 的哈希值计算出在 `table` 数组中的索引。
    2.  **Case 1: 该位置为 `null`？**
        说明没有哈希冲突，直接使用 **CAS (Compare-And-Swap)** 操作，原子地将新节点放入该位置。如果 CAS 成功，操作完成，**全程无锁**。
    3.  **Case 2: 该位置不为 `null`？**
        说明发生了哈希冲突。此时，会使用 **`synchronized` 关键字锁住该位置的头节点**。
    4.  在同步块内部，遍历链表或红黑树，更新或插入节点。
  - **`get` 操作**：
    与 1.7 类似，`get` 操作**全程无锁**。`table` 数组和 `Node` 的 `val`、`next` 字段都用 `volatile` 修饰，保证了可见性，可以直接读取。
  - **`size` 操作**：
    1.8 引入了 `LongAdder` 的思想来高效地计算 `size`。它维护一个 `baseCount` 变量和一个 `CounterCell[]` 数组。
    - 在没有竞争时，直接通过 CAS 更新 `baseCount`。
    - 当有竞争时，会把计数分散到各个 `CounterCell` 中，避免了对单一变量的激烈争抢。
    - `size()` 方法返回 `baseCount` 和所有 `CounterCell` 值的总和。这个值是最终一致的，在没有并发更新时是精确的。

**JDK 1.8 版本的优势**：

- **锁粒度更细**：锁只作用于哈希冲突的链表或红黑树的头节点，不同哈希桶之间的操作完全并发。
- **性能更高**：在无冲突时，完全依赖无锁的 CAS 操作，性能极高。
- **结构更简洁**：摒弃了复杂的 `Segment` 结构，实现更清晰。

### 4. 核心特性总结

- **线程安全**：`ConcurrentHashMap` 是完全线程安全的。
- **高并发**：为高并发读写场景做了深度优化。
- **迭代器是弱一致的 (Weakly Consistent)**：
  - 它的迭代器**不会**抛出 `ConcurrentModificationException`。
  - 迭代器创建后，可以反映创建时或之后某个时间点的状态。它**可能**会也**可能不会**反映创建后发生的修改。可以保证的是，遍历时不会漏掉元素，但可能拿到的是已经过时的数据。
- **Key 和 Value 不能为空**：与 `HashMap` 不同，`ConcurrentHashMap` 不允许 `null` 作为键或值，否则会抛出 `NullPointerException`。这是为了避免二义性：如果 `get(key)` 返回 `null`，你无法判断是这个 Key 对应的 Value 本身就是 `null`，还是 Map 中根本不存在这个 Key。

### 总结

- `ConcurrentHashMap`是一个为高并发设计的、线程安全的 `Map` 实现，是 `Hashtable` 和 `synchronizedMap` 的性能升级替代品。
- `ConcurrentHashMap`的实现经历了重要的演进：
  - **JDK 1.7 使用分段锁**，将锁的粒度从整个 Map 降低到 Segment，实现了有限的并发。
  - **JDK 1.8 放弃了分段锁，改为使用 CAS + `synchronized`**，将锁的粒度进一步降低到哈希桶的头节点，并在无冲突时实现无锁操作，并发性能更高。
- `ConcurrentHashMap`是 Java 后端开发中处理并发 `Map` 需求的首选方案。

## Java 中的 CopyOnWriteArrayList？

`CopyOnWriteArrayList` 是 JUC 包中一个非常有特色的线程安全 `List` 实现。它的设计哲学与 `synchronizedList` 或 `Vector` 完全不同，它不是通过加锁来保护每一次操作，而是通过一种“**写入时复制**”的策略来保证线程安全。

理解 `CopyOnWriteArrayList` 的关键，就在于理解它的名字——**Copy-On-Write**。

### 1. 核心思想：读写分离与“写入时复制”

`CopyOnWriteArrayList` 的核心是一种**读写分离**的思想，它旨在实现**读操作的极致并发**。

- **读操作**：所有的读操作（如 `get()`, `iterator()`, `size()`）都是**完全无锁**的。它们直接访问内部的数组，不需要任何同步，因此性能极高。
- **写操作**：所有的写操作（如 `add()`, `set()`, `remove()`）都是**加锁且昂贵的**。当需要修改 `List` 时，它不会直接在原有的数据上修改，而是遵循以下步骤：
  1.  **加锁**：首先获取一个 `ReentrantLock`，确保同一时刻只有一个写线程。
  2.  **复制**：创建一个底层数组的**全新副本**。
  3.  **修改**：在新创建的副本上执行修改操作（例如，添加一个新元素）。
  4.  **替换**：将内部指向旧数组的引用，原子地切换到指向新的、修改后的数组。
  5.  **解锁**：释放锁。

**一个生动的比喻**：
你可以把 `CopyOnWriteArrayList` 想象成一个正在被很多人阅读的共享文档。

- **阅读者**可以随时来阅读，互不干扰。
- 当**编辑者**想要修改文档时，他不会在原文档上直接涂改，因为这会影响正在阅读的人。他会拿去复印一份（**Copy**），在复印件上完成所有修改（**On-Write**），然后把修改好的复印件替换掉原来的旧文档。
- 在编辑者替换完成之前，所有来阅读的人看到的都还是旧版本的文档。替换完成后，新来的阅读者才会看到新版本。

### 2. 底层实现的关键点

1.  **内部数组 (`array`)**：
    `CopyOnWriteArrayList` 内部维护了一个 `private transient volatile Object[] array;`

    - **`volatile` 关键字至关重要**。它保证了当写线程将引用指向新数组时，这个变更对所有读线程是立即可见的。没有 `volatile`，读线程可能会读到过时的数组引用。

2.  **写操作的锁 (`lock`)**：
    内部使用了一个 `final transient ReentrantLock lock = new ReentrantLock();` 来保证写操作的原子性。

### 3. `CopyOnWriteArrayList` 的优缺点

这种独特的设计带来了非常鲜明的优点和缺点。

#### 优点：

1.  **极高的读性能**：读操作完全不加锁，并发性能非常出色，远超 `Vector` 和 `synchronizedList`。
2.  **线程安全**：它本身是线程安全的，开发者无需额外同步。
3.  **迭代器是“故障安全”的 (Fail-Safe)**：
    - 这是它一个非常重要的特性。`CopyOnWriteArrayList` 的迭代器在创建时，会获取到当时数组的一个引用（快照）。
    - 之后对 `List` 的任何修改，都是在新的数组上进行的，**不会影响**这个迭代器所引用的旧数组。
    - 因此，你可以在遍历 `CopyOnWriteArrayList` 的同时，在另一个线程中对它进行修改，迭代器**绝不会**抛出 `ConcurrentModificationException`。

#### 缺点：

1.  **写操作成本高昂**：每次写操作都要复制整个底层数组，这在时间和空间上开销都很大。如果 `List` 很大，或者写操作很频繁，性能会急剧下降。
2.  **内存占用问题**：在写操作的瞬间，内存中会同时存在新旧两个数组，如果 `List` 很大，会造成双倍的内存占用。
3.  **数据一致性问题（弱一致性）**：
    - `CopyOnWriteArrayList` 只能保证**最终一致性**，而不能保证实时一致性。
    - 一个线程的写操作完成后，其他已经开始读操作（特别是已经创建了迭代器）的线程，看到的可能仍然是修改前的数据。它们读取的是一个“快照”，而不是最新的数据。

### 4. 适用场景

基于其鲜明的优缺点，`CopyOnWriteArrayList` 的适用场景非常明确：

**适用于“读多写少”的并发场景。**

- **监听器/观察者模式 (Listeners/Observers)**：这是最经典的应用场景。一个事件源的监听器列表，通常在系统启动时注册好，之后很少会增加或移除，但事件发生时，需要频繁地遍历这个列表来通知所有监听器。
- **配置管理**：系统配置信息通常加载后就很少变动，但会被各个模块频繁读取。
- **黑白名单**：一些需要频繁查询，但更新频率很低的黑白名单列表。

**绝对不适用于**：

- 写操作远多于或接近读操作的场景。
- 需要数据实时一致性的场景。

### `CopyOnWriteArrayList` vs. `Vector` / `synchronizedList`

| 特性         | `CopyOnWriteArrayList`        | `Vector` / `synchronizedList`      |
| :----------- | :---------------------------- | :--------------------------------- |
| **同步机制** | 写时复制，读写分离            | 全局锁 (`synchronized`)            |
| **读性能**   | **非常高** (无锁)             | **低** (需要获取锁)                |
| **写性能**   | **非常低** (需要复制整个数组) | **中等** (只需获取锁，不需复制)    |
| **迭代器**   | **Fail-Safe** (不会抛 `CME`)  | **Fail-Fast** (并发修改会抛 `CME`) |
| **一致性**   | 弱一致性/最终一致性           | 强一致性                           |
| **适用场景** | **读远多于写**                | 读写都比较均衡，且并发度不高的场景 |

### 总结

- `CopyOnWriteArrayList`是一个为**高并发读取**场景设计的、**线程安全**的 `List`。
- `CopyOnWriteArrayList`核心是 **“写入时复制”** 的策略，通过牺牲写性能和内存占用，换来了无锁的、极致的读性能。
- `CopyOnWriteArrayList`的迭代器是**故障安全 (Fail-Safe)** 的，不会抛出 `ConcurrentModificationException`。
- `CopyOnWriteArrayList`提供的是**弱一致性**，读到的数据可能是“快照”，不保证实时。
- 因此，`CopyOnWriteArrayList`是一个**专用工具**，只适用于 **“读多写少”** 的特定并发场景，绝不能作为 `ArrayList` 的通用线程安全替代品。

## Java 中的 BlockingQueue？

`BlockingQueue` (阻塞队列) 是 Java 并发包（JUC）中一个极其重要的接口，它在 `java.util.Queue` 接口的基础上，增加了**线程阻塞**的特性。可以说，它是解决**生产者-消费者**问题的最佳实践，是构建许多高级并发结构（如线程池）的基石。

### 1. 核心概念：什么是“阻塞”？

`BlockingQueue` 的核心就在于“阻塞”二字。它解决了普通队列在并发环境下需要开发者手动处理的两个棘手问题：

1.  **当队列为空时**：一个消费者线程尝试从队列中获取（`take`）元素，它不会立即返回 `null` 或抛出异常，而是会自动进入**阻塞状态**，直到有其他生产者线程向队列中放入（`put`）一个元素。
2.  **当队列为满时**：一个生产者线程尝试向队列中放入（`put`）元素，如果这是一个有界队列（bounded queue），它不会抛出异常或返回失败，而是会自动进入**阻塞状态**，直到有其他消费者线程从队列中取走一个元素，腾出空间。

这种自动的阻塞与唤醒机制，使得 `BlockingQueue` 成为了一个完美的线程间“**缓冲区**”和“**通信信道**”，极大地**解耦**了生产者和消费者。

### 2. 四组核心 API

`BlockingQueue` 接口提供了四组不同的方法来处理添加和移除元素，以应对不同的需求场景，这也是其设计精妙之处：

| 操作     | 抛出异常    | 返回特殊值 | 阻塞     | 超时阻塞               |
| :------- | :---------- | :--------- | :------- | :--------------------- |
| **插入** | `add(e)`    | `offer(e)` | `put(e)` | `offer(e, time, unit)` |
| **移除** | `remove()`  | `poll()`   | `take()` | `poll(time, unit)`     |
| **检查** | `element()` | `peek()`   | (不适用) | (不适用)               |

- **抛出异常组**：如果操作不能立即执行（如向满队列`add`），会抛出 `IllegalStateException`。
- **返回特殊值组**：如果操作不能立即执行（如向满队列`offer`），会返回 `false`（插入）或 `null`（移除）。
- **阻塞组**：这是 `BlockingQueue` 的核心，如果操作不能立即执行，线程会被**无限期阻塞**，直到条件满足。
- **超时阻塞组**：阻塞组的升级版，线程只会被阻塞给定的时间，如果超时后条件仍不满足，则返回 `false` 或 `null`。

### 3. JUC 中主要的 `BlockingQueue` 实现

JUC 提供了多种 `BlockingQueue` 的实现，以适应不同的场景：

1.  **`ArrayBlockingQueue`**：

    - **结构**：基于**数组**实现的**有界**阻塞队列。
    - **特性**：必须在构造时指定容量。内部使用一个 `ReentrantLock` 和两个 `Condition` (`notEmpty`, `notFull`) 来实现生产者和消费者的精确唤醒。可以配置为公平或非公平模式。
    - **适用**：需要固定大小缓冲区的场景。

2.  **`LinkedBlockingQueue`**：

    - **结构**：基于**链表**实现的**可选有界**阻塞队列。
    - **特性**：如果在构造时不指定容量，其默认容量是 `Integer.MAX_VALUE`，相当于一个无界队列。其内部巧妙地使用了两个独立的锁（`putLock` 和 `takeLock`）来分别控制生产者和消费者的操作，这使得在并发读写时，**吞吐量通常高于 `ArrayBlockingQueue`**。
    - **适用**：绝大多数场景，特别是生产者和消费者速率不匹配时，其“无限”容量可以作为很好的缓冲。`ThreadPoolExecutor` 默认使用的就是它。

3.  **`PriorityBlockingQueue`**：

    - **结构**：一个支持优先级的**无界**阻塞队列。
    - **特性**：存入的元素必须实现 `Comparable` 接口，或者在构造时传入 `Comparator`。队列会根据元素的优先级（自然排序或自定义排序）进行出队。
    - **适用**：需要处理带有优先级的任务的场景。

4.  **`SynchronousQueue`**：

    - **结构**：一个**不存储任何元素**的阻塞队列。它的容量永远是 0。
    - **特性**：这是一种非常特殊的队列，用于线程间的“**直接传递**”或“**接力**”。任何一个 `put` 操作都必须等待一个对应的 `take` 操作，反之亦然。它本身不持有任何元素，只是一个“碰头”的机制。
    - **适用**：需要极高吞吐量、低延迟的生产者-消费者场景。`Executors.newCachedThreadPool()` 使用的就是它。

5.  **`DelayQueue`**：
    - **结构**：一个支持延时获取元素的**无界**阻塞队列。
    - **特性**：存入的元素必须实现 `Delayed` 接口。只有当元素的延迟时间到期后，才能从队列中被 `take` 出来。
    - **适用**：实现定时任务调度、缓存过期等场景。

### 4. 核心应用：生产者-消费者模型

`BlockingQueue` 的设计就是为了完美地实现生产者-消费者模式。

- **生产者 (Producer)**：负责创建数据或任务，并使用 `put()` 或 `offer()` 方法将其放入 `BlockingQueue`。
- **消费者 (Consumer)**：负责处理数据或执行任务，并使用 `take()` 或 `poll()` 方法从 `BlockingQueue` 中获取。
- **`BlockingQueue`**：作为共享的、线程安全的缓冲区，它完美地解决了两者之间的同步问题。

**使用 `BlockingQueue` 的好处**：

1.  **解耦**：生产者和消费者无需关心对方的存在和状态。
2.  **异步化**：生产者和消费者可以以不同的速率运行，队列作为缓冲，可以削峰填谷，应对生产者瞬间产生大量数据的情况。
3.  **简化并发编程**：开发者完全不需要手动编写 `wait()`, `notify()`, `synchronized` 等复杂的线程通信代码，所有底层的同步细节都由队列本身封装好了。

**最典型的应用就是 Java 的线程池 (`ThreadPoolExecutor`)**。

- 外部提交的任务是**生产者**。
- 线程池中的工作线程是**消费者**。
- `BlockingQueue` 就是用来存放待执行任务的**任务队列**。

### 总结

- `BlockingQueue` 是一个**为并发而生**的队列接口，核心特性是**自动化的阻塞和唤醒**机制。
- `BlockingQueue` 提供了**四组不同的 API**以应对不同场景，其中 `put/take` 是其阻塞特性的典型代表。
- JUC 提供了多种实现，如 **`ArrayBlockingQueue` (有界/数组/单锁)**, **`LinkedBlockingQueue` (可选界/链表/双锁)**, **`SynchronousQueue` (无容量/传递)** 等，各有专长。
- `BlockingQueue` 的**终极应用场景是生产者-消费者模式**，通过解耦和缓冲，极大地简化了并发程序的开发，是构建健壮并发系统的关键组件。

## Java 中的线程池？

Java 线程池是 Java 并发编程的灵魂，也是后端开发中用来管理和优化多线程资源的核心技术。任何一个高性能、高并发的后端服务，都离不开对线程池的合理使用。

### 1. 为什么需要线程池？（The "Why"）

在线程池出现之前，我们处理并发任务的方式通常是“即用即创”：
`new Thread(new MyRunnable()).start();`

这种方式存在几个致命的缺陷：

1.  **资源开销大**：线程是一个重量级资源，频繁地创建和销毁线程会消耗大量的系统资源（如内存和 CPU 时间），并涉及到用户态和内核态的切换。
2.  **性能瓶颈**：如果并发请求量巨大，无限制地创建线程会导致 CPU 和内存资源耗尽，甚至导致系统崩溃。
3.  **缺乏管理和控制**：无法对线程进行统一的管理、监控、复用和控制，例如无法限制并发线程的总数，也无法安排任务的执行顺序。

**线程池的核心思想就是：将线程的创建和管理与任务的执行相分离。**

- **线程复用**：预先创建好一定数量的线程，放入一个“池子”中。当任务到来时，从池中取出一个空闲线程来执行，任务执行完毕后，线程并不销毁，而是归还到池中等待下一个任务。
- **并发控制**：通过控制池中线程的数量，可以有效地管理并发度，避免资源过度消耗。
- **统一管理**：提供了任务排队、拒绝策略、生命周期管理等一系列高级功能。

### 2. 核心实现：`ThreadPoolExecutor` 的七大参数（The "What" & "How"）

Java 中线程池的“本体”就是 `ThreadPoolExecutor` 类。理解它的构造函数中的七个参数，是掌握线程池的关键。

```java
  public ThreadPoolExecutor(int corePoolSize,
                            int maximumPoolSize,
                            long keepAliveTime,
                            TimeUnit unit,
                            BlockingQueue<Runnable> workQueue,
                            ThreadFactory threadFactory,
                            RejectedExecutionHandler handler)
```

1.  **`corePoolSize` (核心线程数)**：
    线程池中始终保持存活的最小线程数，即使它们处于空闲状态。除非设置了 `allowCoreThreadTimeOut`。

2.  **`maximumPoolSize` (最大线程数)**：
    线程池能够容纳的线程数量的上限。当工作队列满了之后，如果当前线程数小于最大线程数，线程池会创建新的线程来处理任务。

3.  **`keepAliveTime` & `unit` (存活时间)**：
    当线程池中的线程数量超过 `corePoolSize` 时，多余的空闲线程在被销毁之前可以存活的时间。

4.  **`workQueue` (工作队列)**：
    一个 `BlockingQueue`，用于存放当所有核心线程都在忙碌时，新提交的、等待执行的任务。

    - `ArrayBlockingQueue`：有界队列。
    - `LinkedBlockingQueue`：默认是无界队列，可能导致 OOM。
    - `SynchronousQueue`：不存储元素的队列，来一个任务必须有一个线程接收。

5.  **`threadFactory` (线程工厂)**：
    用于创建新线程。通过自定义 `ThreadFactory`，我们可以给线程设置有意义的名字（如 `my-biz-worker-%d`），设置守护线程状态，或者设置线程优先级等，这对于调试和问题排查至关重要。

6.  **`rejectedExecutionHandler` (拒绝策略)**：
    当工作队列已满，并且线程数也达到了 `maximumPoolSize` 时，新提交的任务会根据此策略来处理。JUC 提供了四种预设策略：
    - `AbortPolicy` (默认)：直接抛出 `RejectedExecutionException` 异常。
    - `CallerRunsPolicy`：由提交任务的线程自己来执行这个任务。这是一种很好的反馈机制，可以减慢任务提交的速度。
    - `DiscardPolicy`：直接悄悄地丢弃这个任务，不抛异常。
    - `DiscardOldestPolicy`：丢弃工作队列中最旧的一个任务，然后尝试重新提交当前任务。

### 3. 线程池的内部工作原理

当一个新任务通过 `execute()` 方法提交给线程池时，其处理流程如下：

1.  **判断核心线程数**：检查当前运行的线程数是否小于 `corePoolSize`。
    - **是**：创建一个新的核心线程来执行该任务，即使其他核心线程是空闲的。
    - **否**：进入下一步。
2.  **尝试加入工作队列**：检查 `workQueue` 是否已满。
    - **否**：将任务放入工作队列中。
    - **是**：进入下一步。
3.  **判断最大线程数**：检查当前运行的线程数是否小于 `maximumPoolSize`。
    - **是**：创建一个新的**非核心线程**来执行该任务。
    - **否**：进入下一步。
4.  **执行拒绝策略**：所有资源（核心线程、队列、最大线程）都已用尽，执行 `RejectedExecutionHandler` 的 `rejectedExecution` 方法。

这个流程是 `ThreadPoolExecutor` 调度任务的核心逻辑。

### 4. `Executors` 工厂类及其风险

为了方便开发者，JUC 提供了 `Executors` 工厂类来创建几种预设配置的线程池：

- **`Executors.newFixedThreadPool(int nThreads)`**：

  - 创建一个**固定大小**的线程池。`corePoolSize` 和 `maximumPoolSize` 相等。
  - 使用 `LinkedBlockingQueue` (无界队列)。
  - **风险**：由于队列是无界的，如果任务提交速度远快于处理速度，会导致任务在队列中无限堆积，最终可能引发**内存溢出 (OOM)**。

- **`Executors.newCachedThreadPool()`**：

  - 创建一个**可缓存**的线程池。`corePoolSize` 为 0，`maximumPoolSize` 为 `Integer.MAX_VALUE`。
  - 使用 `SynchronousQueue`。
  - **风险**：由于最大线程数是近乎无限的，如果瞬时并发量极高，会创建大量线程，同样可能耗尽系统资源，导致**OOM**。

- **`Executors.newSingleThreadExecutor()`**：
  - 创建一个**单线程**的线程池，保证所有任务按顺序执行。
  - `corePoolSize` 和 `maximumPoolSize` 都为 1。
  - 使用 `LinkedBlockingQueue`。
  - **风险**：与 `newFixedThreadPool` 相同，存在队列堆积导致的 OOM 风险。

**【生产环境最佳实践】**
在阿里巴巴的 Java 开发规范中，**强制规定不允许使用 `Executors` 去创建线程池**，而是必须通过 `new ThreadPoolExecutor(...)` 的方式手动创建。
这样做的目的是让开发者明确线程池的运行规则，对核心参数有完全的掌控，避免因使用预设配置而导致的资源耗尽风险。

### 5. 线程池的生命周期管理

线程池是一个需要手动管理的资源，主要通过以下方法：

- **`shutdown()`**：**平滑关闭**。线程池不再接受新任务，但会等待已经提交到队列中的任务全部执行完毕。
- **`shutdownNow()`**：**强制关闭**。线程池不再接受新任务，会尝试通过 `Thread.interrupt()` 中断所有正在执行的任务，并清空工作队列，返回未执行的任务列表。
- **`awaitTermination(long timeout, TimeUnit unit)`**：阻塞当前线程，直到线程池状态变为 `TERMINATED`，或者超时。通常在调用 `shutdown()` 后使用，以确保所有任务都已完成。

### 总结

- Java 线程池是一种**管理和复用线程的核心技术**，旨在提升性能、控制资源。
- Java 线程池**核心是`ThreadPoolExecutor`类**，它的**七大参数**定义了线程池的行为模式。
- Java 线程池的**工作原理**是一个层次分明的决策过程：核心线程 -> 工作队列 -> 最大线程 -> 拒绝策略。
- 虽然`Executors`提供了便捷的工厂方法，但在生产环境中，我们必须**手动创建`ThreadPoolExecutor`** 以避免 OOM 风险。
- 最后，作为一个系统资源，必须**显式地管理其生命周期**，在不再需要时通过`shutdown()`等方法进行关闭。

## Java 中的线程池调优？

### 1. 线程池调优的核心目标

调优的根本目标是在**有限的系统资源**下，实现**最大的处理能力**。这可以分解为几个子目标：

- **高吞吐量 (High Throughput)**：单位时间内处理尽可能多的任务。
- **低延迟 (Low Latency)**：任务从提交到开始执行的等待时间尽可能短。
- **高资源利用率 (High Resource Utilization)**：充分利用 CPU 和内存等资源，但又不能过度消耗导致系统不稳定。
- **系统稳定性 (System Stability)**：避免因任务突增导致 OOM 或系统崩溃。

这几个目标之间往往存在冲突（例如，为了降低延迟可能需要更多线程，但这会增加资源消耗），调优的艺术就在于找到它们之间的最佳平衡点。

### 2. 调优的基础：任务分类

在进行任何参数设置之前，最重要的一步是**分析线程池要执行的任务类型**。这通常分为两类：

1.  **CPU 密集型任务 (CPU-bound)**：

    - **特点**：任务需要进行大量的计算、逻辑判断，CPU 会长时间处于高速运转状态。例如，复杂的算法、加解密、正则表达式匹配等。
    - **调优策略**：这种任务不希望频繁地进行线程上下文切换，因为切换本身会消耗 CPU 周期。因此，线程数不宜设置过多。

2.  **I/O 密集型任务 (I/O-bound)**：
    - **特点**：任务的大部分时间都在等待 I/O 操作的完成，CPU 是空闲的。例如，数据库查询、RPC 远程调用、文件读写、网络请求等。
    - **调优策略**：由于线程在等待时 CPU 是空闲的，我们可以创建更多的线程，以便在一个线程等待 I/O 时，CPU 可以去执行其他线程的任务，从而提高 CPU 的利用率。

### 3. 核心参数的调优策略

#### a. `corePoolSize` 和 `maximumPoolSize` (线程数设置)

这是调优中最核心的部分。

- **对于 CPU 密集型任务**：

  - 理论上，线程数设置为 **CPU 核心数 (N) 或 N+1** 是最优的。
  - **原因**：N 个线程可以并行地在 N 个核心上运行。多出来的一个线程是为了防止某个线程因偶尔的页错误或其他原因阻塞时，CPU 能够有其他线程可以调度，保持 CPU 的时钟周期不被浪费。
  - **获取 CPU 核心数**: `int N = Runtime.getRuntime().availableProcessors();`

- **对于 I/O 密集型任务**：

  - 这是一个经验公式：**线程数 = N \* (1 + 平均等待时间 / 平均计算时间)**
  - **原因**：我们希望在线程等待 I/O 的时候，CPU 能被充分利用。这个公式估算了需要多少个线程才能把 CPU 的空闲时间填满。
  - **简化估算**：在实践中，精确计算等待/计算时间比率比较困难。通常会采用一个**经验值**，比如将线程数设置为 **CPU 核心数的 2 倍**，然后根据实际监控情况进行调整。对于有大量、缓慢的 I/O 操作的应用，这个倍数可能会更高。

- **`maximumPoolSize` 的设置**：
  它定义了系统能承受的并发极限。这个值需要和 `workQueue` 结合考虑。如果 `workQueue` 是有界的，`maximumPoolSize` 应该大于 `corePoolSize`，为系统应对突发流量提供弹性。如果 `workQueue` 是无界的，这个参数就几乎无效了。

#### b. `workQueue` (工作队列)

队列的选择直接影响任务的排队和处理策略。

- **`LinkedBlockingQueue` (无界队列)**：
  - **优点**：可以无限堆积任务，适用于任务量可控，且不希望有任务被拒绝的场景。
  - **调优陷阱**：这是**OOM 的头号元凶**。如果生产者速度远超消费者，内存会被耗尽。在生产环境中必须**慎用**，或对其大小进行监控和报警。
- **`ArrayBlockingQueue` (有界队列)**：
  - **优点**：强制性的容量限制，可以有效地防止内存溢出，是构建稳定系统时的**首选**。
  - **调优**：队列大小的设置需要权衡。太小，会导致任务频繁被拒绝，或频繁创建非核心线程，增加系统抖动；太大，会增加任务的平均等待时间，且占用更多内存。通常需要根据压测结果来确定一个合理的容量。
- **`SynchronousQueue` (同步队列)**：
  - **优点**：极高的吞吐量，任务无需排队，直接传递给线程。
  - **调优**：它要求 `maximumPoolSize` 足够大（通常设为`Integer.MAX_VALUE`，如`CachedThreadPool`），否则任务会非常容易被拒绝。适用于**大量、短暂**的任务场景，但有耗尽系统线程资源的风险。

#### c. `rejectedExecutionHandler` (拒绝策略)

拒绝策略是线程池的最后一道防线。

- `AbortPolicy`：过于粗暴，可能会丢失重要任务。
- `DiscardPolicy` / `DiscardOldestPolicy`：可能会丢失数据，适用于允许任务丢失的场景。
- **`CallerRunsPolicy`**：这是一个非常好的**反馈和降级**策略。当线程池饱和时，它会让提交任务的那个线程（通常是业务线程，如 Tomcat 线程）自己去执行任务。这会**阻塞**业务线程，从而**减慢任务的提交速度**，给线程池一个“喘息”的机会，形成一种**反向压力**。

### 4. 一个完整的调优实践流程

1.  **明确场景与目标**：
    - 这个线程池是做什么的？（例如：处理 RPC 请求、执行异步日志、跑定时任务）
    - 任务是 CPU 密集型还是 I/O 密集型？
    - 调优目标是什么？（例如：RPC 平均响应时间 < 50ms）
2.  **设置初始基准值**：
    - 根据任务类型，套用上面的公式，设置一个合理的初始 `corePoolSize` 和 `maximumPoolSize`。
    - 选择一个**有界的 `ArrayBlockingQueue`**，并估算一个队列大小（例如 100-1000）。
    - 选择一个合适的拒绝策略，`CallerRunsPolicy` 通常是一个安全的选择。
    - **务必**使用自定义的 `ThreadFactory` 给线程命名，方便排查问题。
3.  **进行压力测试与监控**：
    - 使用压测工具（如 JMeter, nGrinder）模拟真实的线上流量。
    - **监控关键指标**：
      - **线程池内部指标 (通过 JMX 或`ThreadPoolExecutor`的 getter 方法)**：
        - `getActiveCount()`: 活跃线程数
        - `getPoolSize()`: 当前线程池总线程数
        - `getQueue().size()`: 队列中等待的任务数
        - `getCompletedTaskCount()`: 已完成的任务数
        - `getTaskCount()`: 曾提交到线程池的总任务数
      - **业务指标**：QPS、响应时间（平均值、99%线）、任务成功/失败率。
      - **系统指标**：CPU 使用率、内存占用、GC 情况、线程上下文切换次数 (`vmstat`, `pidstat`)。
4.  **分析与调整**：
    - **CPU 利用率很低，但队列持续积压** -> I/O 密集型任务，线程数不够，需要**调高 `corePoolSize`**。
    - **CPU 利用率很高（甚至 100%），但 QPS 上不去** -> CPU 密集型任务，线程数可能过多，导致频繁上下文切换，需要**调低 `corePoolSize`**。
    - **频繁触发拒绝策略** -> 流量洪峰处理能力不足。可以考虑**增大队列容量**或**调高 `maximumPoolSize`**。
    - **内存持续增长** -> 如果使用无界队列，检查是否有 OOM 风险。如果是有界队列，检查任务对象本身是否过大或存在内存泄漏。
5.  **重复迭代**：
    调整参数后，回到第 3 步，重新进行压测和监控，直到找到满足业务需求的最佳配置。

## Java 中的 Fork/Join 框架？

Java 的 Fork/Join 框架是 JUC（`java.util.concurrent`）包中一个用于**并行计算**的强大框架，自 JDK 1.7 引入。它专门设计用来解决那些可以被**递归地分解成更小、独立子任务**的问题，其核心思想是**分而治之 (Divide and Conquer)**。

### 1. 核心思想：分而治之 (Divide and Conquer)

Fork/Join 框架的运作模式与经典的递归算法非常相似：

1.  **分解 (Fork)**：如果一个任务足够大，无法直接解决，就将其“分叉”成两个或多个更小的、独立的子任务。这个过程会递归地进行下去，直到子任务足够小，可以直接计算为止。
2.  **解决 (Compute)**：当子任务小到无法再分解时（达到一个预设的阈值），就直接计算这个子任务的结果。
3.  **合并 (Join)**：一个任务的所有子任务都完成后，将它们的结果“合并”起来，形成这个大任务的最终结果。

**一个生动的比喻**：
想象一下，CEO（主线程）有一个非常庞大的项目（大任务）要完成。他不会自己去做，而是把项目分解成几个部分，分给几个部门经理（第一次 Fork）。部门经理觉得自己的任务还是太大，又把它分解成小组任务，分给小组长（第二次 Fork）。小组长拿到任务后，发现可以直接让组员完成（达到阈值，直接 Compute）。组员完成后，把结果汇报给小组长，小组长整合后汇报给部门经理（第一次 Join），部门经理再整合，最终汇报给 CEO（第二次 Join）。

### 2. Fork/Join 的“秘密武器”：工作窃取 (Work-Stealing)

为了让所有 CPU 核心都保持忙碌，最大化并行效率，Fork/Join 框架采用了一种非常聪明的调度策略——**工作窃取**。

- **传统线程池的问题**：传统的 `ThreadPoolExecutor` 使用一个**公共的任务队列**。所有线程都从这个队列中获取任务。在高并发下，这个公共队列会成为一个竞争热点，多个线程同时访问需要加锁，从而限制了性能。
- **Fork/Join 的解决方案**：
  1.  `ForkJoinPool` 中的每个工作线程都维护着一个自己的**双端队列 (Double-Ended Queue, Deque)**。
  2.  当一个线程 A `fork` 出一个新的子任务时，它会把这个子任务**推入自己队列的头部 (push)**。
  3.  线程 A 执行任务时，总是从**自己队列的头部 (pop)** 取任务。这遵循了“后进先出 (LIFO)”的原则，非常适合递归算法，可以更好地利用 CPU 缓存。
  4.  **工作窃取发生**：当线程 B 的工作队列已经空了（自己没事干了），它不会闲着。它会随机地“瞟”一眼其他线程（比如线程 A）的队列，并从该队列的**尾部 (steal)** 窃取一个任务来执行。
  5.  **为什么从尾部窃取？** 因为尾部的任务通常是“最老”的、颗粒度更大的任务。窃取这种任务可以为窃取者带来更多的工作量，减少窃取次数，同时也减少了与任务所有者（从头部取任务）之间发生竞争的可能性。

通过工作窃取，Fork/Join 框架实现了负载均衡，让空闲的线程主动去帮助繁忙的线程，从而极大地提高了 CPU 的整体利用率。

### 3. 核心组件

Fork/Join 框架主要由以下三个核心类组成：

1.  **`ForkJoinPool`**：

    - 执行 Fork/Join 任务的专用线程池，它实现了 `ExecutorService` 接口。
    - 内部的工作线程 (`ForkJoinWorkerThread`) 专门用于执行 `ForkJoinTask`，并且都拥有自己的双端队列。
    - 通常，我们不需要手动创建，可以使用 `ForkJoinPool.commonPool()` 来获取一个全局共享的、默认大小等于 CPU 核心数的线程池。

2.  **`ForkJoinTask<V>`**：

    - 这是所有 Fork/Join 任务的抽象基类。它提供了 `fork()` 和 `join()` 这两个核心方法。
    - `fork()`: 异步地执行当前任务，并立即返回。通常用于启动子任务。
    - `join()`: 阻塞当前线程，直到任务执行完成并返回其结果。

3.  **`ForkJoinTask` 的两个关键子类**：
    - **`RecursiveTask<V>`**：一个会**返回结果**的递归任务。适用于有计算结果需要合并的场景。
    - **`RecursiveAction`**：一个**不返回结果**的递归任务。适用于只需执行一系列操作，而不需要合并结果的场景。

### 4. 一个具体的例子：并行计算大数组求和

假设我们要计算一个非常大的 `long[]` 数组的和。

```java
import java.util.concurrent.RecursiveTask;
import java.util.concurrent.ForkJoinPool;

class SumTask extends RecursiveTask<Long> {
    // 任务分解的阈值
    private static final int THRESHOLD = 10_000;
    private long[] array;
    private int start;
    private int end;

    public SumTask(long[] array, int start, int end) {
        this.array = array;
        this.start = start;
        this.end = end;
    }

    @Override
    protected Long compute() {
        // 1. 解决 (Compute): 如果任务足够小，直接计算
        if (end - start <= THRESHOLD) {
            long sum = 0;
            for (int i = start; i < end; i++) {
                sum += array[i];
            }
            return sum;
        }

        // 2. 分解 (Fork): 任务太大，一分为二
        int middle = start + (end - start) / 2;
        SumTask leftTask = new SumTask(array, start, middle);
        SumTask rightTask = new SumTask(array, middle, end);

        // 异步执行左边的子任务
        leftTask.fork();
        // 右边的子任务可以由当前线程直接计算，以减少线程创建开销
        Long rightResult = rightTask.compute();

        // 3. 合并 (Join): 等待左边子任务完成，并合并结果
        Long leftResult = leftTask.join();

        return leftResult + rightResult;
    }
}

// 如何使用
public class ForkJoinExample {
    public static void main(String[] args) {
        long[] array = createLargeArray(); // 创建一个包含百万个元素的数组
        ForkJoinPool pool = ForkJoinPool.commonPool();
        SumTask task = new SumTask(array, 0, array.length);

        long result = pool.invoke(task); // invoke会阻塞直到任务完成
        System.out.println("Sum: " + result);
    }
}
```

### 5. 适用场景与注意事项

- **适用场景**：
  - **CPU 密集型**的计算任务。
  - 问题本身可以被**递归分解**。例如，大规模数据处理、矩阵乘法、大集合排序、图像处理等。
- **不适用场景/注意事项**：
  1.  **不适用于 I/O 密集型任务**：如果任务中包含大量阻塞的 I/O 操作（如数据库查询、网络请求），工作窃取机制将完全失效。一个线程被阻塞后，它无法再去窃取其他任务，可能导致整个 `ForkJoinPool` 的线程都被阻塞，造成“线程饥饿”。
  2.  **`fork()` 和 `join()` 的调用**：`join()` 方法会阻塞，确保 `fork()` 出的子任务都在 `join()` 之前被调用。
  3.  **`invoke()` vs. `execute()` vs. `submit()`**：向 `ForkJoinPool` 提交任务时，`invoke()` 是同步的，会等待结果返回；`execute()` 和 `submit()` 是异步的。

## Java 中的 CompletableFuture？

`CompletableFuture` 是自 Java 8 引入的，堪称 Java 异步编程领域的“核武器”。它彻底改变了我们处理异步操作的方式，将 Java 从过去笨拙的 `Future` 模式，带入了现代化的、函数式风格的、非阻塞的异步编程新纪元。

### 1. `Future` 的困境：为何需要 `CompletableFuture`？

在 Java 8 之前，我们处理异步任务主要依赖 `ExecutorService.submit()` 返回的 `Future` 对象。但 `Future` 有几个非常明显的、令人痛苦的局限性：

1.  **阻塞式获取结果**：要获取异步任务的结果，你只能调用 `future.get()` 方法。这个方法会**阻塞**当前线程，直到任务完成。这与异步编程的初衷背道而驰，我们异步化就是为了不阻塞，结果为了拿一个结果，又把自己阻塞了。
2.  **无法链式调用**：`Future` 无法告诉你任务何时完成，你只能去轮询 `isDone()` 或者直接 `get()` 阻塞。因此，你很难实现这样的逻辑：“当第一个任务完成后，用它的结果去执行第二个任务，再用第二个任务的结果去执行第三个任务……” 这种操作会写出非常丑陋、低效的“阻塞链”。
3.  **没有统一的异常处理**：`get()` 方法会将任务中的异常包装在 `ExecutionException` 中抛出，你必须在 `try-catch` 块中处理，无法以一种更优雅的回调方式处理。
4.  **无法组合多个 `Future`**：你很难优雅地实现“等待多个异步任务全部完成后再做某件事”或者“只要有一个异步任务完成就做某件事”的逻辑。

`CompletableFuture` 的诞生，就是为了彻底解决以上所有问题。

### 2. 核心优势：非阻塞与回调

`CompletableFuture` 的核心思想是**回调 (Callback)**。它不再需要你主动去“get”结果，而是你预先定义好：“当任务完成后，请自动用它的结果去执行这段代码”。

**一个生动的比喻**：

- `Future` 就像去一家老式餐厅点菜。你下单后，只能坐在桌子旁**死等**，直到服务员把菜端上来（`get()`阻塞）。
- `CompletableFuture` 就像用外卖 App 点餐。你下单后，就可以去做任何其他事情。当外卖送到时，App 会**通知**你（回调），你再去取餐。你甚至可以预设：“取到外卖后（`thenApply`），立即去冰箱拿瓶可乐（下一个任务）”。

### 3. `CompletableFuture` 的四大核心能力

`CompletableFuture` 庞大的 API 可以归纳为四大类核心能力：

#### a. 异步任务的创建 (Creation)

你可以轻松地启动一个异步任务：

- **`runAsync(Runnable runnable)`**: 运行一个没有返回值的异步任务。
- **`supplyAsync(Supplier<U> supplier)`**: 运行一个有返回值的异步任务。

这两个方法都有一个重载版本，可以传入一个自定义的 `Executor` (线程池)。**如果不传，默认会使用 `ForkJoinPool.commonPool()`**。这是一个非常重要的知识点，因为 `commonPool` 是全局共享的，如果你的任务是耗时很长的 I/O 操作，可能会占满 `commonPool` 的线程，影响其他使用它的组件。

#### b. 结果的处理与回调 (Callback)

当任务完成后，可以触发一系列的回调操作，这些操作都以 `then` 开头：

- **`thenApply(Function<? super T,? extends U> fn)`**: 接收上一个任务的结果，进行**转换**，并返回一个新的结果。**有入参，有返回值**。
  - 例：`cf.thenApply(userId -> getUserById(userId))`
- **`thenAccept(Consumer<? super T> action)`**: 接收上一个任务的结果，进行**消费**，但**不返回**任何结果。**有入参，无返回值**。
  - 例：`cf.thenAccept(user -> System.out.println(user.getName()))`
- **`thenRun(Runnable action)`**: **不关心**上一个任务的结果，只要它完成了，就执行指定的 `Runnable`。**无入参，无返回值**。
  - 例：`cf.thenRun(() -> System.out.println("操作完成"))`

**关键点**：以上每个方法都有一个 `*Async` 的版本（如 `thenApplyAsync`）。

- 不带 `Async` 的方法，其回调任务可能由**上一个任务的线程**执行，也可能由**调用 `thenApply` 的主线程**执行，不确定。
- 带 `Async` 的方法，其回调任务会被提交到线程池中**异步执行**，从而保证了主流程的非阻塞。

**异常处理**：

- **`exceptionally(Function<Throwable, ? extends T> fn)`**: 类似于 `try-catch` 中的 `catch` 块。当任何前面的步骤出现异常时，这个回调会被触发，提供一个处理异常并返回一个默认值的机会。

#### c. 任务的组合与编排 (Composition)

这是 `CompletableFuture` 最强大的地方，可以像搭积木一样编排复杂的异步流程。

- **串行关系 (AND)**:

  - `thenCompose(Function<? super T, ? extends CompletionStage<U>> fn)`: 用于连接两个**有依赖关系**的异步任务。它接收上一个任务的结果作为参数，并要求返回一个新的 `CompletableFuture`。
  - **`thenApply` vs. `thenCompose` (面试高频点)**：
    - `thenApply` 用于同步转换：`CompletableFuture<User>` -> `thenApply(user -> user.getName())` -> `CompletableFuture<String>`
    - `thenCompose` 用于异步转换：`CompletableFuture<UserId>` -> `thenCompose(userId -> getUserByIdAsync(userId))` -> `CompletableFuture<User>`

- **并行关系 (AND)**:
  - `thenCombine(CompletionStage<? extends U> other, BiFunction<? super T,? super U,? extends V> fn)`: 将**两个独立**的 `CompletableFuture` 的结果**合并**起来。当两个任务都完成后，用它们的结果作为参数调用 `BiFunction`。
  - `allOf(CompletableFuture<?>... cfs)`: 等待**所有**给定的 `CompletableFuture` 都执行完成。返回 `CompletableFuture<Void>`。
- **并行关系 (OR)**:
  - `anyOf(CompletableFuture<?>... cfs)`: 只要**任意一个**给定的 `CompletableFuture` 执行完成，就结束。返回 `CompletableFuture<Object>`。

### 4. 实践场景对比：从 `Future` 到 `CompletableFuture`

**需求**：查询商品信息，然后根据商品 ID 查询价格，再根据商品 ID 查询库存，最后组合成商品详情。

#### 使用 `Future` 的“地狱”模式：

```java
ExecutorService executor = Executors.newFixedThreadPool(10);
Future<Product> productFuture = executor.submit(() -> getProductInfo());

// 阻塞等待商品信息
Product product = productFuture.get();

// 拿到商品信息后，再分别提交价格和库存的查询任务
Future<Price> priceFuture = executor.submit(() -> getPrice(product.getId()));
Future<Stock> stockFuture = executor.submit(() -> getStock(product.getId()));

// 再次阻塞等待价格和库存
Price price = priceFuture.get();
Stock stock = stockFuture.get();

ProductDetail detail = new ProductDetail(product, price, stock);
```

**问题**：整个流程是串行阻塞的，异步的优势荡然无存。

#### 使用 `CompletableFuture` 的优雅模式：

```java
// 1. 异步获取商品基本信息
CompletableFuture<Product> productCf = CompletableFuture.supplyAsync(() -> getProductInfo());

// 2. 获取商品信息后，再去异步获取价格 (串行依赖)
CompletableFuture<Price> priceCf = productCf.thenComposeAsync(product ->
    CompletableFuture.supplyAsync(() -> getPrice(product.getId()))
);

// 3. 获取商品信息后，再去异步获取库存 (串行依赖)
CompletableFuture<Stock> stockCf = productCf.thenComposeAsync(product ->
    CompletableFuture.supplyAsync(() -> getStock(product.getId()))
);

// 4. 当价格和库存都获取到后，进行组合 (并行组合)
CompletableFuture<ProductDetail> detailCf = priceCf.thenCombine(stockCf, (price, stock) -> {
    // 此时还需要 productCf 的结果，但它肯定已经完成了
    Product product = productCf.join(); // join() 是非受检异常版的 get()
    return new ProductDetail(product, price, stock);
});

// 5. 最终获取结果或继续处理
ProductDetail detail = detailCf.join();
```

**优势**：整个流程被声明式地编排成一个非阻塞的管道，最大化了并行度（价格和库存的查询是并行的），代码清晰且高效。

### 总结

`CompletableFuture` 是对 Java 异步编程的根本性升级：

- 它用**非阻塞的回调机制**取代了 `Future` 的阻塞式 `get()`。
- 它提供了**声明式、函数式**的 API，可以像流式操作一样**组合和编排**复杂的异步任务。
- 它解决了**链式调用、异常处理、多任务组合**等核心痛点。

## Java 中的 `java.util.concurrent` 包？

`java.util.concurrent`（通常简称为 JUC）是 Java 并发编程的核心，也是 Java 从一门语言走向一个成熟的企业级平台的关键里程碑。这个包由并发编程大师 Doug Lea 设计和实现，它提供了一整套高级的、高性能的并发工具，极大地简化了并发程序的开发。

### JUC 的六大核心支柱

#### 1. Executors (线程执行器框架)

这是 JUC 中最基础也是最重要的部分，它提供了一套完整的异步任务执行框架。

- **核心思想**：将**任务的提交**与**任务的执行**（即线程的管理）彻底解耦。
- **关键组件**：
  - `Executor` & `ExecutorService`：定义了任务执行的标准接口。
  - `ThreadPoolExecutor`：线程池的核心实现，通过其七大参数可以精细地控制线程池的行为。
  - `Executors`：一个便捷的工厂类，用于创建预设的线程池（如 `newFixedThreadPool`），但因其潜在的 OOM 风险，在生产环境中**不推荐使用**。
  - `Future<V>` & `Callable<V>`：`Runnable` 没有返回值，`Callable` 则可以。`Future` 代表一个异步计算的结果，可以用来检查任务是否完成、获取结果或取消任务。
  - `CompletableFuture` (Java 8+)：对 `Future` 的革命性增强，支持非阻塞的回调、组合、编排，是现代 Java 异步编程的首选。

#### 2. Concurrent Collections (并发集合)

JUC 提供了一系列线程安全的集合类，它们的性能远超于使用 `Collections.synchronizedXXX()` 包装的传统集合。

- **核心思想**：通过更精细的锁机制或无锁算法，在保证线程安全的同时，最大化并发性能。
- **关键组件**：
  - `ConcurrentHashMap`：高性能的线程安全哈希表。JDK 1.7 使用分段锁，JDK 1.8+ 使用 CAS + `synchronized`，是面试和实战中的重中之重。
  - `CopyOnWriteArrayList` / `CopyOnWriteArraySet`：适用于“读多写少”场景的并发 List/Set，通过“写入时复制”实现读操作的无锁并发。
  - **`BlockingQueue` 接口**：专为生产者-消费者模式设计的阻塞队列，是构建线程池、消息队列等系统的基础。其主要实现包括：
    - `ArrayBlockingQueue`：基于数组的有界队列。
    - `LinkedBlockingQueue`：基于链表的可选界队列。
    - `SynchronousQueue`：不存储元素的“传递”队列。
    - `PriorityBlockingQueue`：支持优先级的无界队列。
    - `DelayQueue`：支持延迟获取的无界队列。

#### 3. Locks (显式锁)

提供了比内置 `synchronized` 关键字更灵活、更强大的锁机制。

- **核心思想**：将锁的获取和释放操作对象化，提供更多控制。
- **关键组件**：
  - `Lock` 接口：定义了锁的基本操作。
  - `ReentrantLock`：可重入的互斥锁，提供了公平/非公平选择、可中断的锁获取、可超时的锁获取等高级功能。
  - `ReentrantReadWriteLock`：读写锁，允许多个读线程并发访问，但在写时互斥，是“读多写少”场景的性能优化利器。
  - `Condition`：与 `Lock` 配合使用，提供了 `wait/notify` 的功能，但更强大，可以创建多个条件队列，实现更精确的线程通信。

#### 4. Atomic Variables (原子类)

提供了对单个变量进行无锁（lock-free）的、原子性的操作。

- **核心思想**：基于硬件支持的 **CAS (Compare-And-Swap)** 指令，实现乐观锁，避免了传统锁带来的线程阻塞和上下文切换开销。
- **关键组件**：
  - **基本类型**：`AtomicInteger`, `AtomicLong`, `AtomicBoolean`, `AtomicReference`。
  - **数组类型**：`AtomicIntegerArray` 等。
  - **字段更新器**：`AtomicIntegerFieldUpdater` 等。
  - **累加器 (Java 8+)**：`LongAdder`, `DoubleAdder`。在高并发下，通过分散热点，其性能远超 `AtomicLong`，是高并发计数统计的首选。

#### 5. Synchronizers (同步辅助工具)

用于协调多个线程之间复杂的协作关系。

- **核心思想**：提供预置的、高级的同步模式，让开发者无需从头构建。
- **关键组件**：
  - `CountDownLatch`：一次性的“倒计时门闩”，用于一个线程等待多个线程完成某项操作。
  - `CyclicBarrier`：可重用的“循环栅栏”，用于一组线程互相等待，直到所有线程都到达一个公共屏障点。
  - `Semaphore`：信号量，用于控制同时访问某个资源的线程数量，常用于限流或资源池管理。
  - `Exchanger`：用于两个线程之间安全地交换数据。

#### 6. Fork/Join Framework (分支/合并框架)

一个专为并行计算设计的框架。

- **核心思想**：采用“分而治之”的策略，将大任务递归地分解成小任务，再将小任务的结果合并。其精髓在于**工作窃取 (Work-Stealing)** 算法，能极大地提高多核 CPU 的利用率。
- **关键组件**：
  - `ForkJoinPool`：专用的线程池。
  - `ForkJoinTask`：任务的基类，主要使用其子类：
    - `RecursiveTask<V>`：有返回值的任务。
    - `RecursiveAction`：无返回值的任务。

### JUC 的共同基石：AQS (AbstractQueuedSynchronizer)

在上述众多组件的背后，有一个“无名英雄”，它就是 `AbstractQueuedSynchronizer` (AQS)。

- **它是什么**：AQS 是一个用来构建锁和同步器的**基础框架**。
- **它的作用**：它封装了同步状态的管理（通过一个 `volatile int state` 变量）、线程的排队（通过一个 CLH 虚拟双向队列）、阻塞与唤醒等一系列底层、复杂的操作。
- **谁依赖它**：JUC 中许多大名鼎鼎的类，如 `ReentrantLock`, `ReentrantReadWriteLock`, `Semaphore`, `CountDownLatch` 等，都是基于 AQS 框架实现的。它们通过继承 AQS 并实现其 `tryAcquire`/`tryRelease` 等模板方法，来定义自己独特的同步逻辑。

理解 AQS，是深入理解 JUC 包内部工作原理的关键。

### 总结

`java.util.concurrent` 包是 Java 并发编程的现代化基石。它通过**执行器框架**、**并发集合**、**显式锁**、**原子类**、**同步辅助工具**和 **Fork/Join 框架**这六大支柱，提供了一整套从宏观任务调度到微观数据操作的解决方案。这些组件大多构建在强大的 **AQS 框架**之上，共同构成了一个功能丰富、性能卓越、设计精良的并发工具库，使得开发者能够更安全、更高效地构建大规模的并发应用程序。
