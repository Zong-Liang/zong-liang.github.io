---
title: "01-潮玩项目面试题"
date: 2024-03-15 14:54:15 +0800
categories: [面试题, 潮玩项目面试题]
tags: [面试八股, 潮玩项目面试题]
pin: false
toc: true
math: true
---

## 详细说说基于 Cache Aside Pattern 构建的多级缓存体系？

### 多级缓存体系的构建与协作

我们这套多级缓存体系主要是为了应对秒杀场景下的高并发读取请求，最大限度地降低数据库压力。它主要分为三级：**Caffeine 本地缓存**、**Redis 集中式缓存**和最后的**MySQL 数据库**。

整个体系遵循 **Cache Aside Pattern (旁路缓存模式)** 的核心思想，协作流程如下：

1.  **读请求流程**：

    - 当一个读请求（例如查询商品详情）到达应用服务器时，我们会首先查询 **Caffeine 本地缓存**。Caffeine 是一个高性能的 Java 本地缓存库，直接在应用内存中，速度最快。
    - 如果本地缓存命中，则直接返回数据，请求结束。
    - 如果本地缓存未命中，我们会接着查询 **Redis 集中式缓存**。Redis 是所有应用实例共享的，可以减少对数据库的重复查询。
    - 如果 Redis 缓存命中，我们会将数据返回给用户，并且**将这份数据写入到 Caffeine 本地缓存中**，这样后续该实例再有相同请求就可以直接从本地获取，提高了访问速度。
    - 如果 Redis 也未命中，这说明缓存里没有数据，我们会执行最后一步：查询 **MySQL 数据库**。
    - 从数据库查到数据后，我们会将数据**依次写入 Redis 缓存和 Caffeine 本地缓存**，最后再将数据返回给用户。

2.  **写请求流程 (数据更新)**：
    - 当需要更新数据时，我们采用的是 **"先更新数据库，再删除缓存"** 的策略。
    - 具体来说，我们会先去更新 MySQL 数据库中的数据。
    - 数据库更新成功后，我们会去**删除 Redis 缓存**以及**通过 Redis 发布/订阅机制通知集群中其他节点删除对应的 Caffeine 本地缓存**，从而保证数据的一致性。选择删除而不是更新缓存，是因为这样操作更简单，可以避免更新缓存时可能出现的复杂计算或关联查询。

### 存在的问题

这套体系虽然能应对大部分场景，但在高并发环境下，仍然存在一些经典的数据一致性问题，尤其是在“读写并发”的场景下：

1.  **缓存与数据库数据不一致**：

    - 这是一个经典问题。假设线程 A 更新了数据库，但在它删除缓存之前，线程 B 发起了读请求。这时线程 B 会读到缓存中的旧数据（脏数据），然后线程 A 才完成缓存删除。尽管后续的请求会重新加载新数据，但在这一瞬间，确实发生了数据不一致。
    - 更极端的情况下，如果采用“先删缓存，再更新数据库”的策略，线程 A 删了缓存，但还没来得及更新数据库，线程 B 就来读，发现缓存没有，就去读了数据库的旧数据并写回缓存。然后线程 A 才完成数据库更新。这会导致缓存中永久性地存储了脏数据。

2.  **并发更新下的“缓存双写不一致”问题**：
    - 当多个线程同时更新同一份数据时，问题会变得更复杂。例如，线程 A 和线程 B 同时更新数据。线程 A 更新数据库，然后线程 B 更新数据库。接着，线程 B 删除了缓存，然后线程 A 删除了缓存。
    - 此时，如果有一个读请求进来，它会从数据库中读取线程 B 更新后的最新值，并写入缓存。这个过程看起来没问题。但问题在于，我们无法保证数据库写操作和缓存删除操作的原子性与顺序性，这为数据不一致埋下了隐患。

### 为什么要引入 Redisson 读写锁

为了解决上述高并发场景下的数据一致性问题，我们引入了 **Redisson 分布式读写锁 (ReadWriteLock)**。它的核心作用是**保证“读写”和“写写”操作的互斥性，从而确保数据更新过程的原子性**。

具体实现如下：

1.  **写操作加“写锁”**：

    - 当有**更新数据**的请求时，我们会先尝试获取该数据对应的 **分布式写锁**。
    - 获取写锁成功后，才执行“**先更新数据库，再删除缓存**”的逻辑。
    - 操作完成后，**释放写锁**。
    - 如果在写操作期间，有其他**读请求**或**写请求**过来，它们会被阻塞。读请求需要等待写锁释放才能获取读锁，而写请求则需要等待写锁释放才能尝试获取写锁。这就从根本上避免了“一个线程在写，另一个线程在读旧数据”的问题。

2.  **读操作加“读锁”**：
    - 当有**读数据**的请求，并且**缓存未命中**，需要从数据库加载数据时，我们会先获取该数据的 **分布式读锁**。
    - 获取读锁后，再去查询数据库，并将数据写入缓存。
    - 操作完成后，**释放读锁**。
    - **读锁是共享的**，也就是说，多个读请求可以同时持有读锁并行从数据库读取数据，不会相互阻塞，保证了读取性能。但是，当有线程持有读锁时，任何尝试获取**写锁**的线程都会被阻塞，直到所有读锁都释放。

**总结一下引入 Redisson 读写锁的价值：**

- **保证了原子性**：将“更新 DB + 删除缓存”这个组合操作变成了一个原子操作，防止了并发冲突。
- **解决了数据不一致问题**：通过“写锁”的排他性，确保了在数据更新期间，不会有任何读请求读到旧的缓存数据，也不会有其他写请求干扰。
- **兼顾了性能**：使用“读写锁”而不是普通的“互斥锁”，允许多个读请求并发执行，只有在“写”操作时才进行互斥，最大限度地保留了系统的读性能。

通过这套机制，我们就在保证高性能的同时，也解决了并发更新下的数据一致性难题，为秒杀业务的稳定运行提供了坚实的基础。

---

## 详细说说什么是缓存穿透、击穿、雪崩以及在你这个多级缓存体系中的你是如何应对这些问题的？

### 1. 缓存穿透 (Cache Penetration)

- **什么是缓存穿透？**
  缓存穿透指的是客户端请求查询一个**根本不存在**的数据。因为这个数据在缓存中不存在，请求会直接打到数据库；又因为数据库中也没有这个数据，所以数据库也查不出结果，并且不会将结果写回缓存。这就导致了每次对这个不存在的数据的请求，都会绕过缓存，直接查询数据库。如果攻击者利用这个漏洞，构造大量不存在的 Key 进行恶意攻击，就会给数据库带来巨大的压力，甚至导致数据库宕机。

- **我的解决方案：**

  1.  **布隆过滤器 (Bloom Filter)**：这是我简历中提到的主要解决方案。布隆过滤器是一种空间效率极高的数据结构，它可以用来判断一个元素是否在一个集合中。

      - **实现方式**：我会将所有可能存在的数据的 Key（例如，所有商品的 ID）提前加载到一个足够大的布隆过滤器中，这个过滤器可以部署在像 Redis 这样的共享存储中，或者分发到各个服务实例的内存里。
      - **查询流程**：当一个读请求进来时，它会先去布隆过滤器里查询这个 Key 是否存在。
        - 如果布隆过滤器判断 **不存在**，那么我们可以 100% 确定这个数据在数据库中也绝对不存在，于是直接返回空结果，请求链路到此为止，根本不会触及后端的缓存和数据库。
        - 如果布隆过滤器判断 **可能存在**（因为它有极低的误判率），我们再继续走后续的“查询缓存 -> 查询数据库”的流程。
      - **优点**：这种方式将绝大多数对无效 Key 的查询请求都在第一道关卡拦截了下来，极大地保护了后端系统。

  2.  **缓存空对象 (Cache Null Values)**：作为一种补充或更轻量级的方案，我们也可以缓存空对象。
      - **实现方式**：当从数据库中查询一个 Key 发现没有对应数据时，我们并不直接返回，而是在缓存（主要是 Redis）中为这个 Key 存入一个特殊的空值（比如一个固定的字符串 "NULL"）。
      - **设置较短的过期时间**：为了防止存储过多的空值占用缓存空间，并且能在数据被创建后及时更新缓存，我们会给这个空值设置一个比较短的 TTL（Time-To-Live），比如 1 到 5 分钟。
      - **优点**：实现简单，也能有效地防止在短时间内对同一个不存在的 Key 的重复攻击。

### 2. 缓存击穿 (Cache Breakdown)

- **什么是缓存击穿？**
  缓存击穿与穿透不同，它指的是一个 **热点 Key**（被频繁访问的数据）。在这个 Key 刚好失效的瞬间，成千上万的并发请求同时涌入，由于此时缓存已经失效，这些请求会全部直接打向数据库，就像把数据库“击穿”了一样，造成数据库压力瞬间剧增。它的特点是 **“单个热点 Key 过期”**。

- **我的解决方案：**

  我采用了“**逻辑过期 + Redisson 互斥锁**”的组合策略，这是一种既能保证高可用又能解决击穿问题的优雅方案。

  1.  **逻辑过期**：我们不给 Redis 中的热点数据设置物理上的 TTL。而是在存储数据时，额外增加一个逻辑过期时间字段，例如 `{"data": {...}, "expireAt": "2025-08-25T10:00:00Z"}`。
  2.  **加锁重建**：
      - 当线程来查询数据时，它从 Redis 中取出数据，并检查其中的逻辑过期时间。
      - **如果未过期**：直接返回数据。
      - **如果已过期**：此时，该线程会尝试获取一个与该 Key 绑定的 **Redisson 分布式互斥锁**。
        - **获取锁成功**：这个线程就获得了重建缓存的“资格”。它会开启一个独立的线程去执行查询数据库、更新缓存中数据和新的逻辑过期时间等操作，然后释放锁。**重要的是，当前请求线程不需要等待重建完成，而是可以直接将 Redis 中存储的旧数据先返回**。这保证了即使用户在缓存重建的瞬间访问，系统的延迟也几乎不受影响。
        - **获取锁失败**：这说明已经有其他线程正在重建缓存了。当前线程无需做任何事，同样直接将 Redis 中的旧数据返回即可。

  通过这种方式，我们确保了任何时候都只有一个线程去重建数据库，并且不会阻塞用户请求，完美地避免了缓存击穿带来的“雪崩效应”。

### 3. 缓存雪崩 (Cache Avalanche)

- **什么是缓存雪崩？**
  缓存雪崩是指在某一个时间段，缓存集中大规模失效，或者缓存服务自身宕机，导致所有请求都无法在缓存中处理，瞬时全部涌向数据库，最终导致数据库资源耗尽而崩溃。它有两种典型场景：

  1.  大量 Key 在同一时间集体过期。
  2.  Redis 服务实例整体宕机或网络不可达。

- **我的解决方案：**

  1.  **针对“集体过期”：TTL 附加随机值**

      - 这是我在简历中提到的“TTL 附加随机值策略”。在为缓存数据设置过期时间时，我们不在一个固定的基础时间上（例如 `3600` 秒），而是额外增加一个小的随机数（例如 `3600 + random(1, 600)` 秒）。
      - 这样一来，每个 Key 的过期时间就被均匀地打散到了一个时间段内，避免了在同一时刻集中失效的风险，从而让缓存重建的压力在时间上得以平滑分布。

  2.  **针对“缓存服务宕机”：构建高可用的系统**
      - **缓存层高可用**：首先，部署高可用的 Redis 集群，比如使用 **Redis Sentinel（哨兵模式）** 或 **Redis Cluster（集群模式）**，确保当主节点宕机时能够自动进行故障转移，保证缓存服务自身的健壮性。
      - **服务降级与熔断**：在应用层面，我们会集成 **Hystrix** 或 **Resilience4J** 这样的熔断组件。当应用检测到 Redis 连接持续异常时，会自动触发熔断机制，暂时关闭对缓存的访问。后续的请求会执行预设的降级逻辑，比如：
        - 直接返回一个兜底的默认数据。
        - 访问本地缓存 Caffeine 作为二级缓存。
        - 向用户返回“系统繁忙，请稍后重试”的友好提示。
          这样做的目的是为了“牺牲非核心功能，保护核心功能”，防止因缓存问题而导致整个系统链路的全面瘫痪。
      - **多级缓存体系**：我所设计的 **Caffeine (本地缓存) + Redis (分布式缓存)** 体系本身就具备一定的容灾能力。即使 Redis 发生故障，对于那些已经被加载到服务实例内存中的热点数据，Caffeine 依然能提供服务，这在一定程度上减轻了 Redis 宕机带来的冲击。

总结一下，对于缓存的三大问题，我的应对策略是一个组合拳：用 **布隆过滤器** 挡住穿透攻击，用 **逻辑过期+分布式锁** 解决热点击穿，用 **TTL 随机值+集群高可用+服务降级** 来预防和应对雪崩。这一整套体系确保了我的缓存在高并发场景下的高效、稳定和可靠。

---

## 详细说说你的成果”商品详情页 QPS 从 500 提升至 4,000+，数据库核心读压力降低 95%以上“是怎么得到的？

这个压测过程是评估我所构建的多级缓存架构效果最关键的一环。下面我将详细阐述我是如何设计和执行压测，以及这些具体数值是如何计算出来的。

### 1. 压测工具与监控体系

为了确保测试的准确性和可观测性，我搭建了一套完整的工具链：

- **压测工具**：我主要使用 **Apache JMeter**。它是一款开源的、功能强大的压力测试工具，可以模拟大量并发用户向目标服务器发送请求。
- **监控体系**：我使用了简历中提到的 **Prometheus + Grafana** 组合。
  - **Prometheus** 负责从各个组件拉取监控指标，包括：
    - **应用服务器指标**：通过 Spring Boot Actuator 暴露的 JVM 内存、CPU 使用率、线程数、HTTP 请求 QPS 和响应时间等。
    - **数据库指标**：通过 `mysqld_exporter` 采集 MySQL 的 QPS、连接数、慢查询、CPU 占用率等。
    - **缓存指标**：通过 `redis_exporter` 采集 Redis 的命中率、内存使用、连接数等。
  - **Grafana** 负责将 Prometheus 采集到的数据进行可视化展示，让我可以在压测过程中实时观测系统各项指标的变化趋势。

### 2. 压测流程与数值来源

我分别对“优化前”和“优化后”的系统架构进行了压测，以进行精确的对比。

#### **第一步：基准测试（优化前 - “QPS 500”）**

- **测试环境**：模拟了一个最原始的架构，即请求直接通过应用服务器查询 MySQL 数据库，没有任何缓存。
- **测试脚本**：在 JMeter 中，我创建了一个测试计划，模拟用户请求商品详情页的 API 接口（例如 `GET /api/product/{productId}`）。为了避免数据库对单一 ID 查询的内部缓存影响测试结果，`productId` 设置为在一个较大的范围内随机取值。
- **执行与观测**：我从一个较低的并发用户数开始，逐步增加（这通常被称为“梯度加压”）。同时，我密切关注 Grafana 上的监控面板。
- **“500 QPS”的由来**：当并发数增加到一定程度时，我观察到以下瓶颈现象：

  1.  **数据库 CPU 占用率** 率先达到警戒线（例如 80%~90%）。
  2.  API 接口的 **P95 响应时间** 开始急剧上升，从几十毫秒飙升到几百甚至上千毫秒。
  3.  系统的 **QPS** 无法再线性增长，稳定在了 **500 左右**。

  这个 **500 QPS** 就是在数据库成为性能瓶颈时，系统所能承受的最大吞吐量。我将其定义为我们优化的基准线（Baseline）。

#### **第二步：优化后测试（“QPS 4,000+”）**

- **测试环境**：部署了我设计的多级缓存架构（Caffeine + Redis）。
- **缓存预热**：在正式压测前，我会先进行一轮“缓存预热”，即通过脚本访问一批热点商品的详情页，确保这些数据已经被加载到 Redis 和各个应用实例的 Caffeine 本地缓存中。这更贴近真实的线上场景。
- **执行与观测**：我使用了与基准测试 **完全相同** 的 JMeter 脚本，同样采用梯度加压的方式。
- **“4,000+ QPS”的由来**：

  1.  在加压过程中，我发现系统的吞吐能力有了质的飞跃。绝大多数请求（缓存命中率高达 99% 以上）都在缓存层（优先是速度最快的本地缓存）被处理掉了。
  2.  数据库的压力非常小，其 CPU 占用率始终维持在极低的水平。
  3.  系统的瓶颈从数据库转移到了应用服务器的 CPU 或网络 I/O。
  4.  最终，在应用服务器资源达到瓶颈前，系统的 QPS 能够稳定地维持在 **4,000 以上**，并且 API 的 P95 响应时间依然非常低（例如 50 毫秒以内）。这个 4,000+ 就是优化后架构的吞吐量。

  这个成果（从 500 到 4,000+）直观地证明了多级缓存将系统的核心瓶颈从慢速的、有状态的数据库，成功转移到了快速的、可水平扩展的应用层的巨大成功。

#### **第三步：“数据库核心读压力降低 95% 以上”的计算**

这个指标是衡量缓存效果最直接的数据，它同样来自于 Prometheus 的监控。

- **计算方式**：我主要关注数据库的 `com_select` 指标（即 SELECT 查询的 QPS）。
  - **优化前**：在 500 QPS 的系统吞吐下，数据库的 `select` QPS 也接近 **500**（因为几乎每个请求都需要查库）。
  - **优化后**：在 4,000+ QPS 的系统吞吐下，我观察到数据库的 `select` QPS 稳定在一个非常低的数值，大约在 **20-25 之间**。这些少数的数据库查询主要来自于缓存未命中的冷数据访问和缓存重建过程。
- **计算**：
  压力降低百分比 = `(优化前的数据库QPS - 优化后的数据库QPS) / 优化前的数据库QPS`
  = `(500 - 25) / 500`
  = `475 / 500`
  = `0.95`，即 **95%**。

因此，“数据库核心读压力降低 95% 以上”这个结论，是通过对比两次压测中数据库的实际查询负载得出的精确计算结果。

总结来说，简历中的这两个成果数据，都是基于一套科学、严谨的性能压测流程和精细化的监控体系得出的，它们真实地反映了我所实施的架构优化带来的显著性能提升。

---

## 详细说说你是如何通过 Redis+Lua 脚本进行库存资格预检的？

**对于商品详情等静态数据：** 执行完整的缓存策略 Caffeine -> Redis -> MySQL，最大化读性能。

**对于商品库存等动态数据：** 请求会绕过 Caffeine，直接访问 Redis，利用其单线程模型和 Lua 脚本的原子性来保证高并发下的数据强一致性。

Redis+Lua 脚本是我在“潮玩新零售”项目中，为实现“零超卖、高吞吐的异步秒杀方案”所设计的核心环节之一。它位于整个请求链路的最前端，承担着**流量过滤阀**的关键角色。

### 1. 为什么需要“库存资格预检”？

在像“新品秒杀”这样的大流量场景下，系统面临的最大挑战是瞬时并发极高。假设我们有 100 件商品，但在秒杀开始的第一秒，可能会有 10 万甚至 100 万个购买请求涌入。

如果我们采用传统的方式，让每个请求都去数据库里查询库存、扣减库存，会发生什么？

1.  **数据库崩溃**：数据库的连接数有限，磁盘 I/O 性能也有限。面对如此巨大的并发写请求，数据库会因为不堪重负而锁死、响应超时，最终导致整个系统瘫痪。
2.  **严重超卖**：在高并发下，多个线程可能同时读取到“库存 > 0”，然后都执行扣减库存操作，这会绕过简单的业务判断，导致库存被扣减成负数，即“超卖”。虽然数据库的事务和行锁能缓解一部分问题，但在这种量级下，锁竞争会急剧恶化性能，甚至导致死锁。

核心痛点在于：**99.9% 的请求都是无效的**（因为商品早就被抢光了）。让这些无效请求穿透到数据库层面，是对宝贵数据库资源的巨大浪费。

因此，我设计了“库存资格预签”这一步，其**核心目标**就是：**在系统最前端、性能最高的层次，以最快的速度过滤掉绝大多数无效请求，只放行少量有效的请求进入后续的核心业务流程。** 而 Redis，作为内存数据库，正是执行这个任务的最佳选择。

### 2. 如何通过 Redis + Lua 脚本实现？

单纯使用 Redis 的多个命令（例如 `GET` + `DECR`）组合，在并发场景下并非原子操作，依然存在超卖风险。例如：

- 线程 A `GET` 到库存为 1。
- 线程 B 也在同时 `GET` 到库存为 1。
- 线程 A 判断库存 > 0，执行 `DECR`，库存变为 0。
- 线程 B 也判断库存 > 0，执行 `DECR`，库存变为 -1。这就超卖了。

为了解决这个问题，我利用了 **Lua 脚本**。Redis 在执行 Lua 脚本时，会将其作为一个 **原子操作** 来执行，中途不会被其他任何命令打断。这就从根本上杜绝了并发竞争问题。

我的实现步骤如下：

**第一步：数据准备**

在秒杀活动开始前，我们会将商品的库存信息预加载到 Redis 中。通常使用一个简单的 String 结构：
`SET stock:product_123 100`

**第二步：编写 Lua 脚本**

我编写了一个 Lua 脚本，这个脚本会接收商品 Key 和用户 ID 作为参数，其内部逻辑如下：

```lua
-- KEYS[1]: 商品库存的 Key (例如: stock:product_123)
-- ARGV[1]: 当前用户的 ID (用于防止一人多单)
-- KEYS[2]: 已购买用户集合的 Key (例如: purchased_users:product_123)

-- 1. 判断用户是否已经抢购过
if redis.call('sismember', KEYS[2], ARGV[1]) == 1 then
    return 2 -- 返回 2, 代表用户重复购买
end

-- 2. 获取当前库存
local stock = redis.call('get', KEYS[1])

-- 3. 判断库存是否存在且大于 0
if not stock or tonumber(stock) <= 0 then
    return 0 -- 返回 0, 代表库存不足
end

-- 4. 如果库存充足, 则扣减库存
redis.call('decr', KEYS[1])

-- 5. 将用户 ID 添加到已购买集合中
redis.call('sadd', KEYS[2], ARGV[1])

return 1 -- 返回 1, 代表抢购成功, 获得资格
```

**第三步：Java 应用调用**

在 Java 代码中，我们通过 `redisTemplate.execute(script, keys, args)` 来执行这个 Lua 脚本。应用会根据脚本返回的结果（0, 1, 或 2）来给用户不同的响应：

- **返回 1 (成功)**：告知用户“您已成功抢到，订单正在处理中”，然后异步地将订单信息发送到 RocketMQ 消息队列。
- **返回 0 (库存不足)**：直接告知用户“商品已售罄”。
- **返回 2 (重复购买)**：直接告知用户“您已购买过该商品”。

### 3. 这么做的好处是什么？

总结下来，采用 Redis + Lua 脚本进行库存资格预检，带来了四大核心优势：

1.  **保证原子性**：将“读库存 -> 判断 -> 写库存”这一系列操作捆绑成一个不可分割的单元，彻底解决了高并发下的数据一致性问题，是实现“零超卖”的基石。
2.  **性能极致**：所有操作都在内存中完成，单个请求的响应时间在微秒级别。这使得它能够轻松应对每秒数万甚至数十万的请求，将 99% 以上的无效请求在第一道关卡就拦截下来。
3.  **保护后端系统**：通过在前端挡住绝大部分流量，极大地降低了后方消息队列、应用服务乃至数据库的压力。这是实现“数据库核心读压力降低 95% 以上”和“秒杀接口吞吐量提升近 50 倍”的关键技术之一。
4.  **实现业务解耦**：资格预检通过后，只是代表用户“拿到了一个购买凭证”。真正的订单创建、支付等复杂流程，被解耦到了后端的异步任务中，通过消息队列进行削峰填谷，进一步提升了系统的稳定性和用户体验。

所以，Redis+Lua 方案是我整个高并发秒杀架构的“流量防火墙”和“资格分发器”，它对于保证系统的最终成功至关重要。

---

## 详细说说你是怎么使用数据库乐观锁与 Redisson 分布式锁的？

在“潮玩新-零售”项目中，数据库乐观锁和 Redisson 分布式锁是我用来保障数据一致性的两把“利剑”。它们分别应用在不同的业务环节和并发冲突场景下，共同构成了我异步秒杀方案中的“双重保险机制”。

### 1. 数据库乐观锁

- **使用场景**：
  数据库乐观锁主要用在**异步消费消息队列（RocketMQ）后，真正扣减数据库库存**的环节。

- **为什么要在这里使用？**

  1.  **最终一致性的保障**：经过了前端 Redis+Lua 的预扣减，能够进入到这个环节的请求已经是“有效请求”了。但我们仍然需要一层最终的保障。乐观锁机制可以确保即使在极端情况下（例如 MQ 消息重复消费），数据库中的库存也不会被错误地多次扣减。
  2.  **性能考虑**：相比于悲观锁（如 `SELECT ... FOR UPDATE`），乐观锁在操作数据库时不会对数据行进行加锁，而是通过版本号或时间戳来判断数据是否被修改。这意味着在高并发写入时，它不会产生锁等待，吞吐性能更高。因为秒杀场景下写冲突的概率实际上已经被前端大大降低，所以乐观锁“先执行，后校验”的模式非常适合。

- **具体实现方式**：

  1.  **增加版本号字段**：我在商品库存表 `product_stock` 中增加了一个 `version` 字段，类型为整型，默认值为 0 或 1。

      ```sql
      CREATE TABLE `product_stock` (
        `id` BIGINT NOT NULL AUTO_INCREMENT,
        `product_id` BIGINT NOT NULL,
        `stock_count` INT NOT NULL,
        `version` INT NOT NULL DEFAULT 0,
        PRIMARY KEY (`id`)
      );
      ```

  2.  **更新时带上版本号条件**：当消费者服务从 RocketMQ 接收到“扣减库存”的消息后，它会执行一个带条件的 `UPDATE` 语句。

      - 首先，它会根据 `product_id` 查询出当前的库存和 `version`。
      - 然后，在执行 `UPDATE` 时，将 `version` 作为 `WHERE` 子句的一部分。

      ```sql
      UPDATE product_stock
      SET
        stock_count = stock_count - 1,
        version = version + 1
      WHERE
        product_id = #{productId}
      AND
        stock_count > 0 -- 确保库存充足
      AND
        version = #{currentVersion}; -- 核心：CAS (Compare and Swap) 操作
      ```

  3.  **判断更新结果**：应用会检查这个 `UPDATE` 语句影响的行数。
      - 如果返回影响行数为 `1`，说明本次更新成功。版本号匹配，且库存成功扣减。
      - 如果返回影响行数为 `0`，说明在“查询 `version`”到“执行 `UPDATE`”这极短的时间窗口内，有另一个线程已经修改了这条数据，导致 `version` 不匹配。此时，本次更新失败。应用可以根据业务需求选择重试（重新查询 `version` 再更新）或者直接标记为失败。

  通过这种方式，我们从数据库层面彻底杜绝了库存超卖的问题。

### 2. Redisson 分布式锁

- **使用场景**：
  Redisson 分布式锁的应用场景比乐观锁更广泛，主要用在需要**跨 JVM 进程、跨服务实例进行资源同步**的场景。在本项目中，主要有两处关键应用：

  1.  **解决缓存击穿**：如我之前所述，在重建热点 Key 的缓存时，使用 Redisson 的互斥锁来确保只有一个线程执行数据库查询和缓存回写操作。
  2.  **防止用户重复下单**：这是一个非常关键的业务层保险。在**异步创建订单**的逻辑开始之前，我会使用 Redisson 分布式锁。

- **为什么要在这里使用？**

  1.  **业务层面的防重**：虽然前端 Redis+Lua 脚本已经利用 `SADD` 命令防止了同一个用户 ID 的多次成功请求，但这只是“资格预检”层面的防重。在复杂的分布式系统中，可能会因为网络重试、MQ 消息重复等原因，导致同一个用户的下单请求被多次处理。
  2.  **保证幂等性**：创建订单是一个非幂等操作。如果不加控制，重复执行会导致为一个用户创建多个相同的订单。我们需要一个强有力的机制来保证“一个用户对一个秒杀商品只能成功创建一个订单”这个业务逻辑的原子性。
  3.  **跨服务同步**：如果创建订单的逻辑由一个微服务集群来处理，那么简单的本地锁（如 `synchronized` 或 `ReentrantLock`）是无效的，必须使用像 Redisson 这样的分布式锁，才能在所有服务实例之间同步状态。

- **具体实现方式**：

  1.  **确定锁的 Key**：锁的粒度需要精心设计。为了防止用户对特定商品重复下单，我会设计一个唯一的锁 Key，例如 `lock:create_order:{userId}:{productId}`。
  2.  **加锁与释放锁**：在创建订单的核心业务逻辑外层，包裹上加锁和解锁的操作。

      ```java
      RLock lock = redissonClient.getLock("lock:create_order:" + userId + ":" + productId);
      boolean isLocked = false;
      try {
          // 尝试加锁，设置等待时间和锁的自动释放时间，防止死锁
          isLocked = lock.tryLock(1, 10, TimeUnit.SECONDS);

          if (isLocked) {
              // --- 核心业务逻辑开始 ---
              // 1. 再次查询订单表，确认该用户是否已经有该商品的订单 (Double Check)
              // 2. 如果没有，则创建订单、写入数据库
              // --- 核心业务逻辑结束 ---
          } else {
              // 获取锁失败，说明有其他线程正在处理，直接返回或抛出异常
              log.warn("未能获取到创建订单的分布式锁，用户ID: {}, 商品ID: {}", userId, productId);
          }
      } catch (InterruptedException e) {
          Thread.currentThread().interrupt();
      } finally {
          if (isLocked && lock.isHeldByCurrentThread()) {
              lock.unlock(); // 确保锁被释放
          }
      }
      ```

### 总结：双重保险机制

综上所述，**数据库乐观锁** 和 **Redisson 分布式锁** 在我的项目中扮演了不同层面但互为补充的角色：

- **Redisson 分布式锁** 作用于 **业务逻辑层面**，它保护的是一个业务流程（如“创建订单”）的原子性和幂等性，防止的是业务逻辑的重复执行。它是一种 **过程锁**。
- **数据库乐观锁** 作用于 **数据存储层面**，它保护的是单条数据（如库存记录）在并发更新时的一致性，防止的是数据状态的错乱。它是一种 **数据锁**。

将两者结合，就构成了从业务入口到数据持久化的双重保险。即使某个环节出现意想不到的并发问题，另一层保险也能兜底，从而最大限度地保障了秒杀场景下数据和业务的最终正确性。

---

## 详细说说你是怎么使用 RocketMQ 对下单、扣减库存等核心流程进行异步化解耦的？

在我之前的回答中，我提到了使用 Redis+Lua 解决了前端流量的“资格预检”问题。但这仅仅是第一步，它保证了只有少量“合格”的请求能够进入后端。然而，创建订单和扣减数据库库存这些操作，涉及到数据库事务、多表写入，相比于内存操作，依然是非常耗时的。

如果让 Web 服务器同步等待这些操作完成，那么在秒杀的洪峰时刻，服务器的线程资源会很快被这些慢操作耗尽，导致系统吞-吐量急剧下降，用户会感觉页面一直在转圈，体验极差。

因此，我使用 RocketMQ 将整个流程拆分为“**同步前端响应**”和“**异步后端处理**”两个部分。

### 异步化解耦的具体实现流程

整个流程是这样的：

#### 1. 前端请求与消息发送（Producer 端）

- **入口**：用户的秒杀请求到达 Controller 层。
- **资格预检**：首先，系统通过调用 Redis+Lua 脚本，进行库存资格的原子性预检。
- **发送事务消息**：如果资格预检通过，证明用户“抢到了”。此时，我们**不会**立即去操作数据库。相反，我们会利用 RocketMQ 的**事务消息**机制来确保操作的原子性：
  1.  **发送半消息 (Half Message)**：生产者（Producer）先向 RocketMQ Broker 发送一个“半消息”。这个消息包含了创建订单所需的所有信息（`userId`, `productId` 等），但此时对于消费者是不可见的。
  2.  **执行本地事务**：半消息发送成功后，生产者会执行一个本地事务。在我的场景中，这个本地事务可以非常轻量，比如记录一条日志或在本地数据库的一个“秒杀活动日志表”中插入一条记录，标记该用户已获得资格。这一步主要是为了后续的消息状态回查。
  3.  **提交或回滚**：如果本地事务成功，生产者就向 Broker 发送 `COMMIT`，Broker 收到后，该消息才对消费者可见。如果本地事务失败，则发送 `ROLLBACK`，Broker 会删除该半消息。
- **立即响应用户**：在生产者将事务消息的流程发起后，Controller 就会**立即**向用户的浏览器返回一个“排队中，订单处理中”的友好提示。从用户的角度看，整个请求已经结束了，响应速度极快（几十毫秒级别），体验非常好。

#### 2. 消息缓冲与削峰填谷（Broker 端）

- **角色**：RocketMQ 在这里扮演了“**蓄水池**”的角色。秒杀开始的瞬间，成千上万的有效请求会转化为消息，在极短时间内涌入 RocketMQ 的 Topic 中。
- **核心作用**：它将前端瞬时的高并发脉冲流量，平滑地转化为后端消费者可以稳定处理的、速率可控的消息流。这就是典型的“**削峰填谷**”。它保护了后端脆弱的数据库资源，防止其被瞬时流量冲垮。

#### 3. 订单处理与库存扣减（Consumer 端）

这是整个流程中**双重保险机制（Redisson 锁 + 乐观锁）** 发挥作用的地方。我的消费者服务（`OrderService`）在监听到消息后，会执行以下一系列严谨的操作：

1.  **获取 Redisson 分布式锁**：

    - **目的**：**保证业务幂等性，防止重复下单**。由于网络等原因，消息可能会被重复投递。消费者在处理消息的第一步，就是尝试获取一个基于 `userId` 和 `productId` 的分布式锁（例如 `lock:create_order:{userId}:{productId}`）。
    - **逻辑**：如果获取锁失败，说明有另一个线程（可能是因为消息重复）正在处理该用户的同一商品的订单，当前消息就可以直接安全地丢弃并确认消费，从而避免了重复创建订单。

2.  **开启数据库事务**：成功获取分布式锁之后，才开始执行真正的数据库操作，并将它们全部包裹在一个大的数据库事务中 (`@Transactional`)。

3.  **事务内核心逻辑**：

    - **a. Double Check**：再次查询订单表，确认该用户是否真的没有该商品的订单。这是分布式锁之后的最后一道防线，是严谨的防御性编程实践。
    - **b. 扣减数据库库存（使用乐观锁）**：这是最终的数据一致性保障。消费者执行 `UPDATE` 语句来扣减库存，但这个语句会带上 `version` 字段作为条件，如：
      ```sql
      UPDATE product_stock
      SET stock_count = stock_count - 1, version = version + 1
      WHERE product_id = ? AND stock_count > 0 AND version = ?;
      ```
      如果 `UPDATE` 影响的行数为 0，说明库存已被其他消费者线程更新（`version` 不匹配），则本次操作失败，事务将回滚。
    - **c. 创建订单记录**：如果乐观锁扣减库存成功（影响行数为 1），则在 `orders` 表中插入一条新的订单记录，初始状态为“待支付”。

4.  **提交事务与释放锁**：如果以上所有步骤都成功，则提交整个数据库事务。无论成功还是失败，最终都要在 `finally` 块中**释放 Redisson 分布式锁**。

5.  **失败处理**：如果在处理过程中发生任何异常（如乐观锁更新失败），整个事务会回滚。RocketMQ 的机制会认为这次消费失败，并在稍后进行重试。如果多次重试仍然失败，该消息最终会被投递到“**死信队列**”（DLQ），等待人工介入排查。

### 总结：异步化解耦带来的价值

通过这套结合了 RocketMQ 和双重保险锁的异步化方案，我实现了：

1.  **极致的用户体验**：前端响应时间从等待数据库的秒级降低到了毫秒级。
2.  **强大的系统弹性**：通过“削峰填谷”有效缓冲了瞬时流量，保护了后端数据库，防止了系统雪崩。
3.  **严谨的数据一致性**：通过 **Redisson 锁保证业务幂等性** 和 **数据库乐观锁保证数据最终一致性** 的双重保险，彻底杜绝了超卖和重复下单问题。
4.  **清晰的服务解耦与高可伸缩性**：下单、库存、商品等服务职责分离，当处理能力不足时，只需水平增加消费者实例即可，扩展性极强。

---

## 详细说说你的成果”秒杀接口吞吐量提升近 50 倍，有效过滤超 99%的无效请求，最终达成零超卖、零重复单的业务目标。”是怎么得到的？

### 1. “秒杀接口吞吐量提升近 50 倍”

这个指标是衡量系统宏观处理能力提升的核心数据，它来自于优化前后的性能压测对比。

- **测试基准（优化前）**：

  - **架构**：最原始的同步阻塞模型。即用户的秒杀请求直接触发应用服务器，服务器在一个事务内同步执行“查询库存 -> 扣减库存 -> 创建订单”等一系列数据库操作，然后才返回响应。
  - **压测方式**：使用 JMeter 模拟大量并发用户请求秒杀接口。
  - **结果**：系统的瓶颈很快出现在数据库的行锁竞争和连接数上。当并发数稍高时，大量请求因等待数据库事务而超时。系统能稳定处理的 QPS 大约在 **80-100** 之间。超过这个值，错误率就会急剧上升。我们取一个中间值 **90 QPS** 作为基准。

- **优化后测试**：
  - **架构**：部署了我完整设计的异步化、多级缓存架构。用户的请求在 Controller 层经过 Redis+Lua 预检后，就立刻发送事务消息到 RocketMQ 并返回。
  - **压测方式**：使用相同的 JMeter 脚本。
  - **结果**：此时，秒杀接口的性能瓶颈从后端数据库转移到了前端的 Web 服务器和 Redis。
    1.  Web 服务器只处理极轻量的内存操作和一次网络消息发送，耗时极短。
    2.  Redis 的单机 QPS 可以轻松达到数万甚至十万。
        在这个架构下，秒杀接口的 QPS 能够稳定地达到 **4,500 甚至更高**。
- **计算**：
  性能提升倍数 = `优化后的 QPS / 优化前的 QPS`
  = `4500 / 90`
  = **50 倍**。

  这个 “50 倍” 是同步阻塞模型和异步化架构在处理前端请求能力上的巨大差异的直接体现。

### 2. “有效过滤超 99% 的无效请求”

这个指标是衡量前端流量“防火墙”——即 **Redis+Lua 脚本**——工作效率的数据。

- **模拟场景**：我们模拟一个典型的秒杀场景：**100 件商品**，但在秒杀开始的瞬间，有 **100,000 个并发请求**涌入。
- **观测点**：
  1.  **Redis 层面**：通过 Redis 的 `MONITOR` 命令或慢查询日志（虽然 Lua 很快，但可以作为观测手段），我们可以统计出 Lua 脚本被执行的总次数。在这个场景下，是 **100,000 次**。
  2.  **RocketMQ 层面**：我们去监控 RocketMQ 的管理后台，查看最终成功投递到 Topic 中的消息数量。
- **结果**：
  - 由于 Lua 脚本的原子性扣减，只有前 **100 个**请求能够成功扣减 Redis 中的库存，并获得“资格”（Lua 脚本返回 1）。
  - 因此，最终只有 **100 条**消息被成功发送到了 RocketMQ，并进入了后续的异步处理流程。
  - 剩下的 `100,000 - 100 = 99,900` 个请求，全都被 Lua 脚本判断为“库存不足”而直接拒绝，根本没有机会进入后端系统。
- **计算**：
  无效请求过滤率 = `被过滤的请求数 / 总请求数`
  = `99,900 / 100,000`
  = **99.9%**。

  这个数据清晰地表明，我设计的这道前端防线，成功地将后端系统的处理压力降低了几个数量级。

### 3. “最终达成零超卖、零重复单的业务目标”

这个是业务层面的最终成果，它不仅仅依赖于性能，更依赖于架构设计的严谨性。它是通过**功能验证**和**异常场景模拟**来证明的。

- **零超卖的保障**：

  1.  **第一层（Redis）**：Lua 脚本的原子性操作，保证了在 Redis 层面绝不会多发一个“资格凭证”。
  2.  **第二层（数据库）**：在消费者端，我使用了**数据库乐观锁** (`UPDATE ... WHERE version = ?`)。即使在极端情况下（例如，因为某些原因多发了一条 MQ 消息），乐观锁的版本号机制也能保证数据库的库存只会被正确地扣减一次。这是数据最终一致性的兜底保障。

  - **验证**：我们进行了大量压测，结束后检查数据库中的商品库存，其最终值永远是 `初始库存 - 成功订单数`，从未出现负数。

- **零重复单的保障**：
  1.  **第一层（Redis）**：在 Lua 脚本中，除了扣减库存，我还会用 `SADD` 命令将成功抢购的 `userId` 添加到一个 Set 集合中。后续该用户的请求会先被 `SISMEMBER` 命令检查，直接拦截，防止在资格预检阶段就产生重复。
  2.  **第二层（分布式锁）**：在消费者端，创建订单的核心逻辑被 **Redisson 分布式锁**包裹。这个锁的 Key 与 `userId` 和 `productId` 绑定。即使 MQ 消息重复消费，由于锁的存在，也只有一个消息能成功执行创建订单的逻辑。
  3.  **第三层（数据库唯一索引）**：作为最后的保险，我们在订单表上为 `user_id` 和 `product_id` 创建了**唯一联合索引**。这样，即使前面所有防线都被（理论上不可能地）突破，数据库层面也会因为唯一键冲突而拒绝插入重复的订单，从而抛出异常，保证了数据的最终正确性。
  - **验证**：我们通过压测，并人为模拟 MQ 消息的重复投递，结束后查询订单表，`GROUP BY user_id, product_id HAVING COUNT(*) > 1` 的查询结果永远为空。

---

## 详细说说你是怎么实现可横向扩展的分布式用户认证体系的？

这套可横向扩展的分布式用户认证体系，是我在“潮玩新零售”项目中为了解决从单体架构演进到分布式微服务架构时，遇到的核心会话管理难题而设计的。

传统的基于 Session 的认证方式，Session 数据是存储在单个服务器的内存中的。在分布式环境下，用户的请求会被负载均衡器分发到不同的服务器实例上，后一台服务器无法获取前一台服务器内存中的 Session 信息，导致用户需要反复登录。

为了解决这个问题，我设计并落地了一套以 **“Token + Redis”为核心**的无状态认证方案，并结合**双层拦截器**和 **ThreadLocal** 实现了高效、安全的认证与授权。

### 1. 核心架构：Token + Redis 替代传统 Session

这套体系的核心思想是**将服务端的会话状态从应用服务器自身剥离，集中存储到外部的共享存储中**，在这里我选择了高性能的内存数据库 Redis。

**认证流程如下：**

1.  **用户登录**：

    - 用户提交用户名和密码到登录接口。
    - 服务端验证凭据是否正确（查询用户数据库）。
    - 验证通过后，服务端**不创建 Session**。而是：
      - 生成一个全局唯一的、无规律的字符串作为 **Token**（例如，使用 UUID）。
      - 将用户的核心信息（如用户 ID、角色、权限列表等）序列化成一个对象（比如 JSON 字符串）。
      - 以生成的 Token 为 Key，用户信息对象为 Value，存入 **Redis**，并设置一个**过期时间**（例如 2 小时），这个过期时间就相当于传统 Session 的超时时间。
    - 服务端将这个 Token 返回给客户端。客户端通常会将其存储在 Local Storage 或 Cookie 中。

2.  **后续请求认证**：
    - 客户端在后续的每一次请求中，都需要在请求头（通常是 `Authorization` Header）中携带这个 Token。
    - 服务端接收到请求后，会通过一个统一的认证入口（我设计中是第一个拦截器）获取到这个 Token。
    - 服务端拿着 Token 去 Redis 中查询。
      - 如果能查询到对应的用户信息，说明用户已登录且会话有效。认证通过。
      - 如果查询不到（Key 不存在或已过期），则说明用户未登录或登录已失效。认证失败，拒绝请求。

**为什么这个体系可以横向扩展？**

关键在于**应用服务器本身是无状态的（Stateless）**。任何一台服务器实例都不在本地存储任何会话信息。用户的会话状态被集中管理在 Redis 中。因此，无论负载均衡器将用户的请求分发到集群中的哪一台服务器，该服务器都能通过相同的 Token 从同一个 Redis 实例中获取到用户的身份信息。这使得我们可以随时增加或减少应用服务器实例，而完全不用担心会话一致性的问题，实现了完美的水平扩展。

### 2. 具体实现：双层拦截器 + ThreadLocal

为了让这套体系在代码层面优雅、高效且安全，我设计了两个关键组件：

#### A. 双层拦截器（认证 + 授权）

我使用 Spring MVC 的 `HandlerInterceptor` 设计了两层拦截器，它们各司其职，形成了责任链。

1.  **认证拦截器（Authentication Interceptor）**：

    - **职责**：这是第一道关卡，只负责 **“你是谁”** 的问题。
    - **流程**：它会拦截所有需要登录才能访问的请求，从请求头中解析出 Token。然后访问 Redis 进行验证。
    - **结果**：
      - 如果验证失败，直接中断请求，返回 401 Unauthorized 状态码。
      - 如果验证成功，它会从 Redis 中反序列化出用户信息对象。**然后，它会做一件非常关键的事：将这个用户信息对象存入 `ThreadLocal` 中**，然后放行请求到下一个拦截器。

2.  **授权拦截器（Authorization Interceptor）**：
    - **职责**：这是第二道关卡，负责 **“你能干什么”** 的问题。
    - **流程**：它运行在认证拦截器之后。它会检查当前请求的接口需要什么权限或角色（例如，通过自定义注解 `@RequiresRole("ADMIN")` 标记在 Controller 方法上）。然后，它会从 `ThreadLocal` 中获取到上一步存入的用户信息，并检查该用户是否具备所需的权限。
    - **结果**：
      - 如果权限不足，中断请求，返回 403 Forbidden 状态码。
      - 如果权限足够，则放行请求到最终的 Controller。

#### B. ThreadLocal：高效、安全地传递用户信息

- **为什么要用 ThreadLocal？**
  如果不使用 ThreadLocal，那么在认证拦截器获取到用户信息后，就必须以方法参数的形式，层层传递给 Controller、Service，甚至 DAO 层。这会造成代码的严重耦合和冗余，非常不优雅。

- **它是如何工作的？**
  Web 服务器（如 Tomcat）通常会为每个请求分配一个独立的线程来处理。`ThreadLocal` 可以在这个线程的生命周期内，创建一个只有该线程能访问的变量副本。
  1.  **存储**：我在认证拦截器中，将用户信息存入一个静态的 `ThreadLocal` 变量。
  2.  **获取**：在后续的业务逻辑中（如授权拦截器、Controller、Service），任何地方都可以通过这个静态的 `ThreadLocal` 变量随时获取到当前登录用户的信息，而无需传递参数。
  3.  **释放**：**这是至关重要的一步**。为了防止内存泄漏（因为 Web 服务器的线程是会被复用的），我会在请求处理完毕后（通常在拦截器的 `afterCompletion` 方法中），调用 `ThreadLocal.remove()` 方法，清空当前线程的副本。

通过这套 **“Token+Redis”** 的无状态架构，结合 **双层拦截器** 的清晰职责划分，以及 **ThreadLocal** 的高效上下文传递，我成功构建了一个既能满足分布式环境下会话共享需求，又具备良好扩展性和代码优雅性的用户认证体系。
