---
title: "01-灵医项目面试题"
date: 2025-02-14 14:34:14 +0800
categories: [面试题, 灵医项目]
tags: [面试八股, 灵医项目面试题]
pin: false
toc: true
math: true
---

## 详细说说你是怎么基于 SpringBatch 构建了自动化的知识处理管道，通过精细化分块（Chunking）等策略优化了知识入库质量的？

简单来说，我们构建这个**自动化的知识处理管道**，核心目标是为了将海量的、格式各异的医疗文档（比如临床指南、医学文献、药品说明书等）高效、高质量地转化成适合大语言模型（LLM）检索和理解的结构化知识片段，并存入向量数据库。我选择使用 **Spring Batch**，主要是看中了它对于处理大规模、可批量的离线任务的成熟支持，包括事务管理、并发处理、失败重试和可监控性等特性，非常适合我们这个场景。

我的具体实现和优化策略可以分为以下几个步骤：

#### 第一步：定义标准化的批处理作业（Job）

我使用 Spring Batch 的 `Job` 和 `Step` 概念来定义整个知识处理流程。一个典型的 `Job` 可能包含以下几个 `Step`：

1.  **数据读取 (ItemReader):** 我会根据不同的数据源实现不同的 `ItemReader`。例如，对于数据库中的结构化数据，我使用 `JdbcCursorItemReader`；对于存储在文件系统中的 PDF 或 Word 文档，我会自定义一个 `ItemReader`，它会先调用像 Apache Tika 这样的库来解析文件内容，然后将原始文本流式地读入管道。

2.  **数据处理 (ItemProcessor):** 这是整个管道的核心，也是精细化分块策略实现的地方。`ItemProcessor` 接收 `ItemReader` 传入的原始数据（比如一整篇文档的内容），然后执行一系列处理，包括：

    - **清洗：** 去除无关信息，如页眉页脚、广告、格式混乱的特殊字符等。
    - **分块 (Chunking)：** 这是关键步骤，我会在这里应用我设计的精细化分块策略，具体细节我将在第二步展开。
    - **元数据附加：** 为每个切分好的知识块（Chunk）附加丰富的元数据，比如来源文件名、章节标题、页码等。这些元数据在后续的检索和答案生成中非常有用，可以用于过滤或作为引用来源展示给用户，增加答案的可信度。
    - **结果封装：** 将处理好的知识块和元数据封装成一个对象，传递给下一步。

3.  **数据写入 (ItemWriter):** `ItemWriter` 负责将 `ItemProcessor` 处理好的知识块进行最终的入库操作。它会批量接收一批知识块，然后调用 `EmbeddingClient` 将这些文本块转换为向量，最后将文本、向量以及元数据批量写入到我们的向量数据库 Weaviate 中。使用批量写入可以极大地提升入库效率。

#### 第二步：在 ItemProcessor 中实现“精细化分块 (Chunking)”策略

“Chunking”的好坏，直接影响到检索的召回率和精准度。如果切分得太粗，一个 Chunk 包含太多无关信息，会引入噪声；如果切分得太细，一个 Chunk 丢失了上下文，模型就无法准确理解。因此，我没有采用简单的固定长度切分，而是设计了一套**多层次的、有策略的精细化分块方案**：

1.  **结构化预处理**：在切分之前，我会先对文本进行预处理。例如，利用正则表达式识别并保留文档中的标题、列表、表格等结构化信息。这些信息本身就是很好的语义边界，可以作为切分的天然断点。

2.  **递归式按语义边界切分 (Recursive Character Text Splitting)**：这是我的核心切分策略。我设定了一个优先的切分分隔符列表，比如 `["\n\n", "\n", "。", "，", ""]`。程序会尝试用最高优先级的`\n\n`（段落）来切分文本。如果切分后的块仍然大于我们设定的 Chunk Size 阈值，那么程序会**递归地**用下一级更细粒度的分隔符（如 `\n` 或句号）对这个“过大的块”进行再切分，直到所有块的大小都处于一个合理的范围（例如 200-500 个 token）。这种方式能最大程度地保证每个 Chunk 都是一个语义完整的单元，比如一个完整的段落或句子。

3.  **处理特殊格式——表格和列表：** 医疗文档中包含大量的表格（如药品剂量表）和列表（如操作步骤）。粗暴的文本分割会完全破坏它们的结构。我的策略是，通过特定的规则识别出这些表格和列表，并**将它们作为一个整体，保留在同一个知识块中**。我甚至会尝试将它们转换为 Markdown 格式的文本，这样既保留了结构，又便于 LLM 理解。

4.  **重叠块（Chunk Overlap）**：为了解决语义在块与块之间被切断的问题，我引入了重叠机制。比如，每个 Chunk 会包含前一个 Chunk 末尾的 50 个 token。这样一来，即使一个关键的知识点恰好在切分边界上，通过重叠也能保证它在两个相邻的 Chunk 中都被完整地保留，从而在向量检索时更容易被召回。

#### 第三步：利用 Spring Batch 的特性优化管道质量与性能

单纯实现功能是不够的，还需要保证管道的稳定和高效。

- **事务与容错**：Spring Batch 提供了开箱即用的块处理（Chunk-oriented Processing）模型。我设置了一个合理的 `commit-interval`（比如每处理 10 篇文档提交一次事务）。这意味着 Reader、Processor 和 Writer 是在一个事务块内完成的。如果在处理第 8 篇文档时发生错误，整个事务会回滚，作业可以从第 1 篇文档重新开始，或者通过配置从失败的那个 Chunk 开始重试，这极大地保证了数据处理的一致性和管道的健壮性。
- **并发处理**：对于海量文档，串行处理效率太低。我利用 Spring Batch 的 `Multi-threaded Step` 功能，配置了一个线程池。这样就可以并行地处理多个文档，每个线程负责一个完整的 `Read-Process-Write` 流程，处理速度得到了数倍提升。

**最终成果就是**，通过这个基于 Spring Batch 构建的自动化管道，我们成功地将过去需要人工介入、耗时数天的知识入库流程，缩短到了几小时内自动完成。更重要的是，通过“精细化分块”等策略，我们产出的知识区块质量非常高，上下文完整、噪声低。这直接体现在我们下游 RAG 系统的评测指标上，正如我简历中提到的，在包含 500 个复杂医学问题的内部测试集上，相比单一的向量检索，我们的 F1 分数实现了 **40%** 的显著增长。这充分证明了高质量的知识入库是整个 AI 应用成功的基石。

---

## 详细说说多阶段混合检索的流程？

我们之所以放弃单一的检索方式，而设计这套复杂的流程，根本原因在于**单一检索方法有其固有的局限性**。

- **单一向量检索**：它的强项在于理解**语义相似性**。比如用户问“治疗高血压的药有哪些？”，它能很好地召回包含“降压药”、“抗高血压药物”等语义相近但字面完全不同的内容。但它的弱点在于对**关键词的精确匹配**能力不足，特别是对于一些特定的医学实体，如药品名“阿司匹林肠溶片”或某个基因编号“BRCA1”，向量检索有时会召回一些语义相关但并不精确的结果。

- **单一稀疏检索（如 BM25）**：它则恰恰相反。它基于词频和逆文档频率，是**关键词匹配的专家**。只要用户的查询包含了关键的术语，BM25 就能非常精准地找到包含这些术语的文档。但它的缺点是无法理解语义，如果用户换一种说法，比如将“心脏病发作”说成“心梗”，BM25 就可能完全匹配不到。

我们的目标是**取其长，补其短**，最大化召回结果的**全面性（Recall）**和**精准性（Precision）**。因此，我设计了如下三个阶段的流程：

### **第一阶段：并行召回 (Parallel Recall) - 拓宽搜索的广度**

在这一阶段，当用户的查询请求进来后，我们会同时启动两个并行的检索任务：

1.  **向量检索**：我们将用户的查询语句通过 Embedding 模型（例如 OpenAI 的 text-embedding-ada-002）转换成一个查询向量。然后，我们使用这个向量去我们的向量数据库 Weaviate 中进行近似最近邻（ANN）搜索。这一路的目标是 **“抓大放小”**，负责召回所有与用户查询**意图和语义**上相关的知识块。

2.  **BM25 稀疏检索**：同时，我们利用 Weaviate 内置的 BM25 功能，对用户的原始查询文本进行关键词检索。这一路的目标是**“精准打击”**，负责召回那些包含了用户查询中**核心关键词或专有名词**的知识块。

执行完这一步后，我们会从两条路分别取回 Top-K（比如 K=20）个结果。然后，我们将这两组结果进行合并和去重，形成一个更大、更多样化的候选知识集合（比如总共有 30-40 个候选块）。

**这个阶段的核心价值在于**：它确保了我们既不会因为用户使用了不同的措辞而漏掉关键信息（向量检索的功劳），也不会因为语义模型的模糊性而错失包含精确术语的黄金文档（BM25 的功劳）。

### **第二阶段：二次精排 (Re-ranking) - 提升结果的精度**

第一阶段虽然保证了召回的全面性，但也引入了一个新问题：候选集虽然大了，但也变得更“嘈杂”了，里面包含了许多相关性不高的结果。而且，向量检索的相似度分数和 BM25 的相关性分数是两种完全不同的度量，无法直接比较和排序。

如果直接把这几十个良莠不齐的知识块都丢给 LLM，不仅会因为超出上下文窗口而报错，更会因为无关信息的干扰而严重影响最终生成答案的质量，甚至诱发幻觉。

因此，我们需要一个更“聪明”的裁判来对这些候选结果进行二次筛选和排序，这就是 **二次精排（Re-ranking）** 阶段，我们引入了 **Cohere Rerank API**。

它的工作原理和优势在于：

- **更先进的模型架构**：与召回阶段使用的“双编码器”（Bi-Encoder）模型不同，Rerank 模型通常是“**跨编码器**”（Cross-Encoder）架构。双编码器是独立地为查询和文档生成向量，然后计算其距离，速度快但精度有限。而跨编码器会将**用户的查询和每一个候选文档成对地 (pair) 输入模型**中，让模型能够进行深度交互和注意力计算，从而对“这个文档是否真的回答了这个查询”做出一个**极为精准的相关性判断**。

- **具体实现**：在我的代码中，我会将第一阶段合并后的候选文档列表，连同原始的用户查询，一起打包发送给 Cohere Rerank API。API 会返回一个经过重新排序的文档列表，每个文档都附有一个新的、从 0 到 1 的、高度可信的相关性分数。

### **第三阶段：上下文构建与答案生成**

在拿到 Cohere Rerank 返回的精排结果后，事情就变得简单了。我们只需要从这个排序列表的顶部选取 Top-N（比如 N=3 或 N=5）个最相关的知识块。

然后，我们将这几个最高质量的知识块作为**可靠的上下文（Context）**，与用户的原始问题一起，构建成一个最终的 Prompt，发送给大语言模型（如 GPT-4），并指令它“请根据以下上下文信息，回答用户的问题”。

**总结一下**，这套“召回-精排”的多阶段流程，就像一个高效的漏斗：

- **入口（并行召回）**：用最宽的口径，确保不漏掉任何一个潜在相关的知识点。
- **中段（二次精排）**：用一个非常智能和精细的滤网，筛掉所有相关性不高的杂质。
- **出口（上下文构建）**：只将最精华、最浓缩的知识点输送给 LLM。

通过这套流程，我们成功地将最相关、最精确的知识喂给了 LLM，极大地提升了最终答案的相关性和准确性，有效抑制了模型幻觉，这也是我们在测试集上 F1 分数能实现显著提升的根本原因。

---

## 详细说说你是怎么利用 SpringAIVectorStore 抽象，将应用与底层向量数据库 Weaviate 解耦的？

传统的开发方式，或者说最直接的方式，是在我的业务代码（比如 `RAGService`）里，直接引入 Weaviate 官方提供的 Java 客户端（SDK）。然后，我的代码就会长得像这样：

```java
// 这是一个“硬编码”或“紧耦合”的例子
WeaviateClient client = WeaviateClient.builder().build(); // 创建 Weaviate 专用客户端
GraphQLQuery query = client.graphQL().get()
    .withClassName("MedicalDocument")
    .withNearText(...) // 使用 Weaviate 特有的 nearText 查询
    .build();
```

这种写法在项目初期可能开发很快，但它会带来一个非常严重的架构问题，那就是 **“紧耦合”**（Tight Coupling）。我的应用程序与底层的 Weaviate 数据库“焊死”在了一起。这会导致几个致命的缺陷：

1.  **供应商锁定 (Vendor Lock-in):** 如果未来出现了一款性能更好、成本更低或者更适合我们特定医疗数据场景的向量数据库（比如 Pinecone, Milvus, ChromaDB 等），我们想要迁移的成本会是巨大的。我们需要深入到所有业务代码中，把所有调用 Weaviate SDK 的地方，全部重写一遍，这无异于一次小型重构，费时费力且风险极高。
2.  **可测试性差:** 在进行单元测试时，我需要去 Mock 那个复杂的 `WeaviateClient` 对象，这通常很麻烦。我真正关心的是业务逻辑，而不是数据库客户端的具体行为。
3.  **代码可读性和维护性下降:** 业务代码中混杂了大量与底层数据库实现细节相关的代码，使得核心业务逻辑不够突出，增加了新成员理解和维护的难度。

### **我的解决方案：拥抱抽象，面向接口编程**

为了解决以上问题，我引入了 **Spring AI 的 `VectorStore` 抽象**。

Spring AI 框架的设计哲学与整个 Spring 生态一脉相承，那就是“**面向接口编程**”和“**依赖倒置**”。`VectorStore` 就是这样一个标准化的接口（可以理解为一个“**约定**”或“**合同**”），它定义了与向量数据库交互的所有标准操作，比如：

- `void add(List<Document> documents)`: 添加文档（及其向量）。
- `Optional<Boolean> delete(Collection<String> id)`: 根据 ID 删除文档。
- `List<Document> similaritySearch(String query, int k)`: 执行相似性搜索。

我的具体做法如下：

1.  **在业务层，依赖接口而非实现**

    在我的 `RAGService` 中，我不再直接依赖 `WeaviateClient`，而是依赖 Spring AI 提供的 `VectorStore` 接口。通过 Spring 的依赖注入（DI），我只需要声明我需要一个 `VectorStore` 的实例即可：

    ```java
    // 这是解耦后的代码
    @Service
    public class RAGService {
        private final VectorStore vectorStore;

        @Autowired
        public RAGService(VectorStore vectorStore) { // 注入的是标准接口
            this.vectorStore = vectorStore;
        }

        public List<String> retrieveRelevantContext(String userQuery) {
            // 我只调用标准接口定义的方法，完全不知道底层是 Weaviate
            List<Document> results = this.vectorStore.similaritySearch(userQuery, 5);
            // ... 后续处理
            return results.stream().map(Document::getContent).collect(Collectors.toList());
        }
    }
    ```

    您可以看到，我的业务代码 `RAGService` 现在变得非常“纯净”，它只知道有一个叫 `VectorStore` 的东西能帮我做向量搜索，但它**完全不知道**这个 `VectorStore` 的背后到底是 Weaviate，还是 Pinecone，或是其他任何数据库。

2.  **在配置层，决定具体实现**

    那么，程序在运行时是如何知道要用 Weaviate 的呢？答案就在于**配置**。

    - **Maven/Gradle 依赖:** 我首先在项目的 `pom.xml` 文件中引入 Weaviate 的 Spring AI Starter 依赖：
      ```xml
      <dependency>
          <groupId>org.springframework.ai</groupId>
          <artifactId>spring-ai-weaviate-store-starter</artifactId>
      </dependency>
      ```
    - **配置文件 (`application.yml`):** 然后，我只需要在 Spring Boot 的配置文件中，提供 Weaviate 的连接信息即可：
      ```yml
      spring:
        ai:
          vectorstore:
            weaviate:
              scheme: http
              host: weaviate-host
              port: 8080
              consistency-level: ONE
      ```
      Spring Boot 的自动配置机制会检测到 Classpath 上有 Weaviate 的 Starter，并且在配置文件中找到了相关配置，于是它就会自动地为我们创建一个 `WeaviateVectorStore` 的实例 Bean，并注入到像 `RAGService` 这样需要 `VectorStore` 接口的地方。

### **带来的架构扩展性优势**

通过这种方式，我们获得的架构扩展性是巨大的。我们可以用一个生动的例子来说明：

**假设一年后，团队经过调研，决定将向量数据库从 Weaviate 切换到 Pinecone。**

- **没有解耦的“噩梦”模式：** 我们需要修改所有调用了 `WeaviateClient` 的 Java 文件，学习 Pinecone 的 SDK，重写所有数据库交互逻辑，然后进行大量回归测试。
- **使用 `VectorStore` 抽象的“优雅”模式：**
  1.  在 `pom.xml` 中，将 `spring-ai-weaviate-store-starter` 替换为 `spring-ai-pinecone-store-starter`。
  2.  在 `application.yml` 中，删除 Weaviate 的配置，换上 Pinecone 的 API Key 和环境配置。
  3.  **就这样，完成了！** 不需要修改任何一行 Java 业务代码。我们的 `RAGService` 毫不知情地就从与 Weaviate 对话切换到了与 Pinecone 对话。

**总结来说，利用 Spring AI 的 `VectorStore` 抽象，我将“与哪个向量数据库对话”这个易变的关注点，从业务代码中剥离，下沉到了配置层面。这使得我们的核心业务逻辑更加稳定、清晰，并赋予了整个系统在未来能够低成本、低风险地适配不同底层技术的强大能力，这正是架构扩展性的核心体现。**

---

## 详细说说“F1 分数实现了 40%的增长”是怎么得到的？

整个评测过程可以分为四个关键步骤：**构建测试集、定义评测标准、执行评测和计算结果**。

### **第一步：构建高质量的内部测试集**

首先，我们需要一个能真正检验系统能力的“考卷”。为此，我们联合了项目组内的医学顾问，共同构建了一个包含 500 个问题的内部测试集。这个测试集有以下几个特点：

1.  **复杂性与多样性**：这些问题并非简单的““xx 药的副作用是什么””，而是模拟了医生在临床和科研中遇到的真实复杂场景。例如：

    - **比较类问题**：“对于二型糖尿病合并早期肾病的患者，SGLT2 抑制剂和 GLP-1 受体激动剂在心血管获益方面有何异同？”
    - **条件推理类问题**：“如果一个患者对阿司匹林过敏，在需要进行抗血小板治疗时，有哪些替代药物选择，各自的禁忌症是什么？”
    - **多跳推理类问题**：“某个特定基因突变（如 KRAS G12C）如何影响非小细胞肺癌的靶向治疗选择，以及目前有哪些已上市或进入三期临床的药物？”

2.  **建立“黄金标准答案” (Ground Truth)**：对于这 500 个问题中的每一个，我们的医学顾问都会**人工地**从我们的知识库中，**标注出所有能够完整回答该问题所必需的知识块（Chunks）**。这些被标注出来的知识块集合，就构成了这个问题的“黄金标准答案”或者叫“Ground Truth Context”。这是我们后续进行自动化评测的基石。

### **第二步：定义评测指标——为什么是 F1 分数？**

在 RAG 的检索（Retrieval）环节，我们最关心的是系统**找回来的上下文质量**如何。我们使用两个经典的指标来衡量：

- **精确率 (Precision)**：衡量的是“**找得准不准**”。公式是 `(找回来的正确的知识块数量) / (所有找回来的知识块总数)`。高精确率意味着系统返回的结果中，无关的“噪音”信息很少。

- **召回率 (Recall)**：衡量的是“**找得全不全**”。公式是 `(找回来的正确的知识块数量) / (标准答案里所有应该被找回的知识块总数)`。高召回率意味着系统很少漏掉回答问题所必需的关键信息。

然而，精确率和召忿率往往是相互制约的。一个系统可以为了追求高召回率而返回海量的结果，但精确率就会惨不忍睹。反之亦然。我们需要一个能够**平衡这两者**的综合指标，这就是 **F1 分数 (F1-Score)**，它是精确率和召回率的调和平均数，能更全面地反映检索系统的整体性能。

### **第三步：执行评测——A/B 对比测试**

我们定义了两个要进行对比测试的系统：

- **系统 A (基线系统)**：**单一向量检索**。即用户的查询只通过向量相似度搜索，直接返回 Top-K 个最相似的知识块。
- **系统 B (我的优化系统)**：**多阶段混合检索**。即我所实现的“BM25 + 向量”并行召回，再用 Cohere Rerank 进行二次精排的流程，最终返回 Top-K 个结果。

我们将 500 个问题分别输入到这两个系统中，记录下它们各自为每个问题返回的知识块列表。然后，我们将这个列表与我们预先标注好的“黄金标准答案”进行比对，为每个问题计算出系统 A 和系统 B 的精确率和召回率，进而得出 F1 分数。

### **第四步：计算结果——40% 增长的由来**

经过对 500 个问题的评测数据进行汇总和平均，我们得到了两个系统的最终性能数值。为了方便您理解，我这里用一组**具体且有代表性**的数字来说明：

1.  **系统 A (基线系统) 的表现**：

    - 平均精确率 (Precision): 约为 0.60 (60%)
    - 平均召回率 (Recall): 约为 0.51 (51%)
    - **计算出的平均 F1 分数: 约为 0.55**

    _解读_：这说明单一向量检索虽然能找到一些语义相关的块，但经常会引入一些不那么精确的结果（精确率不高），并且容易漏掉包含精确关键词的关键信息（召回率不高）。

2.  **系统 B (我的优化系统) 的表现**：

    - 平均精确率 (Precision): 约为 0.81 (81%)
    - 平均召回率 (Recall): 约为 0.74 (74%)
    - **计算出的平均 F1 分数: 约为 0.77**

    _解读_：混合召回保证了召回率，而 Reranker 的精排能力则大幅提升了精确率，整体性能显著优于基线。

3.  **计算增长率**：
    - 增长率 = `(系统B的F1分数 - 系统A的F1分数) / 系统A的F1分数`
    - 代入数值 = `(0.77 - 0.55) / 0.55`
    - 计算结果 = `0.22 / 0.55 = 0.4`
    - 换算成百分比，就是 **40%**。

**所以，“F1 分数实现了 40% 的增长”这个结论，是通过这样一套严谨的、基于“黄金标准答案”的离线评测流程，通过对比优化前后的系统在精确率和召回率这两个核心指标上的综合表现（即 F1 分数）而得出的量化成果。** 这也为我们后续持续优化算法提供了一个可靠的基准。

---

## 详细说说可插拔、自动化的 Agent 多工具协同框架是怎么实现的？

这个框架的核心思想是：**让 LLM 自己来决定应该使用哪个工具来最高效、最准确地完成用户的指令**。

在我详细解释实现之前，我们先明确一下“为什么需要这个框架”。在一个简单的 RAG 系统中，所有问题都默认走向“知识库检索”这一条路。但现实世界是复杂的，用户的意图是多样的：

- 当用户问：“奥司他韦的儿童用药剂量是多少？”，这显然应该去调用一个精准的**药品查询工具**，而不是在海量文献里做模糊的语义搜索。
- 当用户问：“查询一下张三最近一次的体检报告摘要”，这需要调用一个**档案检索工具**去访问 HIS 或 EMR 系统。
- 当用户问：“介绍一下近期关于帕金森病治疗的研究进展”，这才是最适合**RAG 引擎**去处理的开放性问题。

我的框架就是为了实现这种**智能的任务分发和工具调用**，并且要让接入一个新工具的过程变得极其简单，也就是**“可插拔”**。

### **实现原理：基于 Spring AI 的函数调用 (Function Calling)**

我这个框架的技术基石是 Spring AI 对大模型“函数调用”能力的出色封装。整个流程可以拆解为以下几个核心部分：

#### **1. 将业务功能“工具化” (Tooling)**

“可插拔”的第一步，就是要把我们系统中的各种能力（比如查数据库、调用外部 API）封装成一个个标准化的、能被 AI 理解的“工具”（Tools）。在 Spring AI 中，这实现起来异常优雅。

我只需要将一个普通的 Java 方法，通过 `@Bean` 注解注册到 Spring 容器中，并用 `@Description` 注解为它和它的参数提供清晰的自然语言描述。这个“描述”就是我们与 LLM 沟通的“合同”，告诉了 LLM “这个工具是干什么的”以及“使用它需要提供哪些参数”。

**代码示例 (`ToolConfiguration.java`):**

```java
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Description;
import java.util.function.Function;

@Configuration
public class ToolConfiguration {

    // 工具一：药品查询工具
    @Bean
    @Description("根据药品名称，查询该药品的详细信息，如适应症、用法用量等")
    public Function<DrugQueryTool.Request, DrugQueryTool.Response> drugQueryTool() {
        return new DrugQueryTool();
    }

    // 工具二：档案检索工具
    @Bean
    @Description("根据患者姓名或ID，查询其电子病历或档案信息")
    public Function<MedicalRecordTool.Request, MedicalRecordTool.Response> medicalRecordTool() {
        return new MedicalRecordTool();
    }

    // RAG 引擎也可以被封装成一个工具
    @Bean
    @Description("当用户的问题是开放性的、知识性的问题时，调用此工具在医学知识库中进行检索")
    public Function<RagTool.Request, RagTool.Response> ragEngineTool() {
        return new RagTool();
    }
}

// 实际的工具类，这里省略了具体实现
class DrugQueryTool implements Function<DrugQueryTool.Request, DrugQueryTool.Response> {
    public record Request(@Description("药品的通用名称或商品名") String drugName) {}
    public record Response(String info) {}

    @Override
    public Response apply(Request request) {
        // ... 此处是调用药品数据库的业务逻辑 ...
        String drugInfo = "从数据库查询到 " + request.drugName() + " 的信息...";
        return new Response(drugInfo);
    }
}

// MedicalRecordTool 和 RagTool 的实现与上面类似...
```

在这个例子中，`drugQueryTool` 这个 Bean 就成了一个 AI 可识别的工具。LLM 会读懂它的 `@Description`：“根据药品名称，查询该药品的详细信息...”，并且知道它需要一个名为 `drugName` 的字符串参数。

**“可插拔”的特性就体现在这里**：当我需要一个新的工具，比如“手术排班查询”，我只需要按照这个范式，写一个新的方法，打上注解，注册成 Bean，就完成了接入。Agent 框架的核心逻辑完全不需要改动。

#### **2. 构建自动化决策的 Agent (The Agent)**

Agent 的核心是一个智能的编排器或路由。在我的实现中，我将这个决策过程称为**“语义路由 (Semantic Router)”**，而这个 Router 的大脑就是 LLM 本身。

当用户请求进来时，我不再是直接去执行某个业务逻辑，而是将用户的**原始问题**和**当前所有可用工具的描述信息**打包在一起，发送给大模型。

**代码示例 (`AgentService.java`):**

```java
import org.springframework.ai.chat.ChatClient;
import org.springframework.ai.chat.prompt.Prompt;
import org.springframework.ai.openai.OpenAiChatOptions;
import org.springframework.stereotype.Service;

@Service
public class AgentService {

    private final ChatClient chatClient;

    public AgentService(ChatClient chatClient) {
        this.chatClient = chatClient;
    }

    public String processRequest(String userMessage) {
        // 构建调用选项，告诉模型有哪些函数（工具）可供它选择
        OpenAiChatOptions options = OpenAiChatOptions.builder()
                .withFunction("drugQueryTool") // 这里的名称对应上面 @Bean 的方法名
                .withFunction("medicalRecordTool")
                .withFunction("ragEngineTool")
                .build();

        // 创建一个包含用户问题和可用工具选项的 Prompt
        Prompt prompt = new Prompt(userMessage, options);

        // 发起调用，让 LLM 自主决策
        // Spring AI 在底层会自动处理函数调用的循环
        // 1. LLM返回需要调用的函数 -> 2. 框架执行函数 -> 3. 将结果返回给LLM -> 4. LLM生成最终答案
        var chatResponse = chatClient.call(prompt);

        return chatResponse.getResult().getOutput().getContent();
    }
}
```

#### **3. 自动化协同的工作流**

当 `chatClient.call(prompt)` 被执行时，一个神奇的自动化工作流就开始了：

1.  **意图理解与决策**：LLM 接收到用户的请求，比如“帮我查查阿司匹林肠溶片是治什么的？”。同时，它也收到了我们提供的工具清单（`drugQueryTool`、`medicalRecordTool` 等）和它们的描述。LLM 会进行语义分析，发现用户的意图与 `drugQueryTool` 的描述高度匹配。

2.  **生成调用指令**：LLM 不会直接回答问题。它的第一次返回是一个结构化的指令（通常是 JSON 格式），告诉 Spring AI 框架：“请调用名为 `drugQueryTool` 的函数，并传入参数 `{'drugName': '阿司匹林肠溶片'}`”。

3.  **框架执行工具**：Spring AI 框架接收到这个指令，解析它，并真正在 JVM 中调用我们定义的 `drugQueryTool` 这个 Bean 的 `apply` 方法，执行了真实的数据库查询。

4.  **结果反馈与整合**：工具执行完毕后，返回了药品信息字符串。Spring AI 框架会将这个**工具的执行结果**再次发送给 LLM，并附上原始问题，相当于说：“我按照你的指示调用了工具，这是结果，现在请你基于这个结果，给用户一个最终的、自然的回答。”

5.  **生成最终答案**：LLM 在看到查询结果后，才会组织语言，生成最终的人类可读的答案，比如：“阿司匹林肠溶片主要用于...”。

### **成果与价值**

通过这套框架，我们实现了：

- **高度自动化**：LLM 替代了过去需要用大量 `if-else` 或复杂规则引擎才能实现的意图识别和任务分发逻辑。
- **极致的可插拔性**：正如简历中所说，新增一个业务工具的平均集成时间从原先的 **2 个工作日**（需要理解路由规则、修改分发代码、测试回归）**缩短到了 2-3 小时**（只需要按约定写一个带注解的 Bean 即可）。这极大地提升了我们团队的迭代效率和系统的扩展能力。

简单来说，我构建的不是一个写死的程序，而是一个**可生长的、由 AI 驱动的智能任务处理中心**。

---

## 将新业务工具的平均集成时间从原先的 2 个工作日缩短至 2-3 小时是怎么实现的？

假设我们现在要为“灵医”平台新增一个**功能**：**“查询指定科室今天所有医生的排班信息”**。

### **“旧世界”：2 个工作日的流程 (没有 Agent 框架之前)**

在传统的、没有智能 Agent 的架构下，要实现这个功能，一个后端工程师需要做以下一系列繁重且环环相扣的工作：

**第 1 步：意图识别与路由层改造 (约 0.5 - 1 天)**

- 用户的输入是自然语言，比如“心内科今天谁上班？”或者“查一下外科的排班”。系统首先得“听懂”用户的意图是“查询医生排班”。
- 我需要去修改系统最核心的**入口网关**或**意图分发服务**。
- 我可能需要编写**正则表达式**或者**关键词匹配规则** (`if query.contains("排班") || query.contains("谁上班") ...`)来捕获这个意图。
- 这种方式非常脆弱，用户稍微换个说法（比如“今天心内科有哪些大夫？”），规则可能就失效了。为了鲁棒性，我们需要加很多规则，代码会变得臃肿不堪。
- **最关键的是，我在修改一个系统的核心入口，风险非常高，需要非常谨慎，并且需要大量的回归测试。**

**第 2 步：参数提取逻辑开发 (约 0.5 天)**

- 识别出意图后，我还需要从用户的问句中提取出关键参数，也就是 **“科室名称”**（比如“心内科”）。
- 我又要写一套新的正则或者字符串处理逻辑来做这件事。这同样很麻烦，因为用户可能说“心脏内科”、“心血管内科”，我需要维护一个同义词词典，并编写复杂的代码来处理。

**第 3 步：服务编排与调用 (约 0.5 天)**

- 在获取到意图和参数后，我需要在我的业务编排层（Orchestration Layer）里新增一段 `if-else` 或者 `switch-case` 逻辑：
  ```java
  if ("QUERY_SCHEDULE".equals(intent)) {
      String department = extractDepartment(userQuery);
      ScheduleInfo info = scheduleService.queryByDepartment(department);
      // ...格式化结果...
  } else if (...) {
      // ... 其他业务逻辑 ...
  }
  ```
- 每增加一个功能，这个核心的编排逻辑就会长一点，慢慢变成一个难以维护的“巨兽”。

**第 4 步：联调、测试、上线**

- 因为动了这么多核心代码，我需要和前端（如果需要）联调，并且编写详尽的单元测试和集成测试，确保没有影响到任何原有功能。整个流程走下来，两天是非常紧张的。

### **“新世界”：2-3 小时的流程 (基于我的 Agent 框架)**

现在，我们看看在我的可插拔 Agent 框架下，实现同一个功能需要做什么：

**第 1 步：编写核心业务功能的“工具” (约 2 小时)**

- 我**完全不用关心**意图识别、参数提取和路由。我只需要专注于实现这个功能本身。
- 我创建一个新的 Java 类 `DoctorScheduleTool`。
- 在这个类里，我写一个方法，它接受一个`departmentName`字符串，然后去查询数据库，返回排班信息。这是我们工程师最本质、最核心的工作。

**第 22 步：为“工具”添加“说明书” (约 30 分钟)**

- 这是最关键也是最轻松的一步。我只需要用 Spring AI 的注解，给我的工具写上清晰的“说明书”（自然语言描述）。

  ```java
  @Configuration
  public class ToolBox { // 假设我们有个统一的工具箱配置

      @Bean
      @Description("根据科室名称，查询该科室下所有医生今天的排班信息")
      public Function<ScheduleRequest, ScheduleResponse> doctorScheduleTool() {
          // 返回一个实现了排班查询逻辑的实例
          return request -> {
              // ... 查询数据库的核心逻辑 ...
              return new ScheduleResponse(...);
          };
      }
  }

  // 定义请求和响应的数据结构，并为参数添加描述
  public record ScheduleRequest(
      @Description("需要查询的科室名称, 例如 '心内科' 或 '外科'")
      String departmentName
  ) {}

  public record ScheduleResponse(String scheduleInfo) {}
  ```

- 写完这段代码，我就**完成了 99% 的工作**。我不需要去修改任何一行原有系统的核心路由或编排代码。我只是在一个独立的、全新的文件里，增加了一个与世无争的工具。

**第 3 步：测试与“热插拔”**

- 我只需要对 `doctorScheduleTool` 这个独立的工具进行单元测试。
- 然后我启动应用，这个新工具就会被我的 Agent 框架**自动发现并注册**。
- 我现在就可以直接对系统说：“帮我查下心内科今天的排班”，Agent 会**自动完成**之前需要我们写死代码才能做的所有事情：
  1.  **LLM 自动进行意图识别**：它会阅读所有可用工具的 `@Description`，发现 `doctorScheduleTool` 的描述“查询...排班信息”和用户的意图完美匹配。
  2.  **LLM 自动进行参数提取**：它会根据 `ScheduleRequest` 中参数的描述，知道需要一个“科室名称”，并从“心内科今天的排班”中自动抽取出“心内科”作为参数。
  3.  **框架自动执行调用**：框架自动调用我的工具，并将 LLM 提取的参数传入。

### **总结：**

| 对比项         | “旧世界”（2 天）                                 | “新世界”（2-3 小时）                       | 核心差异                                                                       |
| :------------- | :----------------------------------------------- | :----------------------------------------- | :----------------------------------------------------------------------------- |
| **工作重心**   | 编写大量**胶水代码**（意图识别、参数提取、路由） | 只需编写**核心业务逻辑**                   | **将复杂的、非结构化的自然语言处理工作，从工程师的肩上，转移给了大语言模型。** |
| **代码修改**   | 侵入式修改，需改动**核心、公共**的路由代码       | **非侵入式**，只需新增独立的工具类         | **系统的核心稳定性得到了保障，扩展变得像安装插件一样简单、低风险。**           |
| **系统扩展性** | 差，每加一个功能，核心代码就复杂一分             | 极高，系统能力可以无限叠加，而核心框架不变 | **实现了真正的“开闭原则”，对扩展开放，对修改关闭。**                           |

所以，当我说“将集成时间从 2 天缩短到 2-3 小时”，这背后节省的不仅仅是工程师的时间，更是**降低了系统的维护成本和迭代风险**，极大地**提升了团队响应业务需求的速度和灵活性**。

---

## 怎么实现与上游 LLM 供应商的完全解耦？

### **一、 解耦前的痛点：厂商锁定与脆弱的代码**

在没有像 Spring AI 这样的抽象框架之前，如果我们想在 Java 应用中集成一个 LLM 供应商（比如 OpenAI），我们通常需要这样做：

1.  **引入特定厂商的 SDK**：在`pom.xml`中添加`openai-java`之类的依赖。
2.  **编写与 SDK 强绑定的代码**：我们的业务代码会直接使用该 SDK 提供的特定类和方法。

**一个“硬编码”的例子可能长这样：**

```java
// 这是与OpenAI SDK强耦合的代码
import com.openai.client.OpenAIClient;
import com.openai.client.model.ChatCompletionRequest;
import com.openai.client.model.ChatMessage;
// ...

@Service
public class HardcodedOpenAiService {

    private final OpenAIClient openAIClient;

    // ... 构造函数 ...

    public String chat(String userMessage) {
        // 1. 构建的是OpenAI特定的请求对象
        ChatMessage userMsg = new ChatMessage("user", userMessage);
        ChatCompletionRequest request = ChatCompletionRequest.builder()
                .model("gpt-4-turbo")
                .messages(List.of(userMsg))
                .build();

        // 2. 调用的是OpenAI特定的方法
        ChatCompletionResponse response = openAIClient.createChatCompletion(request);

        return response.getChoices().get(0).getMessage().getContent();
    }
}
```

**这种写法的致命缺陷是：**

- **厂商锁定 (Vendor Lock-in)**：我的`HardcodedOpenAiService`完全依赖于 OpenAI 的 SDK。如果有一天，我想换成 Cohere 的模型，或者一个更便宜的开源模型（如通过 Ollama 部署的 Llama3），我将不得不**重写**整个`HardcodedOpenAiService`类，包括它的请求构建、API 调用和响应解析逻辑。
- **代码脆弱**：如果 OpenAI 的 SDK 版本升级，修改了`ChatCompletionRequest`的构建方式，我的代码也必须跟着改，非常脆弱。

### **二、 Spring AI 的解决方案：ChatClient/EmbeddingClient 核心抽象**

Spring AI 的天才之处在于，它没有去发明新的 AI 模型，而是提供了一套**标准的、与供应商无关的接口（Interface）**。我的代码只需要面向这些通用接口编程，而 Spring Boot 的自动配置能力会在幕后处理好与具体厂商的对接。

我主要使用了两个核心抽象：

**1. `ChatClient`：统一的对话接口**

- **作用**：无论后端是 OpenAI、Cohere、Google Gemini 还是本地的 Ollama，`ChatClient`都为我的业务代码提供了一个**完全相同**的调用方式来进行对话。
- **我的实现**：我的 Service 层代码变得极其简洁和稳定。

```java
// 这是与Spring AI抽象解耦的代码
import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.chat.prompt.Prompt;

@Service
public class DecoupledAiService {

    private final ChatClient chatClient; // 注意：这里注入的是一个接口

    public DecoupledAiService(ChatClient.Builder builder) {
        // 通过Builder模式可以进行一些通用配置
        this.chatClient = builder.build();
    }

    public String chat(String userMessage) {
        // 我的代码只与Spring AI的标准Prompt和ChatClient接口交互
        Prompt prompt = new Prompt(userMessage);
        return chatClient.call(prompt).getResult().getOutput().getContent();
    }

    public Flux<String> streamChat(String userMessage) {
        // 流式调用也同样简单
        return chatClient.stream(userMessage).content();
    }
}
```

- **关键点**：我的`DecoupledAiService`**完全不知道**它背后到底是哪个 LLM 在工作。它只知道它在和一个实现了`ChatClient`接口的对象在对话。

**2. `EmbeddingClient`：统一的向量化接口**

- **作用**：在我的 RAG 知识处理管道中，我需要将文本块转换为向量。`EmbeddingClient`提供了统一的`embed()`方法来完成这个任务。
- **我的实现**：在 Spring Batch 的`ItemProcessor`中，我的代码同样只依赖于这个接口。

```java
// 在ItemProcessor中的代码片段
import org.springframework.ai.embedding.EmbeddingClient;

public class KnowledgeChunkProcessor implements ItemProcessor<String, KnowledgeChunk> {

    @Autowired
    private EmbeddingClient embeddingClient; // 注入的是接口

    @Override
    public KnowledgeChunk process(String textChunk) {
        // 调用统一的embed方法，无需关心背后是哪个Embedding模型
        List<Double> vector = this.embeddingClient.embed(textChunk);

        KnowledgeChunk chunk = new KnowledgeChunk();
        chunk.setText(textChunk);
        chunk.setVector(vector);
        return chunk;
    }
}
```

### **三、 如何实现“仅修改配置即更换模型”**

这背后是 Spring Boot**自动配置（Auto-configuration）**的魔力。

1.  **依赖决定实现**：我在`pom.xml`中加入哪个 Spring AI 的 starter 依赖，Spring Boot 就会在应用启动时自动配置那个厂商对应的`ChatClient`和`EmbeddingClient`的**具体实现类**。

    - 想用 OpenAI？加入`spring-ai-openai-spring-boot-starter`。
    - 想用 Cohere？加入`spring-ai-cohere-spring-boot-starter`。

2.  **配置驱动行为**：Spring Boot 会去读取我的`application.yml`文件，来获取 API Key、模型名称、超时时间等具体参数。

**一个完整的“更换供应商”操作流程如下：**

假设我当前正在使用**OpenAI**，现在想无缝切换到**Cohere**。

**第一步：修改 `pom.xml`**

- **移除** OpenAI 的依赖：
  ```xml
  <!-- <dependency>
      <groupId>org.springframework.ai</groupId>
      <artifactId>spring-ai-openai-spring-boot-starter</artifactId>
  </dependency> -->
  ```
- **添加** Cohere 的依赖：
  ```xml
  <dependency>
      <groupId>org.springframework.ai</groupId>
      <artifactId>spring-ai-cohere-spring-boot-starter</artifactId>
  </dependency>
  ```

**第二步：修改 `application.yml`**

- **注释掉或删除** OpenAI 的配置：
  ````yaml
  # spring:
  #   ai:
  #     openai:
  #       api-key: ${OPENAI_API_KEY}
  #       chat:
  #         options:
  #           model: gpt-4-turbo
  ```*   **添加** Cohere的配置：
  ```yaml
  spring:
    ai:
      cohere:
        api-key: ${COHERE_API_KEY}
        chat:
          options:
            model: command-r-plus
  ````

**第三步：重启应用。**

**完成！我的整个应用的 Java 代码一行都不需要修改。**

应用重启后，Spring Boot 的自动配置机制会：

1.  发现`spring-ai-cohere-spring-boot-starter`在 classpath 中。
2.  自动创建一个`CohereChatClient`的实例。
3.  当我`@Autowired ChatClient`时，Spring 的依赖注入容器会把这个`CohereChatClient`实例注入到我的`DecoupledAiService`中。

**总结来说**，通过拥抱 Spring AI 的核心抽象，我将我的业务逻辑与具体的、易变的 LLM 供应商实现进行**了彻底的隔离**。这使得我的系统具备了极高的**灵活性**和**可扩展性**，可以随时根据成本、性能或功能需求，轻松地切换到业界最新、最合适的模型，而无需承担高昂的代码重构代价。

---

## 如何采用 Docker 进行容器化部署，并集成 Actuator+Prometheus+Grafana 进行健康检查与服务监控？

### **第一部分：Docker 容器化 —— 实现标准化的、不可变的交付**

**1. 为什么需要 Docker？(The "Why")**

在没有 Docker 之前，我们面临着经典的**“环境一致性”**难题。“在我电脑上明明是好的，怎么一到服务器上就出错了？” 这类问题层出不穷，原因可能是服务器的 JDK 版本不对、操作系统依赖缺失、或者配置文件被误改。

Docker 通过**“集装箱”**的理念完美解决了这个问题。它将我的 Spring Boot 应用连同其运行所需的一切（操作系统库、JDK、配置文件）打包成一个轻量、可移植、与外界隔离的**镜像（Image）**。这个镜像就是我们交付的**“标准集装箱”**，无论是在开发、测试还是生产环境，运行起来的表现都是完全一致的。

**2. 我是怎么做的？(The "How")**

核心就是编写一个`Dockerfile`。这是一个文本文件，里面包含了构建镜像的所有指令。

**一个我项目中典型的`Dockerfile`如下（包含多阶段构建优化）：**

```dockerfile
# --- 第一阶段：构建阶段 (使用包含构建工具的完整JDK) ---
# 使用一个带有Maven的OpenJDK 17镜像作为基础
FROM maven:3.8.5-openjdk-17 AS build

# 设置工作目录
WORKDIR /app

# 复制pom.xml和src源代码到容器中
COPY pom.xml .
COPY src ./src

# 运行Maven打包命令，跳过测试以加快构建速度
# 将所有依赖打包成一个可执行的fat JAR
RUN mvn clean package -DskipTests

# --- 第二阶段：运行阶段 (使用轻量级的JRE) ---
# 使用一个只包含Java运行时的精简版镜像，大大减小最终镜像体积
FROM openjdk:17-jre-slim

# 设置工作目录
WORKDIR /app

# 从第一阶段(build)中，只复制我们需要的那个JAR文件
COPY --from=build /app/target/lingyi-medical-platform-0.0.1-SNAPSHOT.jar app.jar

# 暴露应用监听的端口 (与Spring Boot的server.port一致)
EXPOSE 8080

# 容器启动时执行的命令
# 增加了JVM参数用于优化内存，并指定Spring Profile
ENTRYPOINT ["java", "-Xms256m", "-Xmx512m", "-Dspring.profiles.active=prod", "-jar", "app.jar"]
```

**部署流程：**

1.  在项目根目录下运行 `docker build -t my-app:1.0 .` 来构建镜像。
2.  将镜像推送到私有或公有的镜像仓库（如 Harbor, Docker Hub）。
3.  在服务器上运行 `docker run -d -p 8080:8080 --name my-app-container my-app:1.0` 来启动容器。

通过这种方式，部署一个新版本变成了一个原子性的、可预测的操作，极大地提升了部署效率和可靠性。

### **第二部分：Actuator + Prometheus + Grafana —— 打造深度可观测性**

部署上去只是第一步，更关键的是要**实时了解**应用的运行状态。这套“监控三剑客”为我提供了从**数据暴露、数据采集到数据可视化**的完整解决方案。

#### **Step 1: Actuator —— 应用的“内置仪表盘”**

- **是什么**：它是 Spring Boot 的一个子项目，能自动地为应用添加一系列用于**监控和管理**的 HTTP 端点。我只需要在`pom.xml`中加入`spring-boot-starter-actuator`依赖即可。
- **我用了哪些核心端点**：
  - `/actuator/health`：检查应用的健康状况。这是最重要的端点，可以被 Docker 的健康检查机制或负载均衡器用来判断容器是否存活、是否准备好接收流量。
  - `/actuator/info`：展示应用的基本信息，我通常会配置它来显示 Git 的提交版本和构建时间，方便追溯线上代码版本。
  - `/actuator/prometheus`：**这是连接后续监控体系的桥梁**。当我在`pom.xml`中额外加入了`micrometer-registry-prometheus`依赖后，这个端点会以**Prometheus 能够识别的格式**，暴露海量的、精细的应用内部指标。

#### **Step 2: Prometheus —— “不知疲倦的指标采集员”**

- **是什么**：一个开源的**时序数据库（Time Series Database）**。它的核心工作模式是**“拉取（Pull/Scrape）”**。
- **它是如何工作的**：
  1.  我配置了一个`prometheus.yml`文件，告诉 Prometheus 服务器要去哪里“拉取”数据。
  2.  在这个配置文件里，我指定了一个抓取目标（Target），就是我部署的 Docker 容器的`IP:Port`，以及具体的路径`/actuator/prometheus`。
  3.  Prometheus 会按照我设定的时间间隔（例如每 15 秒），定时访问这个端点，把所有暴露的指标连同当前的时间戳一起，存入自己的时序数据库中。
- **我采集了哪些关键指标**：
  - `jvm_memory_used_bytes`：监控 JVM 堆内存使用情况，是排查内存泄漏的关键。
  - `http_server_requests_seconds_*`：这是一个**直方图（Histogram）**指标，包含了请求的总数和耗时分布，可以用来计算**QPS（每秒请求数）**和**P99/P95 延迟**。
  - `hikaricp_connections_active`：监控数据库连接池的活跃连接数，防止连接池耗尽。
  - `system_cpu_usage`：监控容器的 CPU 使用率。

#### **Step 3: Grafana —— “华丽的数据驾驶舱”**

- **是什么**：一个开源的**数据可视化**平台。它本身不存储数据，而是作为一个展示层。
- **它是如何工作的**：
  1.  我在 Grafana 中添加 Prometheus 作为**数据源（Data Source）**。
  2.  然后我创建了一个**仪表盘（Dashboard）**，仪表盘由多个**面板（Panel）**组成。
  3.  在每个面板中，我使用**PromQL（Prometheus Query Language）**向 Prometheus 查询我关心的指标，并选择合适的图表类型（如折线图、仪表盘、告警列表）进行展示。
- **我的 Dashboard 上有哪些关键图表**：
  - **核心服务概览**：用仪表盘展示实时的 QPS、P99 延迟和错误率。
  - **JVM 健康状况**：用折线图展示 JVM 堆内存使用、GC 次数和 GC 耗时。
  - **HTTP 请求分析**：按不同的 API 端点分组，展示各自的请求量和延迟。
  - **数据库连接池监控**：用一个带有阈值的图表，展示当前活跃连接数占总连接数的百分比。
- **告警（Alerting）**：我还在 Grafana 中配置了告警规则。例如，如果 P99 延迟连续 5 分钟超过 1 秒，或者 JVM 内存使用率持续高于 90%，Grafana 就会通过 Webhook 触发告警，通知到我们的企业微信或钉钉群，让我们能**在用户投诉之前**就发现并处理问题。

**总结**：这套体系形成了一个完美的闭环：**Docker**保证了部署的**一致性**和**敏捷性**。**Actuator**为应用植入了**“感官”**。**Prometheus**则作为**“中枢神经系统”**，持续收集这些感官信息。最后，**Grafana**作为**“大脑皮层”**，将这些信息处理成直观的洞察，并能在异常时发出**“痛觉信号”（告警）**。这极大地提升了我们对线上系统的掌控力，是保障系统稳定性和可维护性的关键所在。
