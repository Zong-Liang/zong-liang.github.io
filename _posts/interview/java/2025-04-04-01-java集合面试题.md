---
title: "01-Java集合面试题"
date: 2025-04-04 14:24:48 +0800
categories: [面试题, Java集合面试题]
tags: [面试八股, Java集合面试题]
pin: false
toc: true
math: true
---

## Java 中的集合框架及常用工具类？⭐⭐⭐⭐⭐

Java 集合框架（Java Collections Framework）是一个统一的体系结构，用于表示和操作对象集合。它位于 `java.util` 包中，提供了多种数据结构和算法，可以帮助我们高效地存储和处理数据。

这个框架主要由两部分组成：**核心接口**和**实现类**。

### 一、核心接口

Java 集合框架主要有两大根接口：`Collection` 和 `Map`。

#### 1. Collection 接口

`Collection` 接口是处理一组对象的顶层接口。它定义了集合的基本操作，如添加、删除、清空和遍历等。它主要有三个子接口：

- **List (列表):**

  - **特点:** 元素有序，且可以重复。可以通过索引来访问、添加或删除元素。
  - **常用实现类:**
    - **ArrayList:** 基于动态数组实现，非常适合随机访问（通过索引获取元素），查询速度快。但在列表中间插入和删除元素时，由于需要移动后续元素，性能会相对较差。
    - **LinkedList:** 基于双向链表实现。 它在插入和删除元素时性能非常高，因为只需要修改前后元素的指针即可。但是，它的随机访问性能较差，需要从头或尾开始遍历。LinkedList 还实现了 `Deque` 接口，可以作为双端队列或栈来使用。
    - **Vector:** 这是一个历史悠久的实现，与 `ArrayList` 类似，但它是线程安全的，因为其方法大多被 `synchronized` 关键字修饰。由于性能开销较大，现在已不推荐使用，取而代之的是 `java.util.concurrent` 包下的并发集合。

- **Set (集合):**

  - **特点:** 元素无序（通常情况下），且不允许重复。它主要用于存储唯一的元素。
  - **常用实现类:**
    - **HashSet:** 基于哈希表（底层是 `HashMap`）实现。 它不保证元素的顺序，并且允许存入一个 `null` 值。添加和查询元素的时间复杂度接近 $O(1)$。 判断元素是否重复依赖于元素的 `hashCode()` 和 `equals()` 方法。
    - **TreeSet:** 基于红黑树实现。 存入的元素会自动进行排序，可以是自然排序或通过 `Comparator` 自定义排序。 因此，它要求存入的元素是可比较的。
    - **LinkedHashSet:** 继承自 `HashSet`，同时使用链表维护元素的插入顺序。因此，它既有 `HashSet` 的快速查找特性，又能保证元素按插入顺序排列。

- **Queue (队列):**
  - **特点:** 遵循先进先出（FIFO）的原则。常用于需要排队处理任务的场景。`Deque` 是 `Queue` 的一个子接口，代表双端队列，允许在两端进行添加和删除操作。

#### 2. Map 接口

`Map` 接口用于存储键值对（key-value）的数据。 键是唯一的，不能重复，每个键最多映射到一个值。

- **特点:** 它不继承自 `Collection` 接口，自成一体。
- **常用实现类:**
  - **HashMap:** 基于哈希表实现，是 `Map` 最常用的实现。它不保证键值对的顺序，并且允许一个 `null` 键和多个 `null` 值。它的性能非常高，增删改查的时间复杂度接近 $O(1)$。
  - **TreeMap:** 基于红黑树实现。 存入的键值对会根据键（Key）进行排序，可以是自然排序或自定义排序。
  - **LinkedHashMap:** 继承自 `HashMap`，它使用一个双向链表来维护键值对的插入顺序或访问顺序。
  - **Hashtable:** 也是一个历史悠久的实现，与 `HashMap` 类似，但它是线程安全的。同样因为性能问题，现在已不推荐使用，推荐使用 `ConcurrentHashMap`。

### 二、常用工具类

Java 集合框架提供了一些非常有用的工具类，用于对集合进行操作。

#### 1. Collections

这是针对集合容器的工具类，提供了大量静态方法，非常方便。 常用功能包括：

- **排序与随机化:**
  - `sort(List<T> list)`: 对 List 进行排序。
  - `shuffle(List<?> list)`: 随机打乱 List 中元素的顺序。
- **查找与替换:**
  - `binarySearch(List<?> list, T key)`: 在有序的 List 中进行二分查找。
  - `max(Collection<?> coll)` / `min(Collection<?> coll)`: 查找最大/最小值。
- **线程安全转换:**
  - `synchronizedList(List<T> list)` / `synchronizedSet(Set<T> s)` / `synchronizedMap(Map<K,V> m)`: 将指定的非线程安全集合包装成线程安全的集合。
- **创建不可变集合:**
  - `emptyList()` / `emptySet()` / `emptyMap()`: 创建一个空的不可变集合。
  - `singletonList(T o)`: 创建一个只包含一个元素的不可变集合。

#### 2. Arrays

这是针对数组的工具类，虽然不是严格意义上的集合框架的一部分，但经常与集合一起使用。

- `asList(T... a)`: 将一个数组或一组元素转换成一个 `List` 集合。需要注意的是，这个 `List` 是固定大小的，不支持 `add` 和 `remove` 操作。
- `sort(int[] a)` / `sort(Object[] a)`: 对数组进行排序。
- `binarySearch(int[] a, int key)`: 在有序数组中进行二分查找。
- `copyOf(T[] original, int newLength)`: 拷贝数组。

### 总结

在实际开发中，选择哪种集合实现类，主要取决于具体的业务场景：

- 如果需要频繁通过索引访问，且插入和删除操作不多，`ArrayList` 是最佳选择。
- 如果需要频繁地在列表中间进行插入和删除，`LinkedList` 更合适。
- 如果需要存储不重复的元素且不关心顺序，`HashSet` 效率最高。
- 如果需要存储不重复的元素并希望它们自动排序，应选择 `TreeSet`。
- 如果需要存储键值对且不关心顺序，`HashMap` 是首选。
- 如果希望键值对按键排序，应使用 `TreeMap`。

---

## ArrayList 和 LinkedList 有什么区别？

ArrayList 和 LinkedList 是 `List` 接口的两个重要实现，它们之间的主要区别体现在以下几个核心方面：

### 1. 底层数据结构

- **ArrayList:** 它的底层是基于**动态数组**（Dynamic Array）实现的。这意味着 ArrayList 在内存中是一块连续的空间。
- **LinkedList:** 它的底层是基于**双向链表**（Doubly Linked List）实现的。它在内存中的存储空间可以是不连续的，每个元素（节点）除了存储数据本身，还存储着指向前一个元素和后一个元素的引用（指针）。

### 2. 访问效率（查询）

- **ArrayList (快):** 由于其底层是数组，并且实现了 `RandomAccess` 接口，所以它支持快速的随机访问。通过索引（index）直接访问元素的时间复杂度是 $O(1)$，因为可以根据索引直接计算出内存地址。
- **LinkedList (慢):** 它不支持高效的随机访问。如果要访问第 n 个元素，它必须从头节点（或尾节点）开始，沿着链表一个一个地遍历，直到找到目标元素。因此，其查询的时间复杂度是 $O(n)$。

### 3. 插入和删除效率

- **ArrayList (慢，除非在末尾):**

  - **在末尾添加/删除:** 这种操作非常快，时间复杂度接近 $O(1)$（不考虑扩容的情况下）。
  - **在中间或开头添加/删除:** 这种操作效率较低，时间复杂度是 $O(n)$。因为在数组中插入或删除一个元素后，需要移动该位置之后的所有元素来保持数组的连续性。

- **LinkedList (快):**
  - **在开头/末尾添加/删除:** 由于是双向链表，它持有头尾节点的引用，因此在两端进行操作非常高效，时间复杂度为 $O(1)$。
  - **在中间添加/删除:** 只要已经定位到要操作的节点（例如通过迭代器），插入或删除操作本身非常快，只需要修改前后节点的引用即可，时间复杂度为 $O(1)$。但是，如果需要先查找到那个节点，查找过程的时间复杂度仍然是 $O(n)$。

### 4. 内存开销

- **ArrayList:** 主要开销在于其底层数组本身。为了避免频繁扩容，ArrayList 在容量不足时会进行扩容，通常是扩大到原来的 1.5 倍。这可能会导致一部分内存空间的浪费，因为它预留了额外的容量。
- **LinkedList:** 除了存储数据本身，它的每个节点都需要额外存储两个引用（指向前驱和后继的指针）。因此，在存储相同数量的元素时，LinkedList 通常比 ArrayList 占用更多的内存空间。

### 总结与应用场景

| 特性              | ArrayList                                              | LinkedList                                                 |
| :---------------- | :----------------------------------------------------- | :--------------------------------------------------------- |
| **底层结构**      | 动态数组                                               | 双向链表                                                   |
| **随机访问**      | 快 ($O(1)$)                                            | 慢 ($O(n)$)                                                |
| **末尾插入/删除** | 快 ($O(1)$)                                            | 快 ($O(1)$)                                                |
| **中间插入/删除** | 慢 ($O(n)$)                                            | 快 ($O(1)$) (找到后)                                       |
| **内存开销**      | 较小，但可能因扩容浪费空间                             | 较大，每个节点都有额外开销                                 |
| **主要场景**      | **查询多，增删少**的场景。适合作为基础的数据存储容器。 | **增删多，查询少**的场景。特别适合实现队列、栈等数据结构。 |

简单来说，选择的关键在于 **“是否需要频繁地在列表的非末尾位置进行插入和删除操作”**。如果答案是“是”，那么 LinkedList 是更好的选择。如果主要是进行查询或者只在列表末尾进行添加，那么 ArrayList 的性能会更优。

---

## ArrayList 的扩容机制了解吗？

当向 ArrayList 中添加元素，而其内部的数组容量不足以容纳新元素时，ArrayList 就会触发一次**扩容**。这个过程的核心是**创建一个更大的新数组，并将旧数组中的所有元素复制到新数组中**。

下面从源码层面详细解释这个机制：

### 1. 核心思想

ArrayList 的核心是**用空间换时间**。它内部维护一个数组 `elementData` 来存储元素。为了避免每次添加元素都重新分配内存，它会一次性申请一块更大的内存空间，即数组的 `capacity`（容量）通常会大于其实际存储的元素数量 `size`。当 `size` 即将超过 `capacity` 时，才进行扩容。

### 2. 扩容的时机

扩容主要发生在调用 `add(E e)` 方法时。我们可以看一下它的简化版源码：

```java
public boolean add(E e) {
    ensureCapacityInternal(size + 1);  // 关键步骤：确保容量充足
    elementData[size++] = e;           // 将元素放入数组
    return true;
}
```

触发扩容检查的核心方法是 `ensureCapacityInternal()`。

### 3. 扩容的具体流程

整个扩容流程可以分为以下几个关键步骤：

#### **第一步：计算最小需求容量**

`ensureCapacityInternal(int minCapacity)` 方法会判断当前数组是否为默认的空数组，如果是，则取默认容量（`DEFAULT_CAPACITY`，值为 10）和 `minCapacity` 中的较大值作为最小需求容量。否则，直接进入下一步。

#### **第二步：判断是否需要扩容**

`ensureExplicitCapacity(int minCapacity)` 方法会进行真正的判断。它检查传入的最小需求容量 `minCapacity` 是否大于当前数组的长度 `elementData.length`。如果大于，就说明需要扩容，此时会调用 `grow()` 方法。

#### **第三步：执行扩容（核心）**

`grow(int minCapacity)` 方法是扩容机制的核心所在。它的逻辑如下：

1.  **计算新容量**：

    - 获取旧容量：`int oldCapacity = elementData.length;`
    - 计算新容量：`int newCapacity = oldCapacity + (oldCapacity >> 1);`
    - 这里的 `(oldCapacity >> 1)` 是位运算，等同于 `oldCapacity / 2`。所以，**新容量通常是旧容量的 1.5 倍**。

2.  **与最小需求容量比较**：

    - 将计算出的 `newCapacity` 与 `minCapacity` 比较，如果 `newCapacity` 还小于 `minCapacity`，说明 1.5 倍的扩容仍然不够（例如，使用 `addAll` 方法一次性添加大量元素时），此时会直接将 `minCapacity` 作为新容量。
    - `if (newCapacity - minCapacity < 0) newCapacity = minCapacity;`

3.  **与最大容量比较**：

    - 还会检查新容量是否超过了 ArrayList 所能分配的最大数组大小（`MAX_ARRAY_SIZE`，通常是 `Integer.MAX_VALUE - 8`）。如果超过了，会进行特殊处理，最终将容量设置为 `Integer.MAX_VALUE`。

4.  **创建新数组并拷贝元素**：
    - 最后，通过 `Arrays.copyOf(elementData, newCapacity)` 方法，创建一个大小为 `newCapacity` 的新数组，并将旧数组 `elementData` 中的所有元素原封不动地拷贝到新数组中。
    - 然后将 ArrayList 内部的 `elementData` 引用指向这个新的数组。

### 4. 几个关键点的总结

- **初始容量**：如果使用无参构造函数创建 ArrayList，它初始时会是一个空数组。在第一次添加元素时，容量会扩充到 **10**。
- **扩容因子**：每次扩容，容量会增长到原来的 **1.5 倍**。选择 1.5 倍是在时间和空间成本之间的一个权衡。如果因子太大，会浪费内存；如果太小，会导致频繁扩容，影响性能。
- **实现方式**：底层的扩容是通过 `Arrays.copyOf()` 来实现的，这是一个 native 方法，效率较高。
- **性能损耗**：扩容的主要性能开销在于**数组的拷贝**。如果能提前预估需要存储的元素数量，在创建 ArrayList 时就通过构造函数 `ArrayList(int initialCapacity)` 指定一个合理的初始容量，可以有效减少甚至避免扩容操作，从而提高性能。

---

## ArrayList 怎么序列化的知道吗？

ArrayList 确实是可以序列化的，因为它实现了 `java.io.Serializable` 接口。但它的序列化方式比较特殊，并不是采用 Java 默认的序列化机制，而是通过自定义的方式实现的。

核心就在于其存储元素的内部数组 `elementData` 被 `transient` 关键字修饰了。

```java
public class ArrayList<E> ... implements ... java.io.Serializable {
    // ...
    transient Object[] elementData; // 存储元素的数组
    private int size; // 实际元素数量
    // ...
}
```

### 1. 为什么 `elementData` 要被 `transient` 修饰？

`transient` 关键字的作用是告诉 Java 序列化机制，**不要序列化这个字段**。乍一看会很奇怪，`elementData` 是存储所有数据的地方，不序列化它，那序列化的意义何在？

这么做的主要原因是**为了优化空间和性能**：

- **避免序列化空闲空间**：ArrayList 是一个动态数组，其内部数组的容量（`elementData.length`）通常会大于其实际存储的元素个数（`size`）。 比如，一个 ArrayList 容量为 100，但只存了 3 个元素。如果直接序列化 `elementData` 数组，那么剩下的 97 个 `null` 元素也会被写入字节流，这会极大地浪费存储空间和网络带宽。
- **灵活性和安全性**：通过自定义序列化，可以在序列化和反序列化过程中加入自己的逻辑，例如版本控制或者一些校验。

### 2. 自定义序列化：`writeObject` 和 `readObject` 方法

既然不能用默认的序列化，ArrayList 就通过重写两个私有方法 `writeObject()` 和 `readObject()` 来实现自定义的序列化和反序列化。当一个实现了 `Serializable` 接口的类定义了这两个方法，Java 的序列化机制就会调用它们，而不是执行默认的序列化逻辑。

#### **序列化过程 (`writeObject`)**

当对一个 ArrayList 实例进行序列化时，`ObjectOutputStream` 会调用其 `writeObject` 方法。这个方法执行以下操作：

1.  **调用 `s.defaultWriteObject()`**：这个方法会先执行默认的序列化操作，将 ArrayList 中所有**非 `transient` 和非 `static`** 的字段（比如 `size` 字段）写入流中。
2.  **写入 `size`**：显式地调用 `s.writeInt(size)` 将列表的实际大小写入流。
3.  **遍历并写入元素**：它会遍历 `elementData` 数组，但只从索引 0 遍历到 `size - 1`，然后依次将每个**有效的元素**通过 `s.writeObject()` 写入到输出流中。

这样一来，只有真正存在的元素被序列化了，数组中的空闲部分被完全忽略，从而节省了空间。

#### **反序列化过程 (`readObject`)**

当从输入流中反序列化一个 ArrayList 对象时，`ObjectInputStream` 会调用其 `readObject` 方法。这个方法执行以下操作：

1.  **调用 `s.defaultReadObject()`**：首先执行默认的反序列化，读取并恢复非 `transient` 的字段，比如 `size`。
2.  **读取 `size` 并分配数组**：读取之前写入的 `size` 值。然后根据这个 `size` 值，创建一个新的 `elementData` 数组。这样可以确保新创建的数组大小刚刚好，不多也不少。
3.  **遍历并读取元素**：根据 `size` 的大小，循环相应次数，每次调用 `s.readObject()` 从输入流中读取一个对象，并依次存入新创建的 `elementData` 数组中。

### 总结

总的来说，ArrayList 的序列化机制是一个非常巧妙的设计：

1.  通过将核心数组 `elementData` 声明为 `transient`，成功绕过了默认的、可能浪费空间的序列化方式。
2.  通过实现私有的 `writeObject` 和 `readObject` 方法，精准地控制了序列化的内容，只将有效元素写入字节流。
3.  在反序列化时，根据读取到的 `size` 来精确地重构数组，避免了内存的浪费。

---

## fail-fast 了解吗？

fail-fast (快速失败) 是 Java 集合框架中一个非常重要的错误检测机制。

### 1. 什么是 Fail-Fast？

Fail-fast 是一种系统设计哲学，应用在 Java 集合的迭代器（Iterator）上。其核心思想是：**当一个集合在被迭代的过程中，如果其结构被“非迭代器自身”的方式修改了，那么迭代器应该立即感知到，并迅速抛出 `ConcurrentModificationException` 异常，而不是在未来某个不确定的时间点产生无法预料的行为。**

简单来说，就是“我正在遍历你，如果你在我不知情的情况下被别人改了，我就立刻报错，不继续了”。

### 2. Fail-Fast 的工作原理

要理解它的工作原理，就必须提到一个关键的内部变量：`modCount` (modification count)。

1.  **内部计数器 `modCount`**：
    在大多数 `java.util` 包下的非线程安全集合类中（如 ArrayList, HashMap），都有一个名为 `modCount` 的成员变量。这个变量的作用是记录该集合**结构上被修改的次数**。所谓的“结构修改”指的是那些会改变集合大小或结构的操作，比如 `add()`, `remove()`, `clear()` 等，但不包括修改元素内容的操作（例如，`list.get(0).setName("new name")`）。

2.  **迭代器创建时的状态记录**：
    当你通过 `collection.iterator()` 方法获取一个迭代器时，这个迭代器对象内部会创建一个名为 `expectedModCount` 的变量，并用当前集合的 `modCount` 值来初始化它。
    `int expectedModCount = modCount;`

3.  **迭代过程中的检查**：
    在迭代的每一步（即调用迭代器的 `next()` 或 `hasNext()` 方法时），迭代器都会执行一次检查：比较它自己内部存储的 `expectedModCount` 和集合当前实时的 `modCount` 是否相等。
    - **如果相等**：说明从创建迭代器到此刻，集合没有被外部修改过，迭代可以安全继续。
    - **如果不相等**：说明在迭代期间，有人调用了集合的 `add()` 或 `remove()` 方法（而不是迭代器自己的 `remove()` 方法），导致集合的 `modCount` 增加了。这时，迭代器会立刻抛出 `ConcurrentModificationException`。

### 3. 一个典型的 Fail-Fast 例子

```java
import java.util.ArrayList;
import java.util.List;

public class FailFastExample {
    public static void main(String[] args) {
        List<String> list = new ArrayList<>();
        list.add("A");
        list.add("B");
        list.add("C");

        // for-each 循环内部就是在使用迭代器
        try {
            for (String item : list) {
                if ("B".equals(item)) {
                    // 错误操作：在迭代时通过集合本身的方法修改结构
                    list.remove(item);
                }
            }
        } catch (java.util.ConcurrentModificationException e) {
            System.out.println("捕获到了 ConcurrentModificationException！");
            e.printStackTrace();
        }
    }
}
```

在上面的代码中，当 `for-each` 循环（其底层就是 `Iterator`）遍历到 "B" 时，我们调用了 `list.remove(item)`。这个操作会使 `list` 的 `modCount` 加 1。当循环准备进入下一步，调用迭代器的 `next()` 方法时，迭代器会发现它自己的 `expectedModCount` 不再等于 `list` 的 `modCount`，于是立即抛出异常。

### 4. 如何在迭代时安全地删除元素？

要避免这个异常，必须使用迭代器自己的 `remove()` 方法，因为这个方法在删除元素的同时，会更新迭代器内部的 `expectedModCount`，使其与集合的 `modCount` 保持同步。

**正确示例：**

```java
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

public class FailFastCorrectExample {
    public static void main(String[] args) {
        List<String> list = new ArrayList<>();
        list.add("A");
        list.add("B");
        list.add("C");

        Iterator<String> iterator = list.iterator();
        while (iterator.hasNext()) {
            String item = iterator.next();
            if ("B".equals(item)) {
                // 正确操作：使用迭代器自身的 remove() 方法
                iterator.remove();
            }
        }
        System.out.println("删除后的列表: " + list); // 输出: [A, C]
    }
}
```

### 5. Fail-Fast 的意义与局限性

- **意义**：Fail-fast 机制是一种**错误检测机制**，它并不能保证线程安全。它的主要目的是尽早地暴露并发修改相关的问题，提醒程序员代码中可能存在的 bug，从而避免因数据不一致而导致的不可预期的结果。
- **局限性**：它**不是一个完全可靠的机制**。它只是一种尽力而为（best-effort）的检测。在多线程环境下，如果一个线程通过迭代器遍历集合，而另一个线程修改了集合，`ConcurrentModificationException` **不保证一定会被抛出**。只有当迭代器的检查操作恰好发生在修改之后，异常才会被触发。

与 Fail-Fast 相对的是 **Fail-Safe（安全失败）**机制，它通常出现在 `java.util.concurrent` 包下的并发容器中，如 `ConcurrentHashMap` 和 `CopyOnWriteArrayList`。这类容器的迭代器工作在集合的**快照或副本**上，因此不会抛出 `ConcurrentModificationException`，但缺点是可能无法反映集合最新的修改。

---

## fail-safe 了解吗？

Fail-Safe（安全失败）机制与上面的 Fail-Fast 形成鲜明对比，是处理并发修改问题的另一种策略。

### 1. 什么是 Fail-Safe？

Fail-Safe 是一种集合框架的错误处理机制，主要应用于并发容器中。其核心思想是：**在迭代一个集合时，即使集合的结构被其他线程修改，迭代器也不会抛出 `ConcurrentModificationException` 异常。** 它以牺牲数据一致性的实时性为代价，来保证迭代过程的顺利完成。

简单来说，就是“你尽管修改，我（迭代器）假装看不见，继续在我刚开始迭代时的那个版本上工作，保证不出错”。

### 2. Fail-Safe 的工作原理

Fail-Safe 机制的实现通常是基于**在集合的副本或快照（Snapshot）上进行迭代**。它不直接在原始集合上进行遍历。

1.  **创建副本/快照**：当通过 `iterator()` 方法获取迭代器时，Fail-Safe 容器不是直接返回一个与原始数据结构关联的迭代器，而是会创建一个原始集合在某一时刻的副本或快照。
2.  **在副本上迭代**：后续迭代器的所有操作（如 `hasNext()`, `next()`）都是在这个副本上进行的。
3.  **与原始集合解耦**：由于迭代器操作的是副本，因此任何来自其他线程对原始集合的修改（如 `add()`, `remove()`）都完全影响不到这个迭代器。迭代器看到的数据永远是它被创建时的那个版本。

### 3. Fail-Safe 的典型实现类

Fail-Safe 机制主要体现在 `java.util.concurrent` 包下的并发容器中：

- **`CopyOnWriteArrayList`**：这是一个核心的 Fail-Safe 实现。

  - **写操作**：任何修改操作（`add`, `set`, `remove` 等）都会在一个新的底层数组副本上进行，这些操作是加锁的，以保证线程安全。修改完成后，再将指向旧数组的引用切换到这个新数组上。
  - **读操作**：读操作（包括获取迭代器）完全不加锁，并且直接在当前的底层数组上进行。
  - **迭代**：当你获取 `CopyOnWriteArrayList` 的迭代器时，这个迭代器会持有对当前底层数组的引用。即使后续其他线程修改了列表（这会导致列表内部引用一个全新的数组），该迭代器仍然引用着它被创建时的那个旧数组。因此，它不会看到任何新的修改，也不会抛出异常。

- **`ConcurrentHashMap`**：在 JDK 1.8 之前，它的 `Segment` 分段锁设计也体现了 Fail-Safe 的思想，迭代器在遍历时，段内的修改可能不会被反映出来。在 JDK 1.8 之后，虽然底层改为了 `CAS` + `synchronized`，但其迭代器仍然被设计为弱一致性（Weakly Consistent）的，它能容忍并发修改，尽力反映集合在迭代开始或之后某个时间点的状态，但**不保证**能反映迭代期间的所有修改，也**不会**抛出 `ConcurrentModificationException`。

### 4. Fail-Safe 的优缺点

#### 优点：

- **无并发修改异常**：最大的优点是在多线程环境下进行迭代时，不会抛出 `ConcurrentModificationException`，迭代过程不会因为其他线程的修改而中断。
- **高并发读取**：由于迭代器工作在副本上，对原始集合的读取和迭代操作通常是无锁的，这大大提高了并发读取的性能。

#### 缺点：

- **数据一致性问题**：迭代器遍历的是集合的快照，因此它**无法反映出迭代过程中集合的最新变化**。如果一个线程正在遍历，另一个线程添加了一个新元素，遍历的线程是看不到这个新元素的。这是一种数据不一致的表现。
- **内存开销较大**：以 `CopyOnWriteArrayList` 为例，每次写操作都需要复制整个底层数组，当集合非常大时，这会带来巨大的内存开销和时间开销。因此，它只适用于**读多写少**的场景。

### Fail-Fast vs. Fail-Safe 总结

| 特性           | Fail-Fast                                    | Fail-Safe                                   |
| :------------- | :------------------------------------------- | :------------------------------------------ |
| **代表集合**   | `ArrayList`, `HashMap`                       | `CopyOnWriteArrayList`, `ConcurrentHashMap` |
| **工作机制**   | 直接在原集合上迭代，通过 `modCount` 检查修改 | 在集合的副本或快照上迭代                    |
| **并发修改时** | 抛出 `ConcurrentModificationException`       | 不抛出异常，继续迭代                        |
| **数据一致性** | 强一致性（一旦发现不一致就报错）             | 弱一致性（迭代器看到的是旧数据）            |
| **适用场景**   | 单线程环境，或需要严格控制并发修改的场景     | 多线程环境，特别是**读多写少**的场景        |

总而言之，Fail-Fast 和 Fail-Safe 代表了两种不同的设计权衡。Fail-Fast 选择了“暴露问题”，而 Fail-Safe 选择了“容忍问题”并保证程序的运行。在并发编程中，根据业务需求对数据一致性的要求，选择合适的集合类至关重要。

---

## 有哪几种实现 ArrayList 线程安全的方法？⭐⭐⭐⭐⭐

`ArrayList` 本身是线程不安全的，如果在多线程环境下对其进行并发修改，很容易导致数据不一致、抛出 `ConcurrentModificationException` 等问题。

要实现 `ArrayList` 的线程安全，主要有以下几种常见的方法：

### 1. 使用 `Collections.synchronizedList()` 工具类（装饰器模式）

这是 Java 集合框架提供的最直接的包装方法。`Collections` 工具类可以把一个普通的 `ArrayList` 包装成一个线程安全的 `List`。

**实现方式：**

```java
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

List<String> list = new ArrayList<>();
List<String> synchronizedList = Collections.synchronizedList(list);

// 现在 synchronizedList 的所有单个操作都是线程安全的
synchronizedList.add("A"); // synchronized
synchronizedList.get(0);   // synchronized
```

**工作原理：**
它采用的是**装饰器设计模式**。`synchronizedList()` 方法返回一个名为 `SynchronizedList` 的内部类实例，这个实例持有了原始 `ArrayList` 的引用。`SynchronizedList` 中的每一个方法（如 `add`, `get`, `remove` 等）都被 `synchronized` 关键字修饰，它们以自身对象作为锁。这相当于为 `ArrayList` 的每个操作都套上了一层同步锁。

**优缺点：**

- **优点：** 使用简单，是官方提供的标准方法。
- **缺点：**
  - **性能较差：** 它对所有方法都进行同步，即使是某些不需要同步的读操作，也会产生锁的开销，导致并发性能不佳。
  - **复合操作非原子性：** 虽然单个方法是线程安全的，但多个方法的组合操作却不是。例如，“检查后执行”（check-then-act）的场景：
    ```java
    // 存在竞态条件！
    if (!synchronizedList.contains("B")) {
        synchronizedList.add("B");
    }
    ```
    在 `contains()` 和 `add()` 两个方法调用之间，另一个线程可能已经添加了 "B"，导致 "B" 被重复添加。
  - **迭代非线程安全：** 对 `synchronizedList` 进行遍历（如 for-each 循环）时，必须手动在 `synchronized` 块中进行，否则在遍历期间若有其他线程修改了列表，仍然会抛出 `ConcurrentModificationException`。
    ```java
    // 正确的迭代方式
    synchronized (synchronizedList) {
        for (String item : synchronizedList) {
            // ...
        }
    }
    ```

### 2. 使用 `Vector` 类

`Vector` 是一个历史悠久的集合类，从 JDK 1.0 就存在了。它和 `ArrayList` 的功能非常相似，但 `Vector` 内部所有公开的方法都是用 `synchronized` 修饰的，因此它是**天生线程安全**的。

**实现方式：**

```java
import java.util.Vector;

List<String> vector = new Vector<>();
vector.add("A");
vector.add("B");
```

**优缺点：**

- **优点：** 使用简单，和 `ArrayList` API 类似。
- **缺点：** **基本已被弃用**。主要原因是其性能问题。与 `Collections.synchronizedList()` 类似，它锁定了每一个方法，导致并发效率低下。现在有了更高效的并发容器后，已经不推荐使用 `Vector`。

### 3. 使用 `CopyOnWriteArrayList`（推荐用于读多写少）

这是 `java.util.concurrent` 包下提供的一个专门为并发场景设计的 `List` 实现，是实现线程安全的**首选方案之一**。

**实现方式：**

```java
import java.util.concurrent.CopyOnWriteArrayList;

List<String> list = new CopyOnWriteArrayList<>();
list.add("A");
list.add("B");
```

**工作原理：**
它采用了 **“写时复制”（Copy-On-Write）** 的策略。

- **读操作：** 读取数据时完全不加锁，直接读取底层数组的数据，性能非常高。
- **写操作：** 当需要修改（`add`, `remove`, `set`）列表时，它会先加锁，然后**复制一份底层数组的副本**，在新副本上执行修改操作，最后再将内部引用指向这个新的数组。
- **迭代器：** 它的迭代器工作在创建迭代器时所引用的那个数组快照上，是 **安全失败（Fail-Safe）** 的。这意味着在迭代过程中，即使原始列表被修改，迭代器也不会抛出 `ConcurrentModificationException`，它只会遍历快照中的旧数据。

**优缺点：**

- **优点：**
  - 并发读取性能极高，因为读操作无锁。
  - 迭代绝对安全，不会抛出并发修改异常。
- **缺点：**
  - **写操作成本高：** 每次修改都涉及数组的完整复制，非常耗时耗内存。
  - **数据一致性是最终一致性：** 读操作可能读到的是旧数据，因为写操作的结果需要一定时间才能对后续的读操作可见。

**适用场景：**
非常适合**读多写少**的并发场景，例如事件监听器列表、配置信息存储等。

### 总结与对比

| 方法                               | 实现原理                                 | 性能                           | 迭代器安全性                       | 适用场景                         |
| :--------------------------------- | :--------------------------------------- | :----------------------------- | :--------------------------------- | :------------------------------- |
| **`Collections.synchronizedList`** | 装饰器模式，所有方法加 `synchronized` 锁 | 差，所有操作都需要竞争同一把锁 | **Fail-Fast** (不安全，需手动同步) | 遗留代码或不追求高性能的简单场景 |
| **`Vector`**                       | 内部方法使用 `synchronized`              | 差，与前者类似，已被弃用       | **Fail-Fast** (不安全，需手动同步) | 基本不推荐使用                   |
| **`CopyOnWriteArrayList`**         | 写时复制 (Copy-On-Write)                 | **读极快，写极慢**             | **Fail-Safe** (安全)               | **读多写少**的并发场景           |

在现代 Java 开发中，如果需要一个线程安全的列表，我们应该首先考虑**`CopyOnWriteArrayList`**，并评估我们的场景是否符合其“读多写少”的特点。如果写操作非常频繁，那么可能需要考虑使用其他数据结构，如并发队列（`ConcurrentLinkedQueue`）或通过手动使用 `ReentrantLock` 来精细控制 `ArrayList` 的并发访问。

---

## CopyOnWriteArrayList 了解吗？

`CopyOnWriteArrayList` 是 `java.util.concurrent` 包下的一个核心并发容器，

### 1. 核心思想：写时复制 (Copy-On-Write)

`CopyOnWriteArrayList` 的名字已经完美地概括了它的工作原理——**Copy-On-Write**，即“写时复制”。

- **读操作**：当进行读操作时（如 `get()`, `iterator()`, `size()`），它不会加锁，而是直接读取内部数组的数据。这使得读操作的性能非常高，可以支持高并发的读取。
- **写操作**：当需要对列表进行修改时（如 `add()`, `remove()`, `set()`），它不会直接在当前数组上修改，而是：
  1.  **加锁**：首先使用 `ReentrantLock` 独占锁，确保同一时刻只有一个线程能执行写操作。
  2.  **复制**：将底层数组完整地**复制**一份，得到一个全新的数组。
  3.  **修改**：在新复制出来的数组上执行修改操作。
  4.  **替换**：将内部指向底层数组的引用，从旧数组原子性地切换到新数组。
  5.  **解锁**：释放锁。

因为所有的写操作都是在副本上进行的，所以正在进行读操作的线程完全不受影响，它们仍然读取的是修改发生前的那个旧数组的数据。

### 2. `add()` 方法源码分析（简化版）

`add` 方法是理解其机制的最佳入口：

```java
public boolean add(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock(); // 1. 加锁
    try {
        Object[] elements = getArray(); // 获取旧数组
        int len = elements.length;
        Object[] newElements = Arrays.copyOf(elements, len + 1); // 2. 复制并扩容
        newElements[len] = e; // 3. 在新数组上添加元素
        setArray(newElements); // 4. 替换引用
        return true;
    } finally {
        lock.unlock(); // 5. 解锁
    }
}
```

这个过程清晰地展示了“加锁 -> 复制 -> 修改 -> 替换 -> 解锁”的核心流程。

### 3. 迭代器的特性：Fail-Safe (安全失败)

`CopyOnWriteArrayList` 的迭代器是其最重要的特性之一。

- **快照（Snapshot）迭代**：当你调用 `iterator()` 方法时，它会创建一个迭代器，这个迭代器持有对**当前底层数组**的一个引用（即快照）。
- **绝对安全**：迭代器将始终遍历这个它被创建时所引用的旧数组。即使在迭代过程中，有其他线程修改了 `CopyOnWriteArrayList`（这会产生一个全新的数组），也完全不会影响这个迭代器。
- **不抛出 `ConcurrentModificationException`**：由于迭代器和写操作操作的是不同的数组，所以它永远不会检测到并发修改，因此是**Fail-Safe**的。
- **数据非实时性**：这是它的一个重要权衡。迭代器遍历的是快照，所以它**无法反映**在它被创建之后，列表发生的任何修改（增、删、改）。

### 4. 优缺点总结

#### 优点：

1.  **高并发读性能**：读操作完全不加锁，效率非常高。
2.  **线程安全**：迭代过程绝对安全，不会有并发修改异常的烦恼。

#### 缺点：

1.  **内存消耗大**：每次写操作都需要复制整个数组，如果列表中的元素非常多，会占用大量内存。
2.  **写操作性能差**：数组复制和加锁导致写操作的成本非常高昂。
3.  **数据一致性问题**：它只能保证数据的**最终一致性**，而不能保证实时一致性。读操作可能读到的是过期的旧数据。

### 5. 适用场景

基于以上特性，`CopyOnWriteArrayList` 的适用场景非常明确：

**适用于“读多写少”的并发场景。**

一些典型的例子包括：

- **事件监听器（Listeners）列表**：一个组件的监听器列表通常在初始化后很少改变，但事件触发时会频繁地遍历这个列表以通知所有监听器。
- **配置信息存储**：系统的配置信息在启动时加载，运行时基本不修改，但会被各个模块频繁读取。
- **黑白名单等**：需要被频繁查询，但很少被修改的名单。

**反之，如果你的业务场景写操作非常频繁，或者对数据实时性要求很高，那么绝对不能使用 `CopyOnWriteArrayList`。** 在这种情况下，可能需要考虑使用其他并发数据结构，或者通过更细粒度的锁来管理 `ArrayList`。

## 说一下 HashMap 的底层数据结构？⭐⭐⭐⭐⭐

HashMap 的底层数据结构是**哈希表（Hash Table）**，也叫**散列表**。

### JDK 1.7 及以前：数组 + 链表

在 JDK 1.7 以及更早的版本中，HashMap 的底层结构可以非常清晰地概括为 **“数组 + 链表”**。

1.  **数组 (Array)**：HashMap 内部维护一个 `Entry` 类型的数组，这个数组被称为“桶（bucket）”或“哈希桶”。每个桶可以存放一个 `Entry` 对象的引用。
    ```java
    // JDK 1.7 源码
    transient Entry<K,V>[] table;
    ```
2.  **Entry (链表节点)**：`Entry` 是 HashMap 的一个内部类，它包含了四个核心属性：
    - `K key`：键
    - `V value`：值
    - `int hash`：键的哈希值
    - `Entry<K,V> next`：一个指向下一个 `Entry` 的引用。这个 `next` 引用就是构成链表的关键。

**工作流程 (Put 操作):**

1.  **计算哈希**：当调用 `put(key, value)` 方法时，首先会对 `key` 调用其 `hashCode()` 方法，并对得到的哈希码进行一系列的扰动计算，得到最终的 `hash` 值。
2.  **定位数组索引**：使用这个 `hash` 值通过一个取模运算（在源码中是 `hash & (table.length - 1)`）来计算出该键值对应在 `table` 数组中的索引位置。
3.  **处理哈希冲突**：
    - **如果该索引位置没有元素（桶为空）**：直接创建一个新的 `Entry` 对象，并将其放入该桶中。
    - **如果该索引位置已经有元素（发生哈希冲突）**：说明有不同的 `key` 经过计算后映射到了同一个数组索引。这时，就会以**链表**的形式来解决冲突。新的 `Entry` 会通过**头插法**插入到这个链表的头部，并指向原来位置的那个 `Entry`。在插入前，会遍历链表检查 `key` 是否已存在，如果存在则更新 `value` 值。

**缺点：**
这种结构在理想情况下，增删改查的时间复杂度接近 O(1)。但当出现大量哈希冲突时（例如，多个 key 的 `hashCode()` 返回相同的值），会导致某个桶下的链表变得非常长。在这种极端情况下，HashMap 的操作时间复杂度会从 O(1) 退化到 **O(n)**，性能急剧下降。

### JDK 1.8 及以后：数组 + 链表 + 红黑树

为了解决上述链表过长导致的性能问题，JDK 1.8 对 HashMap 的底层实现做出了重大优化。其结构演变为 **“数组 + 链表 + 红黑树”**。

1.  **数组 (Array)**：主体结构仍然是数组，但数组的元素类型从 `Entry` 变成了 `Node`。`Node` 的核心结构与 `Entry` 类似。
    ```java
    // JDK 1.8 源码
    transient Node<K,V>[] table;
    ```
2.  **链表**：处理哈希冲突的基本方式依然是链表。但与 1.7 不同的是，1.8 采用了**尾插法**来插入新节点，这可以在一定程度上避免多线程环境下扩容时产生循环链表的问题（虽然 HashMap 本身是线程不安全的）。

3.  **红黑树 (Red-Black Tree)**：这是 JDK 1.8 最核心的优化。
    - **树化（Treeify）**：当一个桶中的链表长度达到一个**阈值（`TREEIFY_THRESHOLD`，默认为 8）**，并且**整个哈希表的容量（数组长度）也达到了一个阈值（`MIN_TREEIFY_CAPACITY`，默认为 64）**时，这个链表就会被转换成一个**红黑树**。
    - **退化（Untreeify）**：在 `resize`（扩容）的过程中，如果一个树化的桶中的节点数量减少到一定**阈值（`UNTREEIFY_THRESHOLD`，默认为 6）**，它会从红黑树退化回链表。

**为什么要引入红黑树？**
红黑树是一种自平衡的二叉查找树，它的查询、插入、删除操作的时间复杂度稳定在 $O(log n)$。通过将过长的链表转换为红黑树，即使在最坏的情况下，HashMap 的性能也能从 $O(n)$ 提升到 $O(log n)$，这是一个巨大的改进。

### 总结对比

| 特性               | JDK 1.7 及以前         | JDK 1.8 及以后                                        |
| :----------------- | :--------------------- | :---------------------------------------------------- |
| **底层结构**       | 数组 + 链表            | 数组 + 链表 + 红黑树                                  |
| **存储单元**       | `Entry`                | `Node` (链表) / `TreeNode` (红黑树)                   |
| **链表插入方式**   | 头插法                 | 尾插法                                                |
| **冲突处理**       | 始终使用链表来解决冲突 | 当链表长度 > 8 且数组长度 > 64 时，将链表转换为红黑树 |
| **最坏时间复杂度** | $O(n)$                 | $O(log n)$                                            |
| **最坏时间复杂度** | $O(n)$                 | $O(log n)$                                            |

---

## 红黑树了解吗？⭐⭐⭐⭐⭐

### 1. 什么是红黑树？

红黑树，本质上是一种**自平衡的二叉查找树**（Self-Balancing Binary Search Tree）。

它在普通二叉查找树（Binary Search Tree, BST）的基础上，为每个节点增加了一个颜色属性（红色或黑色）。然后通过一系列必须遵守的着色规则，来确保树在动态地插入和删除节点后，依然能维持一个相对平衡的状态，从而避免了普通二叉查找树在最坏情况下退化成链表的问题。

### 2. 为什么需要红黑树？

我们知道，普通的二叉查找树在理想情况下，查找效率很高，是 $O(log n)$。但如果插入的数据是预先排好序的（例如，依次插入 1, 2, 3, 4, 5），BST 就会退化成一个单向链表，其查找效率也会急剧下降到 $O(n)$。

红黑树的设计目的，就是为了解决这个问题。它通过自平衡机制，保证了树的高度始终在对数级别，从而确保了查找、插入、删除等操作的时间复杂度能够稳定在 $O(log n)$。

### 3. 红黑树的五条核心规则

一个有效的红黑树必须同时满足以下五条性质：

1.  **规则一：** 每个节点要么是红色，要么是黑色 (Every node is either red or black)。
2.  **规则二：** 根节点必须是黑色 (The root is black)。
3.  **规则三：** 所有叶子节点都是黑色的 (Every leaf (NIL) is black)。这里的叶子节点指的是指向 `null` 的空节点，它们虽然不存储数据，但在概念上存在，并且是黑色的。
4.  **规则四：** 红色节点的两个子节点必须都是黑色 (If a node is red, then both its children are black)。这条规则也意味着，**从根到叶子的任何路径上，不会出现两个连续的红色节点**。
5.  **规则五：** 从任一节点到其每个叶子节点的所有路径，都包含相同数目的黑色节点 (For each node, all simple paths from the node to descendant leaves contain the same number of black nodes)。这个相同数目的黑色节点数被称为“黑高（Black-Height）”。

正是这五条规则，尤其是第四和第五条，共同保证了红黑树的平衡。

### 4. 红黑树如何保持平衡？

红黑树的自平衡，主要通过两种基本操作来实现：**变色 (Recoloring)** 和 **旋转 (Rotation)**。

当插入或删除一个节点，导致上述五条规则中的某条被破坏时，红黑树会启动一个修复程序，通过一系列的变色和旋转操作，重新调整树的结构，使其再次满足所有规则，从而恢复平衡。

- **变色 (Recoloring)**：将节点的颜色由红变黑，或由黑变红。这是一个开销很小的操作。
- **旋转 (Rotation)**：这是对树结构进行调整的核心操作，分为**左旋**和**右旋**。旋转会改变某些节点的父子关系，但**不会破坏二叉查找树的性质**（即左子树 < 父节点 < 右子树）。
  - **左旋**：将某个节点作为支点，其右子节点“上提”成为新的父节点，原来的父节点“下降”成为新父节点的左子节点。
  - **右旋**：与左旋相反，将左子节点“上提”成为新的父节点。

### 5. 性能与应用

- **性能**：由于红黑树能够保证树的高度最高约为 `2 * log(n+1)`，所以它的各种操作（增、删、查）的时间复杂度都能稳定在 $O(log n)$。

- **应用**：红黑树在实际工程中应用极为广泛。
  1.  **Java 中的集合类**：这是最经典的例子。`HashMap` 在 JDK 1.8 中，当链表长度超过阈值时会转换为红黑树来处理哈希冲突。`TreeMap` 和 `TreeSet` 的底层数据结构完全就是基于红黑树实现的，用于保证元素的有序性。
  2.  **Linux 内核**：比如用它来管理进程调度（CFS 完全公平调度器）和虚拟内存区域。
  3.  **其他语言和数据库**：C++ STL 中的 `map` 和 `set`，以及 Nginx 的定时器管理，还有一些数据库的索引实现中都有它的身影。

### 总结

总而言之，红黑树并不是一个追求“绝对平衡”的树（像 AVL 树那样），而是一种“大致平衡”的、实现效率更高的平衡树。它在插入和删除时所需的调整操作相对较少，提供了稳定且高效的性能，因此在对性能和稳定性要求高的场景中得到了非常广泛的应用。

---

## HashMap 的 put 流程了解吗？

`put(K key, V value)` 方法的执行过程可以概括为以下几个关键步骤：

### 第一步：计算哈希值

`put` 方法的第一步并不是直接用 `key` 的 `hashCode()`，而是调用内部的 `hash(key)` 方法来计算最终的哈希值。

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

- 如果 `key` 是 `null`，哈希值为 0。
- 如果 `key` 不是 `null`，会先获取其 `hashCode()`，然后将该哈希码的**高 16 位**与**低 16 位**进行**异或（XOR）运算**。

**这么做的目的**是为了让哈希值的高位也能参与到后续的索引计算中，从而减少哈希冲突的概率，让数据在数组中分布得更均匀。

### 第二步：检查与初始化哈希表

在 `putVal` 方法内部，首先会检查内部的哈希表（`Node<K,V>[] table`）是否为空。

```java
if ((tab = table) == null || (n = tab.length) == 0)
    n = (tab = resize()).length;
```

如果 `table` 未被初始化（即 `null` 或者长度为 0），会立即调用 `resize()` 方法进行初始化。`resize()` 方法会创建一个默认容量为 16 的 `Node` 数组，并更新相关的阈值（threshold）。

### 第三步：计算数组索引并处理三种情况

接下来，程序会根据第一步计算出的 `hash` 值来确定这个键值对应该存放在 `table` 数组的哪个位置。计算索引的公式是 `i = (n - 1) & hash`，其中 `n` 是数组的长度。这等价于对数组长度取模，但位运算效率更高。

根据计算出的索引位置 `i` 的状态，会有以下三种情况：

#### 情况一：目标位置为空 (No Collision)

```java
if ((p = tab[i]) == null)
    tab[i] = newNode(hash, key, value, null);
```

如果 `table[i]` 为 `null`，说明这个位置没有任何元素，没有发生哈希冲突。此时，直接创建一个新的 `Node` 节点，并将其放入 `table[i]` 位置即可。这是最理想、最快的情况。

#### 情况二：目标位置已有节点 (Hash Collision)

如果 `table[i]` 不为 `null`，说明发生了哈希冲突。这时程序会进一步判断：

1.  **检查头节点是否就是目标 Key**：

    ```java
    if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
        e = p;
    ```

    首先检查该位置的第一个节点 `p` 的 `hash` 和 `key` 是否与要插入的 `key` 完全相同。如果相同，说明是**更新操作**。直接将这个节点 `p` 赋值给一个临时变量 `e`，后续会用新 `value` 覆盖旧 `value`。

2.  **检查节点类型是红黑树还是链表**：

    - **如果是红黑树 (`p instanceof TreeNode`)**：

      ```java
      else if (p instanceof TreeNode)
          e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
      ```

      调用红黑树专属的 `putTreeVal` 方法，按照红黑树的规则将新节点插入进去。这个过程的时间复杂度是 O(log n)。

    - **如果是链表**：
      ```java
      else {
          for (int binCount = 0; ; ++binCount) {
              // ... 遍历链表
          }
      }
      ```
      程序会遍历整个链表。在遍历过程中：
      - 如果找到了一个与待插入 `key` 完全相同的节点，就将该节点赋值给 `e`，跳出循环，后续进行更新操作。
      - 如果遍历到链表末尾仍未找到相同的 `key`，则使用**尾插法**将新节点连接到链表的末尾。

### 第四步：链表树化检查 (Treeify)

在上面链表插入的流程中，当一个新节点被添加到链表末尾后，会立即检查该链表的长度。

```java
if (binCount >= TREEIFY_THRESHOLD - 1) {
    treeifyBin(tab, hash);
    break;
}
```

- `binCount` 记录了链表的长度（从 0 开始计数）。
- 如果链表长度达到或超过了**树化阈值 `TREEIFY_THRESHOLD`（默认为 8）**，就会调用 `treeifyBin` 方法。
- `treeifyBin` 方法会再次检查当前哈希表的总容量 `table.length` 是否小于 `MIN_TREEIFY_CAPACITY`（默认为 64）。如果小于 64，它会优先选择**扩容（resize）**而不是树化；如果大于等于 64，则会将这条链表正式转换为一棵**红黑树**。

### 第五步：更新 Value (如果 Key 已存在)

经过第三步，如果变量 `e` 不为 `null`（意味着找到了一个已存在的 `key`），则会执行更新操作。

```java
if (e != null) { // e 不为 null 说明 key 已存在
    V oldValue = e.value;
    if (!onlyIfAbsent || oldValue == null)
        e.value = value;
    afterNodeAccess(e); // 为 LinkedHashMap 准备的回调
    return oldValue;
}
```

它会用新的 `value` 替换旧的 `value`，并返回旧的 `value` 值。

### 第六步：更新 size 并检查是否需要扩容

如果 `e` 为 `null`，说明这是一个全新的插入操作。

```java
++modCount;
if (++size > threshold)
    resize();
afterNodeInsertion(evict); // 为 LinkedHashMap 准备的回调
```

1.  修改计数器 `modCount` 加一。
2.  将 `HashMap` 的实际大小 `size` 加一。
3.  检查 `size` 是否超过了**扩容阈值 `threshold`**（`capacity * loadFactor`）。如果超过，则调用 `resize()` 方法进行扩容。扩容通常会将容量扩大为原来的两倍。

以上就是 `HashMap.put()` 方法一个完整、严谨的执行流程。它充分体现了在性能、空间和复杂性之间的权衡与优化。

---

## HashMap 怎么查找元素的呢？

HashMap 的查找（或获取）元素的过程，即 `get(Object key)` 方法的实现，逻辑上与 `put` 方法非常相似，但要简单一些，因为它不涉及修改数据结构（如扩容、树化等）。

整个查找流程可以分解为以下几个步骤：

### 第一步：计算哈希值

和 `put` 方法一样，查找的第一步也是调用内部的 `hash(key)` 方法来计算 `key` 的哈希值。

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

- 如果 `key` 为 `null`，它的哈希值就是 0。
- 否则，获取 `key` 的 `hashCode()`，并将其高 16 位与低 16 位进行异或运算，得到最终的哈希值。这一步是为了让哈希分布更均匀，减少冲突。

### 第二步：定位数组索引

接下来，使用计算出的 `hash` 值和哈希表 `table` 的长度 `n`，通过位运算 `(n - 1) & hash` 来快速定位到该 `key` **可能**所在的数组桶（bucket）的索引位置。

### 第三步：检查桶内元素并返回结果

找到目标索引后，程序会检查这个桶（`table[index]`）的状态，并进行相应的查找：

#### 前置检查：

在正式查找前，会有一个快速的判断：

```java
Node<K,V>[] tab; Node<K,V> first; int n;
if ((tab = table) != null && (n = tab.length) > 0 &&
    (first = tab[(n - 1) & hash]) != null) {
    // ... 进入核心查找逻辑
}
```

这个 `if` 语句确保了：

1.  哈希表 `table` 已经被初始化了。
2.  通过 `hash` 计算出的索引位置上**确实有元素**（`first != null`）。
    如果以上任一条件不满足，说明 `key` 肯定不存在，方法直接返回 `null`。

#### 核心查找逻辑：

如果通过了前置检查，说明目标桶里至少有一个节点。这时会有三种情况：

1.  **头节点就是目标 (最快情况)**：

    ```java
    if (first.hash == hash && // 优先比较 hash，效率高
        ((k = first.key) == key || (key != null && key.equals(k))))
        return first.value;
    ```

    程序会首先检查桶里的第一个节点 `first`。

    - 它会先比较 `hash` 值，如果 `hash` 值不同，那么肯定不是同一个 `key`。
    - 如果 `hash` 值相同，再进一步比较 `key` 对象本身（通过 `==` 判断引用是否相同，或者通过 `equals()` 判断内容是否相同）。
    - 如果两者都匹配，说明找到了，直接返回这个节点的 `value`。

2.  **桶内是红黑树**：

    ```java
    if ((e = first.next) != null) {
        if (first instanceof TreeNode)
            return ((TreeNode<K,V>)first).getTreeNode(hash, key).value;
        // ...
    }
    ```

    如果头节点不是要找的元素，程序会检查它是否有 `next` 节点。如果有，并且头节点是 `TreeNode` 的实例，说明这个桶已经树化了。此时会调用 `getTreeNode(hash, key)` 方法，在红黑树中进行查找。红黑树的查找效率是 $O(log n)$，非常高效。

3.  **桶内是链表**：
    ```java
    do {
        if (e.hash == hash &&
            ((k = e.key) == key || (key != null && key.equals(k))))
            return e.value;
    } while ((e = e.next) != null);
    ```
    如果桶内是链表，程序会进入一个 `do-while` 循环，从第二个节点开始，沿着链表逐个向下遍历。对每个节点都进行和第一步一样的 `hash` 和 `key` 的比较。
    - 如果找到匹配的节点，就返回其 `value`。
    - 如果遍历完整个链表都没有找到，循环结束。

### 第四步：未找到，返回 null

如果以上所有步骤都执行完毕，仍然没有找到匹配的 `key`，那么 `get` 方法最终会返回 `null`。

### 总结

HashMap 的 `get` 流程是一个高效且严谨的过程：

1.  **计算哈希** -> 定位到数组中的特定位置。
2.  如果该位置**为空**，直接返回 `null`。
3.  如果该位置**不为空**：
    - 先检查**第一个节点**是否匹配。
    - 如果不匹配，判断该位置是**红黑树**还是**链表**结构。
    - 在相应的结构中（红黑树或链表）进行查找。
4.  如果找到，返回对应的 `value`；如果遍历完整个结构都没找到，返回 `null`。

由于哈希函数的良好设计和冲突解决机制（链表/红黑树），理想情况下 `get` 操作的时间复杂度是 $O(1)$，最坏情况下（所有元素都在一个红黑树桶中）是 $O(log n)$。

---

## HashMap 的 hash 函数是怎么设计的?

HashMap 的 `hash()` 函数设计是一个非常精妙的工程实践，它的核心目标只有一个：**在计算效率和散列均匀性之间取得最佳平衡，从而尽可能地减少哈希冲突。**

### 1. `hash()` 函数的源码

首先，我们来看一下 JDK 1.8 中 `HashMap` 的 `hash()` 函数源码：

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

这个实现非常简洁，我们来分解一下它的步骤：

1.  首先，判断 `key` 是否为 `null`。如果是，则直接返回 0。这也是为什么 `HashMap` 允许 `key` 为 `null`，并且它总是被放在数组的第 0 个位置。
2.  如果 `key` 不为 `null`，就获取它的原始哈希码 `h = key.hashCode()`。
3.  最关键的一步：将原始哈希码 `h` 与 **`h` 无符号右移 16 位后的结果**进行**异或（`^`）运算**。

### 2. 设计思想：为什么要这么做？

要理解这个设计的巧妙之处，我们必须先知道 `HashMap` 是如何根据这个 `hash` 值计算数组索引的。

`HashMap` 的数组长度 `n` 始终是 2 的幂次方。计算索引的公式是：
`index = hash & (n - 1)`

这个位运算 `& (n - 1)` 非常高效，它等价于 `hash % n`（取模运算），但性能要好得多。

**这里的关键问题就出现了**：因为 `n` 是 2 的幂（例如，默认是 16），那么 `n - 1` 的二进制表示就是低位全是 `1`（例如，15 的二进制是 `0000...00001111`）。当 `hash` 值与 `n - 1` 进行 `&` 运算时，**只有 `hash` 值的低位会参与运算，高位部分完全被忽略了！**

**举个例子：**
假设 `n=16`，`n-1=15`（二进制 `...1111`）。

- 一个 `hashCode` 是 `01010101 00110011 11110000 00001010`
- 另一个 `hashCode` 是 `11110000 10101010 01010101 00001010`

可以看到，这两个 `hashCode` 的高位完全不同，但它们的低 4 位都是 `1010`。如果直接用它们去计算索引，`hashCode & 15` 的结果会完全一样，都等于 10。这就造成了**哈希冲突**。

如果一个类的 `hashCode()` 方法实现得不好，导致生成的哈希码只在**高位**有变化，低位变化很小，那么在使用 `HashMap` 时，无论高位怎么变，算出来的索引都可能集中在几个固定的位置，导致哈希冲突非常严重。

### 3. `hash()` 函数的解决方案

`h ^ (h >>> 16)` 这个操作就是为了解决上述问题。

- `h >>> 16`：`>>>` 是无符号右移，它将 `h` 的高 16 位移动到低 16 位的位置。
- `^`：是异或运算，它的特点是“不同为 1，相同为 0”。

通过将原始哈希码 `h` 的**高 16 位**与**低 16 位**进行**异或**操作，相当于把高位的特征混合到了低位中。这样一来：

- 原始 `hashCode` 的高位发生任何变化，都会通过异或运算**影响到最终 `hash` 值的低位**。
- 最终得到的 `hash` 值，其低位部分包含了原始 `hashCode` 高位和低位的双重信息。

这样，在后续进行 `hash & (n - 1)` 运算时，即使数组长度 `n` 比较小（只用到低几位），这些低位也已经“吸收”了高位的信息，使得最终的索引分布更加均匀，大大减少了冲突的可能性。

### 4. 与 JDK 1.7 的对比

值得一提的是，在 JDK 1.7 中，`hash()` 函数的设计更为复杂，进行了多次位移和异或，目的是扰动得更充分。

```java
// JDK 1.7 的 hash 函数
static int hash(int h) {
    h ^= (h >>> 20) ^ (h >>> 12);
    return h ^ (h >>> 7) ^ (h >>> 4);
}
```

到了 JDK 1.8，设计者们发现，过度复杂的扰动对于性能有一定影响，而仅仅将高 16 位和低 16 位做一次异或，就已经能达到很好的散列效果了。因此，1.8 的设计是**在效率和散列效果之间做出的更优权衡**。

### 总结

`HashMap` 的 `hash()` 函数设计是一个非常典型的“扰动函数”（Disturbance Function）。它的设计目标非常明确：
**通过一次简单高效的位运算，将 `key.hashCode()` 的高位信息混合到低位中，以增强最终 `hash` 值的随机性，使其在后续通过 `& (n-1)` 计算索引时，能够更均匀地分布在数组中，从而有效降低哈希冲突的概率。**

---

## 为什么 hash 函数能减少哈希冲突？

`hash()` 函数本身并不能**杜绝**哈希冲突，但它的设计能够**显著地减少**哈希冲突的概率。

其根本原因在于：**它将 `key` 原始 `hashCode()` 的高位信息也混合到了低位中，从而增加了最终 `hash` 值的随机性，避免了因 `hashCode()` 实现不佳而导致的聚集性冲突。**

下面我将分步拆解这个过程，来解释“为什么”能减少冲突。

### 第一步：理解问题的根源——索引计算的局限性

`HashMap` 确定一个 `key` 应该放在数组哪个位置的公式是：`index = hash & (length - 1)`。

- `length` 是 `HashMap` 内部数组的长度，它永远是 2 的幂（如 16, 32, 64...）。
- `length - 1` 在二进制下，其低位会全是 `1`（如 16-1=15，二进制为 `...00001111`）。

这个 `& (length - 1)` 的位运算，虽然高效，但它有一个天生的“缺陷”：**只有 `hash` 值的低位部分参与了索引计算，高位部分被完全忽略了。**

### 第二步：看一个“坏”的例子（如果没有 `hash()` 函数）

假设我们不使用 `HashMap` 的 `hash()` 函数，而是直接用 `key.hashCode()` 来计算索引。
并且我们现在有一批 `key`，它们的 `hashCode()` 方法实现得不太好，导致它们的值虽然各不相同，但**低位部分却完全一样**。

比如，`HashMap` 的容量是 16 (`length-1` 是 15，二进制 `...1111`)。

- `Key A` 的 `hashCode()`: `01010101 00110011 11110000 00001011`
- `Key B` 的 `hashCode()`: `11110000 10101010 01010101 00001011`

这两个 `hashCode()` 是完全不同的整数，但它们的低 4 位都是 `1011`。

在计算索引时：

- `index A` = `...00001011` & `...00001111` = `1011` (十进制 11)
- `index B` = `...00001011` & `...00001111` = `1011` (十进制 11)

**结果：** 尽管 `Key A` 和 `Key B` 的 `hashCode` 相差巨大，但因为只有低位参与运算，它们计算出了完全相同的索引，导致了严重的哈希冲突。

### 第三步：看 `hash()` 函数如何解决这个问题

现在，我们把 `HashMap` 的 `hash()` 函数请回来：
`final int hash(Object key) { int h; return (h = key.hashCode()) ^ (h >>> 16); }`

它做的最关键的一件事就是：`^ (h >>> 16)`。

1.  `h >>> 16`：把 `hashCode` 的**高 16 位**移动到低 16 位的位置上。
2.  `^` (异或)：将原始的 `hashCode` 和它移位后的结果进行异或。

**异或运算的特性是，它会把两个操作数的差异性体现出来。**

我们再用刚才那两个“坏”的 `key` 来计算：

- **对于 Key A**:

  - `h_A` = `01010101 00110011 11110000 00001011`
  - `h_A >>> 16` = `0000... 01010101 00110011` (高 16 位 `0101...` 被移到了低位)
  - `new_hash_A` = `h_A ^ (h_A >>> 16)`。它的低位部分现在是 `(11110000 00001011)` 与 `(01010101 00110011)` 异或的结果。

- **对于 Key B**:
  - `h_B` = `11110000 10101010 01010101 00001011`
  - `h_B >>> 16` = `0000... 11110000 10101010` (高 16 位 `1111...` 被移到了低位)
  - `new_hash_B` = `h_B ^ (h_B >>> 16)`。它的低位部分现在是 `(01010101 00001011)` 与 `(11110000 10101010)` 异或的结果。

**关键点来了**：由于 `Key A` 和 `Key B` 的高 16 位完全不同，所以它们在与自身低位进行异或后，得到的**新哈希值的低位部分**，变得**不再相同**了！

当这两个新的哈希值再去和 `(length - 1)` 做 `&` 运算时，它们有极大的概率会得到不同的索引值，从而**避免了这次冲突**。

### 总结

`HashMap` 的 `hash()` 函数就像一个**搅拌机**。

- **输入**：一个可能只在高位有变化的 `hashCode`。
- **搅拌过程**：将高位的信息（`h >>> 16`）“倒下来”，与低位信息（`h`）进行“搅拌”（异或 `^`）。
- **输出**：一个高低位特征充分混合的新 `hash` 值。

这个函数确保了，即使 `key` 的 `hashCode()` 方法实现得不好，只要它的高位和低位整体上是不同的，那么最终计算出的索引也会有很好的散列性，从而将元素均匀地分布在 `HashMap` 的数组中，这就是它能减少哈希冲突的根本原因。

---

## 为什么 HashMap 的容量是 2 的幂次方？

将 HashMap 的容量（即内部数组的长度）设计为 2 的幂次方，这是一个非常关键且有意的设计，它带来了两大核心优势：**提高哈希计算效率**和**辅助扩容时的数据迁移**。

### 1. 优势一：提高索引计算效率，并使哈希分布更均匀

这是最主要的原因。

#### （1）用位运算替代取模运算

我们知道，要将一个元素放入哈希表中，需要先计算出它在数组中的索引位置。一个通用的计算方法是**取模（或称取余）运算**：
`index = hashCode % capacity`

然而，取模（`%`）运算在计算机底层执行起来是相对昂贵的，它涉及到除法指令，性能不高。

当 `capacity` 被设计为 2 的幂次方时（例如 16，二进制是 `10000`），就可以使用一个性能高出数倍的**位与（`&`）运算**来替代取模运算。其等价公式为：
`index = hashCode & (capacity - 1)`

**为什么等价？**

- 当 `capacity` 是 2 的幂时，`capacity - 1` 的二进制形式会是低位全为 1 的一串数字。例如，`capacity = 16` (`10000`)，`capacity - 1 = 15` (`00001111`)。
- 任何数与一个低位全是 1 的数进行 `&` 运算，结果就等于这个数本身对 `capacity` 取模。
- 例如，`27 & 15`：
  - 27 的二进制是 `00011011`
  - 15 的二进制是 `00001111`
  - `&` 运算结果是 `00001011`，即十进制的 11。
  - 同时，`27 % 16` 的结果也等于 11。

使用位运算 `&` 代替 `%`，是 HashMap 在性能上的一个重要优化。

#### （2）配合 `hash()` 函数，实现均匀分布

仅仅为了位运算还不够。如果哈希算法不好，即使容量是 2 的幂，也可能导致冲突。
`HashMap` 的 `hash()` 函数 `(h = key.hashCode()) ^ (h >>> 16)` 将 key 的哈希码的高 16 位与低 16 位进行异或，让高位信息也参与到低位的构成中。

**容量是 2 的幂次方**这个设计，与**`hash()` 函数**的设计是相辅相成的。

1.  `hash()` 函数保证了最终得到的哈希值足够“散列”。
2.  `& (capacity - 1)` 运算保证了在计算索引时，这个“散列”的哈希值的每一位都能被有效利用，从而让数据能均匀地分布在数组的各个位置，减少哈希冲突。

### 2. 优势二：方便扩容（Resize）时的数据迁移

当 HashMap 的元素数量超过阈值（`capacity * loadFactor`）时，需要进行扩容（`resize`），通常是容量翻倍（例如从 16 变为 32）。

当容量从 `oldCapacity` 变为 `newCapacity`（`newCapacity = oldCapacity * 2`）时，由于容量是 2 的幂次方，元素在扩容后迁移到新数组的位置只有两种可能：

1.  **留在原索引位置**。
2.  **移动到 `原索引 + oldCapacity` 的位置**。

这个规律的判断依据非常巧妙，只需要看元素 `hash` 值的某一个特定位是 0 还是 1。具体来说，是看 `hash & oldCapacity` 的结果。

**举例说明：**

- 假设 `oldCapacity = 16`（二进制 `00010000`），`newCapacity = 32`。
- 旧的索引计算是 `hash & 15`（`...00001111`）。
- 新的索引计算是 `hash & 31`（`...00011111`）。

对比 `hash & 15` 和 `hash & 31`，它们唯一的区别就在于 `hash` 值的**第 5 位**（从右数，`16` 的二进制中 `1` 所在的位置）是否参与了 `&` 运算。

- **如果 `(hash & 16) == 0`**：说明 `hash` 值的第 5 位是 `0`。那么它与 `15` 和 `31` 进行 `&` 运算的结果是一样的。因此，该元素在新数组中的**索引不变**。
- **如果 `(hash & 16) != 0`**：说明 `hash` 值的第 5 位是 `1`。那么它与 `31` 进行 `&` 运算的结果，会比与 `15` 进行 `&` 运算的结果多出一个 `16`（`00010000`）。因此，该元素在新数组中的索引就是 **`原索引 + 16`**（即 `原索引 + oldCapacity`）。

通过这个简单的判断，`HashMap` 在扩容时，不需要为每个元素重新计算一遍完整的哈希索引。它只需要遍历旧数组的每个桶，然后将桶中的链表（或红黑树）拆分成两条新的链表：一条留在原索引，另一条移动到新索引。这大大简化了数据迁移的逻辑，提高了扩容的效率。

### 总结

综上所述，`HashMap` 的容量被设计为 2 的幂次方，并非偶然，而是为了实现两个关键的性能优化：

1.  **计算索引时**：使用高效的位运算 `&` 代替低效的取模 `%`，并配合 `hash()` 函数保证了元素的均匀分布。
2.  **扩容时**：能够通过一个简单的位检查 `(hash & oldCapacity)`，快速确定元素在新数组中的位置，简化了数据迁移的计算。

这两个设计共同保证了 `HashMap` 在各种操作下都能维持较高的性能。

---

## 如果初始化 HashMap，传一个 17 的容量，它会怎么处理？

如果我们在初始化 `HashMap` 时，传入一个像 17 这样的非 2 的幂次方的容量，`HashMap` **不会直接使用 17 作为其内部数组的容量**。

相反，它会通过内部的一个算法，找到**第一个大于等于 17 的 2 的幂次方数**，并将这个数作为其实际的初始化容量。

对于 17 来说，第一个大于等于它的 2 的幂次方数是 **32**。因此，`new HashMap(17)` 最终创建出来的 `HashMap`，其内部数组的容量（`capacity`）将是 **32**。

### 实现原理：`tableSizeFor()` 方法

这个转换逻辑是在 `HashMap` 的一个静态内部方法 `tableSizeFor(int cap)` 中实现的。当调用 `HashMap(int initialCapacity)` 这个构造函数时，它内部就会调用 `tableSizeFor` 来计算一个合规的容量值。

我们来看一下 `tableSizeFor` 的源码（JDK 1.8），它的实现非常巧妙，完全是基于位运算：

```java
static final int tableSizeFor(int cap) {
    int n = cap - 1;
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

#### 算法解析：

这个算法的核心思想是：**将一个数 `n` 的二进制表示中，从其最高位的 `1` 开始，把右边的所有 `0` 都变成 `1`。** 最后再加 1，就得到了第一个大于等于原始输入值的 2 的幂次方。

我们用 `cap = 17` 来走一遍流程：

1.  **`int n = cap - 1;`**

    - `cap` = 17 (二进制 `0001 0001`)
    - `n` = 16 (二进制 `0001 0000`)
    - 这一步非常关键。它处理了如果输入本身就是 2 的幂次方（比如 16）的情况。如果不减 1，16 会被计算成 32，这是不希望的。通过减 1，16 变成了 15 (`01111`)，经过后续操作再加 1，还能变回 16。

2.  **`n |= n >>> 1;`**

    - `n` = `0001 0000`
    - `n >>> 1` = `0000 1000`
    - `n |= ...` = `0001 0000 | 0000 1000` = `0001 1000`
    - 这一步将最高位的 `1` 向右“传染”了一位。

3.  **`n |= n >>> 2;`**

    - `n` = `0001 1000`
    - `n >>> 2` = `0000 0110`
    - `n |= ...` = `0001 1000 | 0000 0110` = `0001 1110`
    - 现在最高位的 `1` 已经向右“传染”了两位，把 `11` 变成了 `1111`。

4.  **`n |= n >>> 4;`**

    - `n` = `0001 1110`
    - `n >>> 4` = `0000 0001`
    - `n |= ...` = `0001 1110 | 0000 0001` = `0001 1111`

5.  **`n |= n >>> 8;`** 和 **`n |= n >>> 16;`**

    - 这两步继续“传染”，但此时低位已经是全 `1`，所以 `n` 的值 `0001 1111` 不再改变。

6.  **`return ... n + 1;`**
    - 经过上述操作后，`n` 的值是 `0001 1111` (十进制 31)。
    - 最后 `n + 1` = 32 (二进制 `0010 0000`)。
    - 这个结果就是大于等于 17 的最小的 2 的幂次方。

### 总结

因此，当尝试用 `new HashMap(17)` 初始化时，其内部的执行流程是：

1.  构造函数接收到 `initialCapacity = 17`。
2.  调用 `tableSizeFor(17)` 方法。
3.  该方法通过一系列精妙的位运算，计算出结果为 32。
4.  `HashMap` 会将这个计算出的 32 作为其内部哈希表（数组）的初始容量。
5.  同时，它会根据加载因子（默认为 0.75）计算出扩容阈值 `threshold`，即 `32 * 0.75 = 24`。

所以，最终我们得到的是一个**容量为 32**、**扩容阈值为 24** 的 `HashMap` 实例，而不是一个容量为 17 的实例。这个设计完全是为了保证 `HashMap` 后续高效的位运算和扩容机制能够正常工作。

---

## 你还知道哪些哈希函数的构造方法呢？

好的，面试官。除了 `HashMap` 内部使用的那种基于位运算的扰动函数外，在更广泛的数据结构和算法领域，还有很多经典的哈希函数构造方法。了解这些方法有助于我们根据不同的数据特征选择最合适的哈希策略。

以下是一些常见的哈希函数构造方法：

### 1. 直接定址法 (Direct Addressing Method)

这是最简单的一种方法。它直接使用关键字 `key` 本身，或者 `key` 的线性函数作为哈希地址。

- **公式：** `H(key) = key` 或 `H(key) = a * key + b`
- **示例：** 假设我们要统计 0-100 岁之间每个年龄的人口数量，我们可以创建一个大小为 101 的数组，用年龄直接作为数组的索引。
- **优缺点：**
  - **优点：** 非常简单，速度极快，完全不会产生哈希冲突。
  - **缺点：** 对关键字的要求非常苛刻。它要求关键字集合必须是近乎连续的，且不能太大，否则会造成巨大的空间浪费。比如，用员工的身份证号作为 `key`，这个方法就完全不可行。

### 2. 除留余数法 (Division Method)

这是最常用，也是最经典的哈希函数构造方法。它通过取关键字 `key` 对某个数 `p` 求余数，来作为哈希地址。

- **公式：** `H(key) = key % p`
- **示例：** 假设哈希表长度为 11，关键字为 123，则哈希地址为 `123 % 11 = 2`。
- **关键点：** 这个方法的效果好坏，**极度依赖于 `p` 的选择**。
  - `p` 通常选择**小于等于**哈希表长度 `m` 的**最大质数（素数）**。
  - 选择质数可以大大降低周期性关键字（例如 `key` 都是 10 的倍数）导致的冲突。
  - `p` 绝对不能是 2 的幂次方，否则就相当于只取了 `key` 的二进制低位，高位信息全部丢失，会大大增加冲突概率。这与 `HashMap` 的设计思路形成了对比：`HashMap` 正是选择了 2 的幂次方作为容量，但它通过一个独立的 `hash()` 扰动函数来弥补这个问题。

### 3. 平方取中法 (Mid-Square Method)

这种方法通过计算关键字 `key` 的平方，然后取结果的中间几位作为哈希地址。

- **过程：** 计算 `key * key` -> 截取中间 `k` 位。
- **示例：** 关键字为 1234，哈希表长为 1000（需要 3 位地址）。
  - `1234 * 1234 = 1522756`
  - 可以取中间的 3 位 `227` 作为哈希地址。
- **优缺点：**
  - **优点：** 结果的中间几位通常会受到关键字所有位的影响，因此散列效果比较均匀。
  - **缺点：** 计算开销比除留余数法大，且对于 `key` 的取值范围需要有所考量，以防平方后溢出。

### 4. 折叠法 (Folding Method)

该方法将关键字从左到右分割成位数相等的几个部分（最后一部分可能不等），然后将这些部分的值相加（或其他运算），取结果的后几位作为哈希地址。

- **过程：** 分割 -> 相加。
- **示例：** 关键字为 `9876543210`，哈希表长为 10000（需要 4 位地址）。
  - 可以分割成 `987`, `654`, `321`, `0`。
  - 将它们相加：`987 + 654 + 321 + 0 = 1962`。
  - `1962` 就可以作为哈希地址。
- **优缺点：**
  - **优点：** 适合用于关键字位数特别长的情况，可以保证关键字的每一部分都参与到哈希地址的计算中。
  - **缺点：** 计算过程相对复杂。

### 5. 基数转换法 (Radix Conversion Method)

这种方法主要用于处理字符串类型的关键字。它将字符串看作是某个基数（Radix）的数字，然后将其转换为一个整数作为哈希值。

- **过程：** 将字符串的每个字符看作一个“数位”，乘以基数的相应次幂再求和。
- **示例：** 这正是 Java 中 `String.hashCode()` 方法的实现原理。它选择 31 作为基数。对于字符串 "cat"：
  - `hash = 'c' * 31^2 + 'a' * 31^1 + 't' * 31^0`
- **关键点：** 基数 `R` 的选择也很重要，通常选择一个不大不小的质数（如 31），可以获得很好的散列效果。选择 31 还有一个好处是 `31 * i` 可以被编译器优化为 `(i << 5) - i`，效率很高。

### 总结

没有一种哈希函数是万能的。选择哪种方法取决于：

- **关键字的类型和分布特征**：是整数、浮点数还是字符串？数据是连续的还是稀疏的？
- **性能要求**：是追求极致的计算速度，还是更好的散列均匀性？

在实际应用中，我们常常将这些基本方法组合使用，或者像 `HashMap` 那样，在通用方法（如位运算）的基础上增加扰动函数，以达到更好的综合效果。

---

## 解决哈希冲突有哪些方法？

好的，面试官。哈希冲突（Hash Collision）是指两个或多个不同的关键字（Key），通过同一个哈希函数计算后，得到了完全相同的哈希地址。这是一个在哈希表设计中必须解决的核心问题。

解决哈希冲突主要有两大类方法：**开放链址法（Separate Chaining）** 和 **开放定址法（Open Addressing）**。

---

### 1. 开放链址法 (Separate Chaining)，也称“拉链法”

这是 `Java.util.HashMap` 所采用的核心解决方案。

- **核心思想：**
  在哈希表的每个桶（bucket）中，不再只存储一个元素，而是存储一个数据结构（通常是**链表**）的头指针。所有哈希到同一个地址的元素，都以节点的形式加入到这个链表中。

- **工作流程：**

  1.  当插入一个新元素时，首先计算其哈希地址。
  2.  找到对应的桶，将新元素插入到该桶的链表中（在 JDK 1.7 中是头插法，JDK 1.8 改为尾插法）。
  3.  当查找一个元素时，先找到对应的桶，然后遍历该桶的链表，逐个比较关键字是否匹配。

- **优化（如 JDK 1.8 的 HashMap）：**
  当某个桶中的链表长度过长时（默认阈值为 8），为了避免查找效率从 O(1) 退化到 O(n)，该链表会被转换成一个**红黑树**，从而将此桶内的查找效率稳定在 O(log n)。

- **优缺点：**
  - **优点：**
    - 实现简单，逻辑清晰。
    - 对装载因子（Load Factor）的容忍度更高，即使装载因子大于 1 也能正常工作。
    - 删除元素的操作非常简单，直接在链表或红黑树中删除即可。
  - **缺点：**
    - 需要额外的空间来存储指针，有一定内存开销。
    - 链表节点的存储在内存中可能不连续，对 CPU 缓存不友好。

---

### 2. 开放定址法 (Open Addressing)

这种方法的核心思想是，所有元素都必须存储在哈希表数组的内部。当发生冲突时，就去寻找下一个**可用的、开放的**地址来存放元素。

- **核心公式：** `H_i(key) = (H(key) + d_i) % m`，其中 `H(key)` 是原始哈希函数，`d_i` 是探测序列，`m` 是哈希表长。

根据探测序列 `d_i` 的生成方式不同，又分为以下几种：

#### a) 线性探测 (Linear Probing)

- **探测方式：** `d_i = 1, 2, 3, ...`。如果位置 `i` 被占用，就尝试 `i+1`，再被占用就尝试 `i+2`，以此类推。
- **缺点：** 容易产生“**一次聚集**”（Primary Clustering）现象。即一旦一个位置发生冲突，它附近的空位也很容易被填满，形成一个连续的区块，导致后续查找和插入的探测次数越来越多。

#### b) 二次探测 (Quadratic Probing)

- **探测方式：** `d_i = 1², -1², 2², -2², ...`。如果位置 `i` 被占用，就尝试 `i+1²`, `i-1²`, `i+2²`...
- **优点：** 探测的步长不再是固定的 1，而是跳跃式的，可以有效地减轻“一次聚集”现象。
- **缺点：** 可能会产生“**二次聚集**”（Secondary Clustering）。如果两个关键字的原始哈希地址相同，那么它们的整个探测序列也将完全相同，这在一定程度上影响了效率。

#### c) 双重散列 / 再哈希法 (Double Hashing)

- **探测方式：** `d_i = i * H_2(key)`。即使用**第二个独立的哈希函数 `H_2`** 来生成探测的步长。
- **优点：** 这是开放定址法中最好的一种。由于步长是由第二个哈希函数决定的，所以即使原始哈希地址相同的两个关键字，它们的探测序列也大概率不同，极大地减少了聚集现象，使得元素分布更均匀。
- **缺点：** 计算开销更大，因为需要额外执行一次哈希函数。

---

### 3. 再哈希法 (Rehashing)

这其实不算是解决单个冲突的方法，而是一种**动态维护哈希表性能**的策略。

- **核心思想：**
  当哈希表的装载因子（Load Factor）达到某个预设的阈值时，说明哈希表太满了，冲突的概率会急剧增加。此时，程序会创建一个**更大的新哈希表**（通常是原大小的两倍），然后遍历旧表中的所有元素，重新计算它们在新表中的哈希地址，并将其插入到新表中。
- **作用：**
  通过扩大表空间，使得每个桶能容纳的平均元素数减少，从而从根本上降低了哈希冲突的概率。这是一种“空间换时间”的策略，几乎所有现代的哈希表实现（如 `HashMap`）都会使用它。

### 总结对比

| 方法           | 核心思想                     | 优点                               | 缺点                                       |
| :------------- | :--------------------------- | :--------------------------------- | :----------------------------------------- |
| **拉链法**     | 在冲突位置建立链表或红黑树   | 实现和删除简单，对装载因子容忍度高 | 额外指针开销，对 CPU 缓存不友好            |
| **开放定址法** | 冲突时寻找下一个空闲单元格   | 节省内存，缓存友好                 | 实现和删除复杂，对装载因子敏感，易产生聚集 |
| **再哈希法**   | 表满时创建更大的表并重新插入 | 从根本上降低冲突概率，维持高性能   | 扩容瞬间开销较大                           |

在实际应用中，拉链法因其实现的稳定性和简便性而最为常用。开放定址法则在对内存和缓存性能有极致要求的特定场景下使用。而再哈希法是保证所有哈希表在高负载下依然可用的必备策略。

---

## 为什么 HashMap 链表转红黑树的阈值为 8 呢？

将 HashMap 中链表转红黑树的阈值 `TREEIFY_THRESHOLD` 设定为 8，这是一个非常典型的工程决策，它并不是一个随意的数字，而是设计者在**时间效率**和**空间效率**之间进行权衡和实验后得出的一个理想值。

这个答案主要可以在 `HashMap` 源码的注释中找到。核心原因可以概括为以下几点：

### 1. 核心依据：泊松分布 (Poisson Distribution)

HashMap 的设计者假设，在哈希函数设计良好的情况下，节点在哈希表桶中的分布是随机的，这种分布规律可以用**泊松分布**来近似。

泊松分布是用来描述单位时间内随机事件发生次数的概率分布。在这里，“随机事件”就是指一个元素被哈希到某个特定的桶中。

根据泊松分布的概率公式，可以计算出在一个桶中，出现不同数量节点的概率：

| 节点数 (k) | 概率 P(k)       |
| :--------- | :-------------- |
| 0          | 0.367879441     |
| 1          | 0.367879441     |
| 2          | 0.183939721     |
| 3          | 0.061313240     |
| 4          | 0.015328310     |
| 5          | 0.003065662     |
| 6          | 0.000510944     |
| 7          | 0.000072992     |
| **8**      | **0.000009124** |
| 9          | 0.000001014     |

从这张表中我们可以看到：

- 一个桶中出现 8 个节点的概率已经**小于千万分之一**（`0.000009124`）。
- 这是一个极小的概率事件。在正常的业务场景下，如果哈希函数足够好，一个桶的链表长度几乎不可能达到 8。

因此，设计者认为，如果链表长度真的达到了 8，那么有很大概率不再是偶然的哈希冲突，而是**当前的哈希函数设计本身存在问题**（例如，很多人为地构造了具有相同 `hashCode` 的对象来攻击系统）。在这种极端情况下，有必要采取一种更稳妥的措施来保证性能，即将链表转换为红黑树，把最坏的 O(n) 查询时间优化到 O(log n)。

### 2. 时间与空间成本的权衡

选择 8 也是对两种数据结构操作成本的权衡。

- **时间成本**：

  - **链表**：查询的时间复杂度是 O(k)，其中 k 是链表长度。当 k 较小时（比如 1 到 7），遍历链表的开销非常小，甚至可能比操作复杂的红黑树更快（因为红黑树需要进行旋转、变色等维护操作）。
  - **红黑树**：查询的时间复杂度是 O(log k)。当 k 较大时，log k 的优势就体现出来了。`log2(8) = 3`，意味着在有 8 个节点的红黑树中，最多只需要 3 次比较就能找到元素，而链表平均需要 `8/2=4` 次。

- **空间成本**：
  - 红黑树的节点 (`TreeNode`) 大约是普通链表节点 (`Node`) 的**两倍大**。因为 `TreeNode` 除了要维护 `key`, `value`, `hash`, `next` 之外，还需要额外存储 `parent`, `left`, `right` 指针以及 `red` 颜色属性。
  - 如果过早地进行树化（比如链表长度为 2 或 3 就树化），会造成不必要的空间浪费。

### 3. “7” 和 “9” 为什么不好？

- **如果阈值是 7**：可能会导致树化过于频繁。虽然链表长度为 7 的概率也很小，但相比 8 还是高了一个数量级。在一些临界情况下，可能会为了一点点性能提升而付出更多的空间代价，并且让系统设计变得更复杂。
- **如果阈值是 9**：意味着要容忍更长的链表。链表长度为 8 的情况已经非常罕见了，再等到 9 的概率更是微乎其微。将阈值设为 9，对于抵御恶意攻击的意义不大，反而让系统在极端情况下的性能容忍度降低了。

### 总结

综上所述，选择 8 作为链表树化的阈值，是一个非常务实的、基于统计学和工程实践的决策：

1.  **基于泊松分布**，链表长度达到 8 是一个极小概率事件，可以作为判断哈希攻击或哈希函数实现不佳的一个临界点。
2.  在**性能上**，当节点数达到 8 时，红黑树 O(log n) 的优势开始显现，足以弥补其更复杂操作带来的开销。
3.  在**空间上**，避免了在链表长度还很短时就进行树化，节约了内存空间。

它是一个在“正常情况下的普遍性能”和“极端情况下的容错性能”之间取得的精妙平衡点。同时，与之对应的，将树退化回链表的阈值设为 6，也是为了避免在 7 和 8 之间频繁地进行树化和反树化的抖动。

---

## HashMap 扩容发生在什么时候呢？

`HashMap` 的扩容（Resize）是一个核心的动态调整机制，它主要发生在**向 `HashMap` 中添加新元素之后**。

具体来说，扩容的触发时机是在 `putVal()` 方法（`put()` 方法的核心实现）成功插入一个**新的键值对**（而不是更新一个已存在的键值对）之后，通过以下代码来检查和触发的：

```java
// 在 putVal 方法的末尾
++modCount;
if (++size > threshold) {
    resize();
}
afterNodeInsertion(evict);
```

从这段代码中，我们可以精确地总结出扩容的触发条件：

**当 `HashMap` 中实际存储的键值对数量 `size`，在加 1 后，严格大于其当前的扩容阈值 `threshold` 时，就会立即触发 `resize()` 方法。**

### 深入理解触发条件

为了完全理解这个时机，我们需要搞清楚两个关键变量：`size` 和 `threshold`。

1.  **`size`**:

    - 这个变量非常直观，它就代表了 `HashMap` 中当前已经存储的**键值对（Entry/Node）的数量**。
    - 每次成功**插入**一个新元素，`size` 就会加 1。如果是**更新**一个已存在的 `key` 的 `value`，`size` 是不会改变的。

2.  **`threshold` (扩容阈值)**:
    - 这是一个非常关键的临界值。它告诉 `HashMap`：“当你的容量达到这个数时，就应该准备扩容了”。
    - 它的计算公式是：`threshold = capacity * loadFactor`
      - **`capacity`**: `HashMap` 内部数组的当前容量（长度），它始终是 2 的幂次方。
      - **`loadFactor` (加载因子)**: 一个介于 0 和 1 之间的浮点数，默认为 **0.75**。它决定了哈希表在被填满到什么程度时进行扩容。

### 几个典型的扩容场景示例

#### 场景一：使用默认构造函数 `new HashMap()`

1.  **初始化**: `new HashMap()` 不会立即分配空间。`table` 为 `null`，`size=0`，`threshold=0`。
2.  **第一次 `put`**:
    - 在 `putVal` 方法内部，检测到 `table` 为 `null`，会触发第一次 `resize()`。
    - 这次 `resize` 会将 `capacity` 初始化为默认的 **16**，并将 `threshold` 设置为 `16 * 0.75 = 12`。
    - 然后插入第一个元素，`size` 变为 1。
3.  **继续 `put`**:
    - 当插入第 2, 3, ..., 12 个元素时，每次插入后 `size` 都会加 1，但 `size` 始终小于等于 `threshold` (12)，所以不会扩容。
4.  **触发扩容**:
    - 当插入**第 13 个**元素时（假设这 13 个元素的 key 都不相同），流程如下：
      - 先将第 13 个元素插入到哈希表中，此时 `size` 从 12 变为 13。
      - 然后执行判断 `if (++size > threshold)`，即 `if (13 > 12)`。
      - 条件成立，立即触发 `resize()` 方法。

#### 场景二：链表树化过程中的扩容

还有一个稍微特殊的扩容时机。当一个桶中的链表长度达到树化阈值 8 时，`HashMap` 并不一定会立即进行树化。它会先检查当前哈希表的总容量 `capacity`。

- **如果 `capacity < MIN_TREEIFY_CAPACITY` (默认为 64)**: `HashMap` 会认为当前哈动，而不是进行树化。它认为，通过扩大表空间，让元素散列得更开，比将一个短链表树化更为划算。
- **如果 `capacity >= 64`**: 才会真正地将该链表转换为红黑树。

所以，**当链表长度达到 8，但总容量小于 64 时**，也会触发一次扩容。

### 总结

`HashMap` 的扩容主要发生在以下两个时间点：

1.  **主要时机**: 在通过 `put()` 方法成功插入一个**新**的键值对后，如果 `HashMap` 的 `size`（元素总数）**大于**了当前的 `threshold`（扩容阈值）。
2.  **次要时机**: 在插入一个元素后，如果某个桶中的链表长度达到了 8，但此时 `HashMap` 的总容量 `capacity` 还小于 64，系统会优先选择**扩容**而不是树化。

理解这个机制对于优化 `HashMap` 的性能至关重要。如果在创建 `HashMap` 时能预估到要存储的元素数量，通过指定一个合适的初始容量，就可以有效地减少甚至避免昂贵的扩容操作。

---

## HashMap 的扩容机制了解吗？

`HashMap` 的扩容机制（即 `resize()` 方法）是其能够动态适应数据量变化、维持高性能的关键所在。

`resize()` 方法的核心目标有两个：

1.  **扩大容量**：创建一个新的、容量更大的数组（通常是旧容量的两倍）。
2.  **数据迁移**：将旧数组中的所有元素高效地、重新计算索引后，迁移到新数组中。

### 1. 扩容的时机

正如上一个问题所说，当 `HashMap` 的 `size` 超过 `threshold`（容量 \* 加载因子）时，或者在满足树化条件但容量小于 64 时，会触发 `resize()`。

### 2. 扩容的具体流程

`resize()` 方法的执行过程可以分为以下几个步骤：

#### 第一步：计算新容量和新阈值

1.  获取旧的容量 `oldCap` 和旧的阈值 `oldThr`。
2.  **计算新容量 `newCap`**：
    - 如果旧容量 `oldCap` 大于 0，新容量 `newCap` 就是 `oldCap` 的两倍（`oldCap << 1`）。
    - 如果 `HashMap` 是首次初始化（`oldCap` 为 0），`newCap` 会被设置为默认的初始容量 `DEFAULT_INITIAL_CAPACITY`（即 16）。
3.  **计算新阈值 `newThr`**：
    - 通常情况下，`newThr` 就是 `newCap * loadFactor`。
    - 如果是首次初始化，`newThr` 会被设置为默认的 `16 * 0.75 = 12`。

在计算过程中，还会检查新容量是否超过了 `MAXIMUM_CAPACITY`（2^30），以防止无限扩容。

#### 第二步：创建新数组

根据计算出的 `newCap`，创建一个新的 `Node` 数组：
`Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];`
并将 `HashMap` 内部的 `table` 引用指向这个新数组。

#### 第三步：数据迁移（最核心的步骤）

这是 `resize()` 方法最复杂也最巧妙的部分。程序会遍历旧数组 `oldTab` 的每一个桶（bucket），将桶中的元素迁移到新数组 `newTab` 中。

对于 `oldTab` 中的每一个非空桶，其内部的元素（可能是单个节点、链表或红黑树）的处理方式如下：

1.  **如果桶内只有一个节点**：

    - 直接计算该节点在新数组中的索引 `hash & (newCap - 1)`，然后将其放入 `newTab` 的对应位置。

2.  **如果桶内是红黑树 (`TreeNode`)**：

    - 调用红黑树专属的 `split()` 方法。这个方法会高效地将一棵树拆分成两棵树：一棵留在低位（原索引），另一棵移动到高位（`原索引 + oldCap`）。这个过程同样利用了 `(e.hash & oldCap) == 0` 这个判断条件，但处理逻辑比链表更复杂。

3.  **如果桶内是链表（最常见的情况）**：

    - 这部分的优化是 JDK 1.8 `resize` 机制的精华所在。它**不再像 JDK 1.7 那样为每个节点都重新计算一次哈希索引**。
    - 它利用了一个规律：当容量从 `oldCap` 翻倍到 `newCap` 时，一个元素在新数组中的索引，要么**与原索引相同**，要么是**`原索引 + oldCap`**。
    - 这个规律的判断依据就是 `(e.hash & oldCap) == 0` 是否成立。

      - `e.hash & oldCap` 这个位运算，实际上是在检查 `e.hash` 值在 `oldCap` 的二进制最高位（例如 `oldCap=16`，就是在检查第 5 位）上是 `0` 还是 `1`。

    - **具体迁移逻辑如下**：
      - 创建两个新的链表头指针：`loHead` (低位链表) 和 `hiHead` (高位链表)。
      - 遍历旧桶中的整个链表，对每个节点 `e` 进行判断 `(e.hash & oldCap) == 0`：
        - 如果为 **true** (结果为 0)，说明该节点在新数组中的**索引不变**。将该节点追加到 `loHead` 链表的尾部。
        - 如果为 **false** (结果不为 0)，说明该节点在新数组中的索引变为 **`原索引 + oldCap`**。将该节点追加到 `hiHead` 链表的尾部。
      - 遍历完整个旧链表后，我们就得到了两个新的链表。
      - 将 `loHead` 链表直接放入新数组的**原索引**位置 `newTab[j]`。
      - 将 `hiHead` 链表放入新数组的**高位索引**位置 `newTab[j + oldCap]`。

### 4. JDK 1.8 扩容机制的优点

- **高效的数据迁移**：通过 `(e.hash & oldCap)` 的巧妙判断，将一次完整的 rehash 过程，简化为对一个链表的两次遍历，避免了对每个 key 都重新计算 hash，大大提升了扩容效率。
- **解决了并发死循环问题**：JDK 1.7 在多线程环境下扩容时，由于采用头插法，可能会导致链表形成环形，造成死循环。JDK 1.8 改为尾插法，并使用了 `loHead/hiHead` 这种方式来保持元素在链表中的相对顺序，从根本上解决了这个问题（虽然 `HashMap` 本身就不是线程安全的，并发使用仍然会出问题，但这特指扩容时的死循环 bug）。

### 总结

`HashMap` 的扩容是一个精密的“两步走”过程：

1.  **创建新家**：将容量翻倍，创建一个更大的数组。
2.  **搬家**：遍历旧数组，通过一次巧妙的位运算判断，将旧桶中的链表（或红黑树）高效地拆分成两部分，并分别放置到新数组的两个确定位置上。

这个过程保证了 `HashMap` 在动态增长时，依然能维持其高效的性能。

---

## JDK 8 对 HashMap 做了哪些优化呢？

JDK 8 对 `HashMap` 的优化是革命性的，它极大地提升了 `HashMap` 在极端情况下的性能和稳定性。这些优化主要集中在以下几个关键方面：

### 1. 引入红黑树，优化哈希冲突处理

这是 JDK 8 最核心、最著名的优化。

- **优化前 (JDK 1.7)**：
  当发生哈希冲突时，所有冲突的键值对都以**链表**的形式存储在同一个哈希桶中。如果哈希函数设计不当，或者遇到恶意的哈希碰撞攻击，可能导致某条链表变得非常长。在这种情况下，对该链表的查询、插入操作的时间复杂度会从理想的 $O(1)$ 退化为 $O(n)$，性能急剧下降。

- **优化后 (JDK 1.8)**：
  - 引入了**红黑树**作为链表的备用结构。
  - 当一个桶中的链表长度达到**树化阈值 `TREEIFY_THRESHOLD`（默认为 8）**，并且**哈希表的总容量也达到 `MIN_TREEIFY_CAPACITY`（默认为 64）**时，这条链表就会被转换为一棵红黑树。
  - **效果**：红黑树是一种自平衡的二叉查找树，其所有操作的时间复杂度都稳定在 $O(log n)$。通过这个优化，`HashMap` 在最坏情况下的性能得到了质的提升，从 $O(n)$ 优化到了 $O(log n)$，有效地抵御了哈希碰撞攻击。

### 2. 优化扩容（Resize）算法

JDK 8 对数据迁移的逻辑进行了彻底的重新设计，使其更加高效。

- **优化前 (JDK 1.7)**：
  扩容时，需要遍历旧哈希表中的每一个 `Entry`，并**为每一个 `Entry` 重新计算一次完整的哈希索引** (`rehash`)，然后放入新哈希表的对应位置。这个过程相对耗时。

- **优化后 (JDK 1.8)**：
  - 利用了容量是 2 的幂次方的特性。当容量从 `oldCap` 翻倍到 `newCap` 时，一个元素在新数组中的索引，要么**与原索引相同**，要么是 **`原索引 + oldCap`**。
  - 这个判断仅需一次简单的位运算 `(e.hash & oldCap) == 0` 即可完成。
  - 因此，扩容时不再需要 `rehash`，而是将一个旧桶中的链表高效地**拆分成两个子链表**（一个低位链表，一个高位链表），然后分别放置到新数组的两个确定位置。这个过程大大减少了计算量，提升了扩容效率。

### 3. 链表插入方式的改变（头插 -> 尾插）

- **优化前 (JDK 1.7)**：
  发生哈希冲突时，新节点是通过**头插法**插入到链表的。这样做代码实现上稍微简单一些。但它有一个致命缺陷：在多线程环境下并发扩容时，可能会导致链表形成**环形链表**，造成 `get()` 操作时出现死循环，CPU 占用 100%。

- **优化后 (JDK 1.8)**：
  改为了**尾插法**。同时，结合新的扩容算法，元素在扩容前后在链表中的相对顺序得以保持，从根本上解决了扩容时产生环形链表的 bug。（需要强调的是，这并不意味着 `HashMap` 变成了线程安全的，并发写操作依然会产生数据覆盖等问题）。

### 4. `hash()` 函数的简化

- **优化前 (JDK 1.7)**：
  `hash()` 函数为了追求更好的散列效果，进行了 4 次位移和 4 次异或，计算相对复杂。

- **优化后 (JDK 1.8)**：
  `hash()` 函数被简化为 `(h = key.hashCode()) ^ (h >>> 16)`，只进行了一次位移和一次异或。设计者认为，在引入了红黑树作为“兜底”方案后，哈希算法不再需要过度追求极致的散列效果。简化的算法在大多数情况下已经足够好，并且能带来更高的计算效率。这是一种在性能和复杂性之间的权衡。

### 总结

| 优化点            | JDK 1.7 及以前   | JDK 1.8 及以后       | 带来的好处                                      |
| :---------------- | :--------------- | :------------------- | :---------------------------------------------- |
| **冲突处理**      | 只有链表         | **链表 + 红黑树**    | 提升最坏情况下的性能至 $O(log n)$，防范哈希攻击 |
| **扩容算法**      | 逐个 rehash      | **拆分链表为高低位** | 提高扩容效率                                    |
| **链表插入**      | 头插法           | **尾插法**           | 解决了并发扩容时产生环形链表的问题              |
| **`hash()` 函数** | 较复杂的扰动函数 | **简化的扰动函数**   | 在保证足够散列性的前提下，提高了计算效率        |

总而言之，JDK 8 对 `HashMap` 的优化是系统性的，它不仅通过引入红黑树解决了最核心的性能瓶颈问题，还对扩容、插入等细节进行了打磨和改进，使得 `HashMap` 变得更加高效、稳定和健壮。

---

## HashMap 是线程安全的吗？

**`HashMap` 不是线程安全的。**

在未经任何外部同步措施的情况下，如果在多线程环境中使用 `HashMap`，尤其是有线程会修改 `HashMap` 的结构（即添加或删除键值对），那么程序的行为将是不可预测的，可能会导致数据不一致甚至更严重的问题。

### 为什么 HashMap 不是线程安全的？

`HashMap` 的所有操作（如 `put`, `get`, `remove`）在设计时都没有考虑并发执行的情况，其内部没有使用任何同步机制（如 `synchronized` 或 `Lock`）。这会导致在多线程并发访问时，出现以下几种典型的严重问题：

#### 1. 数据覆盖或丢失 (Data Race)

这是最容易发生的问题。假设两个线程同时对同一个 `HashMap` 执行 `put` 操作。

- 线程 A 计算出 `key` 的索引位置，正准备插入。
- 此时 CPU 时间片切换，线程 B 也计算出同一个 `key`（或另一个 `key` 但哈希冲突）的索引位置，并成功将自己的数据插入。
- CPU 再次切换回线程 A，线程 A 继续执行，将自己的数据插入到那个位置，这就会**覆盖**掉线程 B 刚刚插入的数据。
  结果就是，线程 B 的 `put` 操作仿佛从未发生过，造成了数据丢失。

#### 2. 并发修改异常 (ConcurrentModificationException)

这是 `HashMap` 的快速失败（Fail-Fast）机制导致的。

- 当一个线程正在通过迭代器（例如，使用 for-each 循环）遍历 `HashMap` 时，另一个线程突然修改了 `HashMap` 的结构（调用了 `put` 或 `remove` 方法）。
- 修改操作会导致 `HashMap` 内部的 `modCount` 计数器加一。
- 迭代器在下一次调用 `next()` 时，会检查它自己保存的 `expectedModCount` 是否还等于 `HashMap` 的 `modCount`。
- 当发现两者不相等时，迭代器会立刻抛出 `ConcurrentModificationException`，以防止在数据不一致的情况下继续操作。

#### 3. 扩容时的死循环 (JDK 1.7 中最著名的问题)

在 JDK 1.7 及更早版本中，这个问题尤其致命。

- **原因**：多个线程同时检测到需要扩容（`resize`），并同时执行扩容操作。JDK 1.7 的扩容数据迁移采用的是**头插法**，这会导致链表的顺序在迁移后被反转。
- **后果**：在并发环境下，两个线程同时对一个链表进行反转和迁移，极有可能导致链表的指针形成一个**环形链表**。
- **表现**：一旦环形链表形成，后续的 `get()` 操作如果恰好落在这个桶中，就会陷入无限循环，导致 CPU 占用率飙升至 100%。虽然 JDK 1.8 改用尾插法解决了这个特定的死循环 bug，但这并不能改变 `HashMap` 整体的非线程安全属性。

### 如何获得线程安全的 Map？⭐⭐⭐⭐⭐

既然 `HashMap` 不能用于并发环境，Java 提供了以下几种替代方案来实现线程安全的 Map：

#### 1. `Hashtable` (不推荐使用)

- **实现方式**：一个非常古老的类（从 JDK 1.0 就存在）。它通过在几乎所有公开方法（如 `put`, `get`）上都使用 `synchronized` 关键字来保证线程安全。
- **缺点**：它的锁是作用于**整个对象**的。这意味着无论你操作哪个数据，都会锁住整个 `Hashtable`。在多线程环境下，所有线程都必须串行地访问它，导致性能极差，是并发编程中的一个巨大瓶颈。

#### 2. `Collections.synchronizedMap()` (不推荐使用)

- **实现方式**：这是一个基于装饰器模式的工具方法。它接收一个普通的 `Map`（如 `HashMap`），并返回一个线程安全的包装类。其内部也是通过一个 `synchronized (mutex)` 代码块来包裹所有的方法调用。
- **缺点**：与 `Hashtable` 类似，它也是对整个 Map 对象进行加锁，性能低下。此外，对于复合操作（如“先检查再放入”）和迭代操作，仍然需要用户手动在 `synchronized` 块中完成，否则无法保证原子性。

#### 3. `ConcurrentHashMap` (强烈推荐)

这是 `java.util.concurrent` 包下专门为高并发场景设计的 Map，也是现代 Java 开发中的**首选方案**。

- **实现方式**：它采用了更为先进和精细的并发控制策略，而不是简单粗暴地锁住整个对象。
  - **JDK 1.7**：采用**分段锁（Segment）** 技术。将整个哈希表分成多个段（默认为 16），每个段拥有自己独立的锁。不同线程操作不同段的数据时，可以完全并发执行，大大提高了吞吐量。
  - **JDK 1.8**：放弃了分段锁，采用了粒度更小的 **CAS + `synchronized`** 机制。在无冲突时，使用 CAS 操作来尝试写入。当发生冲突时，只 `synchronized` 锁住当前桶的**头节点**（或红黑树的根节点），锁的粒度被缩小到了极致，并发性能得到了进一步的提升。

### 总结

| 特性          | `HashMap`        | `Hashtable`         | `Collections.synchronizedMap` | `ConcurrentHashMap`                  |
| :------------ | :------------- | :---------------- | :---------------------------- | :--------------------------------- |
| **线程安全**  | 否             | 是                | 是                            | **是**                             |
| **锁机制**    | 无             | 对象锁 (锁整个表) | 对象锁 (锁整个表)             | **分段锁/CAS+synchronized (锁桶)** |
| **性能**      | 高 (单线程)    | 差 (并发)         | 差 (并发)                     | **高 (并发)**                      |
| **null 支持** | key/value 均可 | key/value 均不可  | 取决于被包装的 Map            | **key/value 均不可**               |

**结论**：`HashMap` 不是线程安全的，在任何需要并发访问的场景下，都应该使用 **`ConcurrentHashMap`**。

---

## HashMap 内部节点是有序的吗？

**`HashMap` 内部的节点（或者说键值对）在绝大多数情况下是无序的。**

我们不能对 `HashMap` 中元素的迭代顺序做任何假设。当你遍历一个 `HashMap` 时，元素的输出顺序可能会和你插入时的顺序完全不同，并且这个顺序在 `HashMap` 发生扩容等结构性变化后，还可能会再次改变。

### 为什么 `HashMap` 是无序的？

`HashMap` 的“无序性”是其底层**哈希表（Hash Table）** 数据结构直接导致的结果。

1.  **存储位置由哈希值决定**：
    当一个键值对被放入 `HashMap` 时，它的存储位置不是由插入的先后顺序决定的，而是由它的 `key` 的**哈希值**决定的。

    - 程序会计算 `key` 的 `hashCode()`。
    - 通过 `HashMap` 内部的 `hash()` 函数进行扰动计算。
    - 最后通过 `hash & (capacity - 1)` 的位运算，计算出它在底层数组中的索引位置。

    这个索引位置几乎是随机的，只与 `key` 本身有关，与插入时机无关。例如，你可能先 `put("apple", 1)`，再 `put("banana", 2)`，但 "banana" 的哈希值计算出的索引可能比 "apple" 的更靠前。

2.  **扩容会打乱顺序**：
    `HashMap` 的迭代顺序不仅在插入时是无序的，它还是**动态变化的**。当 `HashMap` 的元素数量超过阈值并触发扩容（`resize`）时，其内部数组的容量会发生变化。
    - 所有旧的元素都需要被重新计算它们在新数组中的位置并进行迁移。
    - 这个过程会彻底打乱原有的元素排列。一个原本在同一个链表上的两个元素，扩容后可能被分配到两个完全不同的桶中。

因此，依赖 `HashMap` 的迭代顺序来进行任何业务逻辑都是极其危险和错误的做法。

### 如果需要有序的 Map，应该用什么？

Java 集合框架充分考虑到了对有序性的需求，并提供了专门的 Map 实现：

#### 1. `LinkedHashMap`：按插入顺序或访问顺序排序

- **实现原理**：`LinkedHashMap` 继承自 `HashMap`，它在 `HashMap` 的基础上，额外维护了一个**贯穿所有节点的双向链表**。这个双向链表记录了元素的插入顺序。
- **特性**：
  - 当你遍历 `LinkedHashMap` 时，元素会严格按照你**插入时的顺序**输出。
  - 它还提供了一个特殊的构造函数 `LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder)`。如果最后一个参数 `accessOrder` 传 `true`，那么每次调用 `get()` 方法访问一个元素时，该元素会被移动到双向链表的末尾。这样，遍历时输出的就是**最近最少使用（LRU）** 的顺序。这个特性是实现 LRU 缓存的经典方案。

#### 2. `TreeMap`：按 Key 的自然顺序或自定义顺序排序

- **实现原理**：`TreeMap` 的底层数据结构是**红黑树**，它是一种自平衡的二叉查找树。
- **特性**：
  - `TreeMap` 中的所有元素都始终处于**排序状态**。
  - 排序的依据是 `key`：
    - **自然排序**：如果 `key` 的类实现了 `Comparable` 接口（如 `Integer`, `String`），`TreeMap` 会按照 `compareTo()` 方法定义的顺序进行排序。
    - **自定义排序**：可以在创建 `TreeMap` 时传入一个 `Comparator` 对象，`TreeMap` 会使用这个比较器来对 `key` 进行排序。

### 总结

| Map 实现            | 是否有序 | 排序规则                     | 底层数据结构         |
| :------------------ | :------- | :--------------------------- | :------------------- |
| **`HashMap`**       | **无序** | 无                           | 数组 + 链表 + 红黑树 |
| **`LinkedHashMap`** | **有序** | 插入顺序 或 访问顺序         | 哈希表 + 双向链表    |
| **`TreeMap`**       | **有序** | Key 的自然顺序 或 自定义顺序 | 红黑树               |

所以，结论非常清晰：`HashMap` 本身是无序的。当需要保证 Map 的有序性时，应该根据具体需求（按插入顺序还是按 Key 排序）来选择使用 `LinkedHashMap` 或 `TreeMap`。

---

## 讲讲 HashSet 的底层实现？

一言以蔽之：**`HashSet` 的底层就是通过一个 `HashMap` 来实现的。**

您可以将 `HashSet` 看作是 `HashMap` 的一个“马甲”或“包装器”。`HashSet` 自身并不实现任何复杂的数据存储逻辑，它将所有操作都委托给了其内部的一个私有 `HashMap` 实例来完成。

### 1. 核心设计思想

- `HashSet` 中存储的元素，实际上被当作了其内部 `HashMap` 的**键（Key）**。
- `HashMap` 的 `put(key, value)` 方法要求必须有一个值（Value），但 `Set` 集合只关心元素本身，不关心值。为了解决这个问题，`HashSet` 的所有键都共享一个**静态的、final 的、虚拟的哑元对象（Dummy Object）`PRESENT`** 作为它们的值。

我们可以在 `HashSet` 的源码中清晰地看到这个结构：

```java
public class HashSet<E> ... implements Set<E> ... {

    // 底层的 HashMap 实例
    private transient HashMap<E, Object> map;

    // 一个静态的、作为所有 Value 的虚拟对象
    private static final Object PRESENT = new Object();

    // 构造函数：初始化一个内部的 HashMap
    public HashSet() {
        map = new HashMap<>();
    }

    // ... 其他构造函数
}
```

### 2. 核心方法的实现

`HashSet` 的所有核心方法，都是对其内部 `map` 对应方法的简单调用：

#### `add(E e)` 方法

```java
public boolean add(E e) {
    return map.put(e, PRESENT) == null;
}
```

- **工作流程**：当你调用 `set.add("apple")` 时，`HashSet` 内部实际执行的是 `map.put("apple", PRESENT)`。
- **如何保证唯一性**：这巧妙地利用了 `HashMap.put()` 方法的返回值。
  - 如果 `map` 中不包含键 `"apple"`，`put` 操作会成功插入，并返回 `null`。此时 `add` 方法返回 `true`，表示添加成功。
  - 如果 `map` 中已经包含了键 `"apple"`，`put` 操作会用新的 `PRESENT` 覆盖旧的 `PRESENT`，并返回**旧的值**（也就是 `PRESENT`）。因为返回值不是 `null`，所以 `add` 方法返回 `false`，表示元素已存在，添加失败。
  - 这就天然地保证了 `Set` 中元素的唯一性。

#### `contains(Object o)` 方法

```java
public boolean contains(Object o) {
    return map.containsKey(o);
}
```

- 判断一个元素是否存在于 `Set` 中，等价于判断这个元素是否存在于 `map` 的键集合中。

#### `remove(Object o)` 方法

```java
public boolean remove(Object o) {
    return map.remove(o) == PRESENT;
}
```

- 从 `Set` 中移除一个元素，等价于从 `map` 中移除一个键值对。`map.remove()` 在成功移除后会返回被移除的 `value`（即 `PRESENT`），所以通过比较返回值是否为 `PRESENT` 来判断是否移除成功。

### 3. `HashSet` 特性的来源

`HashSet` 的所有特性，都直接继承自其底层的 `HashMap`：

- **无序性**：`HashSet` 是无序的，因为 `HashMap` 的键是无序的。元素的存储位置由其哈希值决定，与插入顺序无关。
- **唯一性**：如上所述，利用了 `HashMap` 键的唯一性。判断元素是否重复，依赖于元素的 `hashCode()` 和 `equals()` 方法，这和 `HashMap` 判断 `key` 是否重复的逻辑完全一样。
- **允许 `null` 值**：`HashSet` 允许存入一个 `null` 元素，因为 `HashMap` 允许一个 `null` 作为键。
- **性能特征**：由于所有操作都直接委托给了 `HashMap`，所以 `HashSet` 的性能特征与 `HashMap` 完全一致。在没有大量哈希冲突的情况下，其 `add`, `remove`, `contains` 操作的平均时间复杂度都是 $O(1)$。在最坏情况下（JDK 8+），由于 `HashMap` 会树化，时间复杂度为 $O(log n)$。
- **线程不安全**：`HashSet` 是线程不安全的，因为 `HashMap` 是线程不安全的。

### 总结

`HashSet` 的实现是一个典型的**适配器模式**的应用。它本身不处理复杂逻辑，而是聪明地将 `Set` 的接口适配到了一个功能强大的 `HashMap` 上。它没有重复造轮子，而是巧妙地利用了 `HashMap` 已经非常成熟和高效的键管理机制，来快速实现 `Set` 接口的核心功能。

理解了 `HashMap` 的工作原理，就等于理解了 `HashSet` 的绝大部分工作原理。同理，`LinkedHashSet` 的底层是 `LinkedHashMap`，`TreeSet` 的底层是 `TreeMap`，这都是 Java 集合框架一脉相承的设计哲学。
