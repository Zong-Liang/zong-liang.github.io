---
title: "灵医_01_高级RAG引擎构建与全链路优化"
date: 2025-06-02 00:00:01 +0800
categories: [JAVA后端项目, 灵医智能医疗辅助平台]
tags: [RAG, Agent, Spring AI]
pin: false
toc: true
math: true
---

## 相关概念:

### Spring AI:

Spring AI 是一个应用于 AI 工程的应用框架，旨在将 Spring 生态系统的设计原则（如模块化和可移植性）应用于 AI 领域。它简化了在 Java 应用程序中集成 AI 功能的过程，提供了与多种 AI 模型和向量数据库交互的抽象接口。

### Spring Batch:

Spring Batch 是一个轻量级、综合性的批处理框架，用于开发企业系统中的批量数据处理应用。它并非调度框架，而是与调度器协同工作，为海量数据处理提供日志记录、事务管理、任务重启和资源管理等可重用功能。在本项目中，它被用于自动化处理大规模的医学知识。

### 知识处理策略 (Chunk & Multi-representation):

- **Chunk (分块):** 这是在 RAG 应用中预处理文档的关键步骤，即将大型文档分割成更小、更易于管理的文本片段（块）。高质量的分块有助于大语言模型更好地理解检索到的知识，从而提高生成结果的相关性和连贯性。常见的策略包括固定大小分块、语义分块和递归分块。

- **Multi-representation (多表示):** 这种策略的核心思想是为同一文档创建多种不同的表示形式（例如，摘要、关键词、不同角度的描述等），并将这些表示嵌入到向量空间中。当用户查询时，系统会先在多种表示中进行检索，找到最相关的文档，然后将原始文档作为上下文提供给大语言模型，从而提升检索的准确性和对复杂查询的理解能力。

### HyDE (Hypothetical Document Embeddings - 假设性文档嵌入):

这是一种查询扩展技术。它首先利用大型语言模型针对用户的原始查询生成一个“假设性”的答案或文档。然后，将这个假设性文档进行向量化，并用其在向量数据库中进行检索。其基本思想是，一个完整的、包含上下文的假设性答案在语义上可能比简短的原始查询更接近相关的知识片段。

### 混合搜索 (Hybrid Search):

混合搜索结合了两种或多种不同的检索技术，以取长补短，提高检索的全面性和准确性。本项目中结合了：

- **BM25 稀疏检索:** BM25 是一种基于词频的经典排序算法，属于稀疏检索。它通过计算查询词在文档中的频率和分布来评估相关性，能有效匹配关键词，但可能忽略语义信息。
- **Weaviate 向量检索:** Weaviate 是一个开源的向量数据库，它将数据对象通过机器学习模型转化为高维向量进行存储和检索。这种基于向量的稠密检索能够理解查询和文档之间的语义相似性，即使它们的关键词不完全匹配。

### Cohere Rerank API:

在通过混合搜索获得初步的检索结果后，使用 Cohere Rerank API 进行二次精排。这是一个智能的交叉编码模型，它会同时评估查询和每个检索到的文档，并根据它们之间的语义相关性对文档列表进行重新排序，从而将最相关的结果排在前面。这有助于进一步提升最终提供给大语言模型的信息质量。

### Spring AI VectorStore 抽象:

Spring AI 提供了一个名为 VectorStore 的抽象接口，它统一了与不同向量数据库交互的方式。通过面向这个抽象接口而非具体的向量数据库实现（如 Weaviate）进行编程，可以实现底层数据库的松耦合。这意味着未来如果需要更换向量数据库（例如，从 Weaviate 切换到 Milvus 或 PGVector），只需要更换相应的依赖和配置，而无需大规模修改业务代码，从而极大地提升了系统架构的灵活性和可维护性。

## 知识点:

### RAG (Retrieval-Augmented Generation)

1. 是什么： 一种将信息检索（Retrieval）与大语言模型生成（Generation）相结合的技术范式。它首先从外部知识库（如 Weaviate）中检索出与用户问题相关的上下文信息，然后将这些信息连同原始问题一起喂给 LLM，让模型基于给定的“开卷材料”来生成答案。
2. 为什么用：

   - 解决幻觉： 强制 LLM 依据权威、私有的知识回答，极大降低“一本正经地胡说八道”的概率。
   - 知识更新： 无需重新训练庞大的 LLM，只需更新外部知识库，即可让系统掌握最新知识，成本极低。
   - 数据安全与私有化： 核心知识掌握在自己的数据库中，而非完全依赖外部模型的黑盒知识。

3. 面试深挖：

   - Q: "请对比一下 RAG 和模型微调（Fine-tuning）的优缺点和适用场景。"
     - A: RAG 擅长“知识注入”型任务，当需要模型精确掌握并引用特定、频繁更新的知识时（如本项目），是首选，成本低、见效快。微调擅长“能力迁移”型任务，即让模型学会一种新的对话风格、格式或特定领域的“语感”，但无法保证它能记住所有微调数据中的具体事实。在本项目中，我们用 RAG 来确保医学知识的准确性，未来可以考虑用微调来让模型的回答更具“医生口吻”。
   - Q: "一个 RAG 系统的核心组成部分有哪些？"
     - A: 至少包括四部分：1. 数据索引管道（Indexing Pipeline），负责将原始文档处理、分块、向量化后存入知识库；2. 检索器（Retriever），根据用户问题从知识库中查询相关信息；3. 上下文整合器（Context Integrator），将检索到的信息和问题整合成对 LLM 友好的 Prompt；4. 生成器（Generator），即 LLM 本身，负责根据 Prompt 生成最终答案。

### 数据处理 (Chunking & Multi-representation)

1. 是什么：

   - Chunking (分块)： 将长文档切分成更小的、语义完整的文本块。
   - Multi-representation (多表示)： 为同一个文本块创建多种形式的索引，例如：原始文本、该文本的摘要、可能引出该文本的假设性问题等。

2. 为什么用：这是提升检索质量的关键！

   - 分块大小很关键： 太大则噪声多，LLM 处理成本高；太小则语义不完整。需要根据文档类型反复调试。
   - 多表示解决“错配”问题： 用户的提问方式是多样的，可能与原文的措辞差异很大。通过为文档创建摘要或假设性问题索引，可以从不同角度捕捉用户的查询意图，显著提升召回率。

3. 面试深挖：

   - Q: "你项目里是如何做文本分块的？考虑了哪些因素？"
     - A: 我们初期采用基于固定大小（如 512 个 token）和重叠（overlap，如 64 个 token）的递归字符分割。但很快发现对医学文献效果不佳，因为会切断关键的逻辑。后期我们演进了策略：1. 优先按 Markdown 的标题、段落等结构化标记进行分割，保证语义完整性；2. 针对表格、列表等特殊格式，编写了专门的解析器，将其转换为对 LLM 友好的文本格式后再分块。分块大小是实验出来的，在我们的场景下，300-500 个 token 的块在召回率和精确率之间取得了较好平衡。
   - Q: "为什么要用 Multi-representation？能举个具体的例子吗？"
     - A: 当然。比如我们有一段原文描述了“XX 药品的第三期临床试验结果表明其对特定基因突变的患者有效率达到 70%”。用户的提问可能是“有没有治疗 XX 基因突变的特效药？”。如果只对原文索引，可能因为文本不直接匹配而找不到。但如果我们为这段原文额外生成了一个假设性问题索引：“治疗 XX 基因突变用什么药？”，那么用户的查询就能被精准地召回。这就是多表示的威力。

### 高级检索 (混合检索 & Rerank)

1. 是什么：

   - 混合检索： 结合向量检索（理解语义，如 Weaviate）和传统的关键词检索（如 BM25 算法），并合并两路结果。
   - Rerank (重排序)： 在从检索器拿到初步的文档列表后（比如 20 个），并不直接喂给 LLM，而是先用一个更轻量、更专业的模型（如 Cohere Rerank）对这 20 个文档与问题的相关性进行打分和重新排序，只取最靠前的几个（比如 3-5 个）作为最终上下文。

2. 为什么用：

   - 混合检索优势互补： 向量检索擅长理解意图但可能忽略专有名词，关键词检索正好相反。对于医学领域的精确术语、药品名（如“阿司匹林”而不是“止痛药”），混合检索效果远超单一检索。
   - Rerank 是“降本增效”的利器： 1. 增效： 将最相关的知识排在最前面，显著提升 LLM 生成答案的质量。2. 降本： 可以放心地让检索器召回更多候选文档（提高召回率），再通过 Rerank 精选出少量高质量文档喂给 LLM，有效节省了 LLM 昂贵的上下文 Token 费用。

3. 面试深挖：

   - Q: "为什么需要 Rerank？直接让检索器返回 Top-K 不就行了吗？"
     - A: 这是一个经典的权衡。检索器的相似度得分（如余弦相似度）是“对称的”，即 sim(A, B) == sim(B, A)，它无法很好地捕捉“问题”和“答案”之间的那种“非对称相关性”。而 Rerank 模型专门为此训练，它能更好地判断一个文档是否是“对某个问题的回答”。此外，如刚才所说，它允许我们用“粗召回 + 精排序”的模式，在控制成本的同时最大化检索效果。

## 场景题

- Q: "有用户反馈，查询某个罕见病时，系统总是回答‘知识库中暂无相关信息’，但实际上数据库里有相关文献。你会从哪些方面去排查和解决？"
  - A: 这是一个典型的“召回失败”问题，我会按以下链路排查：
    1. **分块与索引问题：** 首先检查那篇文献是否被成功处理。它的分块是否合理？有没有可能因为分块太碎导致关键信息丢失？向量生成是否成功？
    2. **查询扩展问题：** 查看用户原始 Query 和经过 HyDE 扩展后的 Query。是不是 HyDE“帮了倒忙”，生成了错误的假设性答案，导致检索跑偏？可以尝试关闭 HyDE，直接用原始 Query 检索看能否命中。
    3. **检索算法问题：** 罕见病名称是专有名词，很可能是向量检索的短板。我会立刻尝试用 BM25 关键词检索，看是否能命中。如果能，就说明我们的混合检索权重需要调整，或者需要强化关键词检索在特定场景的优先级。
    4. **Embedding 模型问题：** 用户查询和文献的向量在空间中距离可能确实很远。这可能说明我们选用的 Embedding 模型对医学领域的专业术语理解能力不足。可以考虑更换一个在医学语料上预训练过的 Embedding 模型。
    5. **Rerank 问题（可能性小，但需排查）：** 是否有可能文档被召回了，但在 Rerank 阶段被错误地排到了后面？可以查看 Rerank 前的候选列表来确认。
